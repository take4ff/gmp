# %%
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import importlib
from datetime import datetime
from torch.utils.data import DataLoader
from torch import nn, optim
from sklearn.metrics import classification_report
import sys

# matplotlibæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import module.input_mutation_path as imp
import module.get_feature as gfea
import module.mutation_transformer3 as mt
import module.make_dataset as mds
import module.evaluation2 as ev
import module.save2 as save

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# %%
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å†èª­ã¿è¾¼ã¿ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
importlib.reload(imp)
importlib.reload(gfea)
importlib.reload(mt)
importlib.reload(mds)
importlib.reload(ev)
importlib.reload(save)

# %%
# å®Ÿé¨“è¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# =============================================================================

# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
folder_name = "../model/20250707_train5/bert/"
save_dir = os.path.join(folder_name, current_time)
os.makedirs(save_dir, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
model_config = {
    'num_epochs': 30,
    'batch_size': 256,
    'd_model': 256,
    'nhead': 8,
    'num_layers': 4,
    'learning_rate': 1e-4,
    'weight_decay': 1e-5,
    'auto_adjust': True,  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´æ©Ÿèƒ½
    'use_pretrained_bert': True,  # äº‹å‰å­¦ç¿’æ¸ˆã¿BERTã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    'bert_model_name': 'bert-base-uncased',  # ä½¿ç”¨ã™ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«
    'freeze_bert_layers': 2  # ä¸‹ä½ä½•å±¤ã‚’ãƒ•ãƒªãƒ¼ã‚ºã™ã‚‹ã‹ï¼ˆ0ãªã‚‰å…¨ã¦å­¦ç¿’ï¼‰
}

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²è¨­å®š
data_config = {
    'test_start': 36,
    'ylen': 1,
    'val_ratio': 0.2,
    'feature_idx': 6,  # proteinç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    'nmax': 100000000,
    'nmax_per_strain': 1000000
}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
dataset_config = {
    'strains': ['B.1.1.7','P.1','BA.2','BA.1.1','BA.1','B.1.617.2','B.1.351','B.1.1.529'],
    'usher_dir': '../usher_output/',
    'bunpu_csv': "table_heatmap/250621/table_set/table_set.csv",
    'codon_csv': 'meta_data/codon_mutation4.csv',
    'cache_dir': '../cache',  # ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    'filter_options': 'unique'
}

def force_print(message):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§å¼·åˆ¶å‡ºåŠ›"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {message}")
    sys.stdout.flush()

print(f"å®Ÿé¨“è¨­å®šå®Œäº† - ä¿å­˜å…ˆ: {save_dir}")
print(f"å¯¾è±¡å¤‰ç•°æ ª: {dataset_config['strains']}")
print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š: d_model={model_config['d_model']}, nhead={model_config['nhead']}, num_layers={model_config['num_layers']}")

names, lengths, base_HGVS_paths = imp.input(
dataset_config['strains'], 
dataset_config['usher_dir'], 
nmax=data_config['nmax'], 
nmax_per_strain=data_config['nmax_per_strain']
)
if dataset_config['filter_options'] == 'unique':
    base_HGVS_paths = [list(item) for item in dict.fromkeys(tuple(path) for path in base_HGVS_paths)]

print(f"å¯¾è±¡å¤‰ç•°æ ªã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(base_HGVS_paths)}")

bunpu_df = pd.read_csv(dataset_config['bunpu_csv'])
codon_df = pd.read_csv(dataset_config['codon_csv'])

def separate_data(base_HGVS_paths):
    datas = []
    for i in range(0, len(base_HGVS_paths)):
        base_HGVS_path = base_HGVS_paths[i]


        data_ts = {}
        for i in range(len(base_HGVS_path)):
            for mutation in base_HGVS_path[i].split(','):
                if mutation != '':
                    if data_ts.get(i+1) is None:
                        data_ts[i+1] = []
                    data_ts[i+1].append(mutation)
            
        datas.append(data_ts)
    return datas

data = separate_data(base_HGVS_paths)

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®å®Ÿè¡Œ
train_x, train_y, val_x, val_y, test_x, test_y = mds.create_time_aware_split_modified(
    data, data_config['test_start'], data_config['ylen'], data_config['val_ratio']
)

def extract_protein(y, codon_df, bunpu_df):
    # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã‚’æŠ½å‡º
    new_y = []
    for mutations in y:
        proteins = []
        for mutation in mutations:
            temp, temp, protein, temp, temp = gfea.Mutation_features(mutation, codon_df, bunpu_df)
            proteins.append(protein)
        new_y.append(proteins)
    return new_y

# ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º
train_y_protein = extract_protein(train_y, codon_df, bunpu_df)
val_y_protein = extract_protein(val_y, codon_df, bunpu_df)

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«ã®çµåˆ
train_x2, train_y2 = mds.add_x_by_y(train_x, train_y_protein)
val_x2, val_y2 = mds.add_x_by_y(val_x, val_y_protein)

print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_x2)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_x2)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_x)} ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—")
# %%
# BERTé¢¨ã®èªå½™æ§‹ç¯‰ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
# =============================================================================

def create_mutation_vocabulary():
    """å¡©åŸºå¤‰ç•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã®èªå½™ã‚’ä½œæˆ"""
    base = ['A', 'C', 'G', 'T']
    
    # ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
    vocab = {
        '<PAD>': 0,
        '<UNK>': 1,
        '<CLS>': 2,
        '<SEP>': 3
    }
    
    # å¡©åŸºå¤‰ç•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ (ä¾‹: A1C, A1G, A1T, ...)
    # 4ç¨®é¡ã®å¡©åŸº Ã— 3ç¨®é¡ã®å¤‰ç•° Ã— 30,000ä½ç½® = 360,000ãƒ‘ã‚¿ãƒ¼ãƒ³
    mutations = []
    for b1 in base:
        for b2 in base:
            if b1 != b2:  # åŒã˜å¡©åŸºã¸ã®å¤‰ç•°ã¯é™¤å¤–
                for pos in range(1, 30001):  # ä½ç½®1-30000
                    mutations.append(f"{b1}{pos}{b2}")
    
    # å¤‰ç•°ã‚’èªå½™ã«è¿½åŠ 
    for i, mutation in enumerate(sorted(mutations), start=4):
        vocab[mutation] = i
    
    return vocab

def create_protein_vocabulary():
    """ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³èªå½™ã‚’ä½œæˆ"""
    protein_names = [
        "non_coding1", "nsp1", "nsp2", "nsp3", "nsp4", "nsp5", "nsp6", "nsp7", "nsp8", "nsp9", "nsp10",
        "nsp12", "nsp13", "nsp14", "nsp15", "nsp16", "non_coding2", "S", "non_coding3", "ORF3a", 
        "non_coding4", "E", "non_coding5", "M", "non_coding6", "ORF6", "non_coding7", "ORF7a", 
        "ORF7b", "non_coding8", "ORF8", "non_coding9", "N", "non_coding10", "ORF10", "non_coding11"
    ]
    
    vocab = {
        '<PAD>': 0,
        '<UNK>': 1
    }
    
    for i, protein in enumerate(protein_names, start=2):
        vocab[protein] = i
    
    return vocab

# èªå½™è¾æ›¸ã‚’ä½œæˆ
print("èªå½™è¾æ›¸ã‚’æ§‹ç¯‰ä¸­...")
mutation_vocab = create_mutation_vocabulary()
protein_vocab = create_protein_vocabulary()

print(f"å¤‰ç•°èªå½™ã‚µã‚¤ã‚º: {len(mutation_vocab):,}")
print(f"ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³èªå½™ã‚µã‚¤ã‚º: {len(protein_vocab):,}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®å®šç¾©
class MutationBERTDataset(torch.utils.data.Dataset):
    def __init__(self, x_data, y_data, mutation_vocab, protein_vocab, max_length=512):
        self.x_data = x_data
        self.y_data = y_data
        self.mutation_vocab = mutation_vocab
        self.protein_vocab = protein_vocab
        self.max_length = max_length
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        all_proteins = []
        for proteins in y_data:
            all_proteins.extend(proteins)
        
        unique_proteins = sorted(list(set(all_proteins)))
        self.classes_ = unique_proteins
        self.protein_to_idx = {protein: idx for idx, protein in enumerate(unique_proteins)}
        self.num_classes = len(unique_proteins)
        
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, idx):
        # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ§‹ç¯‰
        x_sample = self.x_data[idx]
        y_sample = self.y_data[idx]
        
        # å¤‰ç•°ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ§‹ç¯‰
        mutation_tokens = [self.mutation_vocab['<CLS>']]
        
        # x_sampleãŒè¾æ›¸ã‹ãƒªã‚¹ãƒˆã‹ã‚’åˆ¤å®š
        if isinstance(x_sample, dict):
            # è¾æ›¸ã®å ´åˆï¼š{ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: [å¤‰ç•°ãƒªã‚¹ãƒˆ]}
            for ts, mutations in sorted(x_sample.items()):
                for mutation in mutations:
                    token_id = self.mutation_vocab.get(mutation, self.mutation_vocab['<UNK>'])
                    mutation_tokens.append(token_id)
                mutation_tokens.append(self.mutation_vocab['<SEP>'])
        elif isinstance(x_sample, list):
            # ãƒªã‚¹ãƒˆã®å ´åˆï¼š[å¤‰ç•°1, å¤‰ç•°2, ...]
            for mutation in x_sample:
                token_id = self.mutation_vocab.get(mutation, self.mutation_vocab['<UNK>'])
                mutation_tokens.append(token_id)
            mutation_tokens.append(self.mutation_vocab['<SEP>'])
        else:
            # ãã®ä»–ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
            raise ValueError(f"Unsupported x_sample type: {type(x_sample)}")
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚
        if len(mutation_tokens) > self.max_length:
            mutation_tokens = mutation_tokens[:self.max_length]
        else:
            padding_length = self.max_length - len(mutation_tokens)
            mutation_tokens.extend([self.mutation_vocab['<PAD>']] * padding_length)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
        attention_mask = [1 if token != self.mutation_vocab['<PAD>'] else 0 for token in mutation_tokens]
        
        # ãƒ©ãƒ™ãƒ«ï¼ˆæœ€åˆã®ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ã‚’ä½¿ç”¨ï¼‰
        label = self.protein_to_idx[y_sample[0]]
        
        return {
            'input_ids': torch.tensor(mutation_tokens, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
train_dataset = MutationBERTDataset(train_x2, train_y2, mutation_vocab, protein_vocab)
val_dataset = MutationBERTDataset(val_x2, val_y2, mutation_vocab, protein_vocab, train_dataset.max_length)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_loader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=model_config['batch_size'], shuffle=False)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
print(f"æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {train_dataset.max_length}")
print(f"ã‚¯ãƒ©ã‚¹: {train_dataset.classes_}")

# %%
# BERTé¢¨ã®Transformerãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
# =============================================================================

class MutationBERTModel(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, num_classes=36, max_seq_length=512):
        super(MutationBERTModel, self).__init__()
        
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        
        # Embeddingå±¤
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_length, d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, num_classes)
        )
        
    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_length = input_ids.shape
        
        # Position IDs
        position_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        embeddings = token_embeds + position_embeds
        
        # Attention mask for transformer (inverted)
        if attention_mask is not None:
            # True for masked positions
            transformer_mask = (attention_mask == 0)
        else:
            transformer_mask = None
        
        # Transformer
        hidden_states = self.transformer(embeddings, src_key_padding_mask=transformer_mask)
        
        # CLSãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›ã‚’ä½¿ç”¨ï¼ˆæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        cls_output = hidden_states[:, 0, :]
        
        # åˆ†é¡
        logits = self.classifier(cls_output)
        
        return logits

class PretrainedBERTModel(nn.Module):
    def __init__(self, bert_model_name, num_classes, mutation_vocab, freeze_layers=0):
        super(PretrainedBERTModel, self).__init__()
        
        try:
            import transformers
            from transformers import AutoModel, AutoConfig, AutoTokenizer
            self.use_pretrained = True
            
            # BERTã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨èªå½™ã‚’å–å¾—
            print(f"ğŸ¤— transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦BERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™: {bert_model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
            original_vocab_size = len(self.tokenizer)
            print(f"å…ƒã®BERTèªå½™ã‚µã‚¤ã‚º: {original_vocab_size:,}")
            
            # ã‚«ã‚¹ã‚¿ãƒ å¤‰ç•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’BERTèªå½™ã«è¿½åŠ 
            print("å¤‰ç•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’BERTèªå½™ã«è¿½åŠ ä¸­...")
            mutation_tokens = []
            for token, _ in sorted(mutation_vocab.items(), key=lambda x: x[1]):
                if token not in ['<PAD>', '<UNK>', '<CLS>', '<SEP>']:  # ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã¯é™¤ã
                    mutation_tokens.append(token)
            
            print(f"è¿½åŠ ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(mutation_tokens):,}")
            
            # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ï¼ˆãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–ï¼‰
            num_added_tokens = self.tokenizer.add_tokens(mutation_tokens)
            print(f"BERTèªå½™ã« {num_added_tokens:,} å€‹ã®å¤‰ç•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            print(f"èªå½™ã‚µã‚¤ã‚º: {original_vocab_size:,} â†’ {len(self.tokenizer):,}")
            
            # èªå½™ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆã‚«ã‚¹ã‚¿ãƒ â†’BERTï¼‰- é«˜é€ŸåŒ–ç‰ˆ
            print("èªå½™ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
            self.vocab_mapping = {}
            
            # ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡¦ç†
            special_tokens = {
                '<PAD>': self.tokenizer.pad_token_id or self.tokenizer.unk_token_id,
                '<UNK>': self.tokenizer.unk_token_id or 0,
                '<CLS>': self.tokenizer.cls_token_id or self.tokenizer.unk_token_id,
                '<SEP>': self.tokenizer.sep_token_id or self.tokenizer.unk_token_id
            }
            
            for token, custom_id in mutation_vocab.items():
                if token in special_tokens:
                    self.vocab_mapping[custom_id] = special_tokens[token]
                else:
                    # å¤‰ç•°ãƒˆãƒ¼ã‚¯ãƒ³ã¯æ–°ã—ãè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ç›´æ¥IDã‚’å–å¾—
                    bert_id = self.tokenizer.convert_tokens_to_ids(token)
                    self.vocab_mapping[custom_id] = bert_id
            
            print(f"èªå½™ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(self.vocab_mapping):,} ãƒˆãƒ¼ã‚¯ãƒ³")
            
            # é«˜é€Ÿå¤‰æ›ç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            print("é«˜é€Ÿå¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")
            max_custom_id = max(mutation_vocab.values())
            self.vocab_mapping_tensor = torch.full(
                (max_custom_id + 1,), 
                self.tokenizer.unk_token_id, 
                dtype=torch.long
            )
            
            for custom_id, bert_id in self.vocab_mapping.items():
                self.vocab_mapping_tensor[custom_id] = bert_id
                
            # ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²ï¼ˆGPUã«è‡ªå‹•ç§»å‹•ï¼‰
            self.register_buffer('vocab_mapping_tensor', self.vocab_mapping_tensor)
            print("é«˜é€Ÿå¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")
            
            # BERTãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å–å¾—
            config = AutoConfig.from_pretrained(bert_model_name)
            self.d_model = config.hidden_size
            
            # BERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            self.bert = AutoModel.from_pretrained(bert_model_name)
            
            # èªå½™ã‚µã‚¤ã‚ºãŒå¤‰æ›´ã•ã‚ŒãŸã®ã§ã€åŸ‹ã‚è¾¼ã¿å±¤ã‚’ãƒªã‚µã‚¤ã‚º
            self.bert.resize_token_embeddings(len(self.tokenizer))
            print(f"BERTåŸ‹ã‚è¾¼ã¿å±¤ã‚’ãƒªã‚µã‚¤ã‚º: {original_vocab_size:,} â†’ {len(self.tokenizer):,}")
            
            # ä¸‹ä½å±¤ã‚’ãƒ•ãƒªãƒ¼ã‚º
            if freeze_layers > 0:
                for i, layer in enumerate(self.bert.encoder.layer):
                    if i < freeze_layers:
                        for param in layer.parameters():
                            param.requires_grad = False
                print(f"BERTä¸‹ä½{freeze_layers}å±¤ã‚’ãƒ•ãƒªãƒ¼ã‚ºã—ã¾ã—ãŸ")
            
            # åˆ†é¡ãƒ˜ãƒƒãƒ‰
            self.classifier = nn.Sequential(
                nn.Dropout(0.1),
                nn.Linear(self.d_model, self.d_model // 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(self.d_model // 2, num_classes)
            )
            
        except ImportError:
            print("âš ï¸ transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.use_pretrained = False
            
    def forward(self, input_ids, attention_mask=None):
        if not self.use_pretrained:
            raise RuntimeError("äº‹å‰å­¦ç¿’æ¸ˆã¿BERTãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # ã‚«ã‚¹ã‚¿ãƒ èªå½™IDã‚’BERTèªå½™IDã«å¤‰æ›ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
        # ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã‚’ä¸€åº¦ã«å¤‰æ›
        input_ids_flat = input_ids.flatten()
        
        # ç¯„å›²å¤–ã®IDã‚’unk_token_idã«ã‚¯ãƒ©ãƒ³ãƒ—
        valid_mask = input_ids_flat < len(self.vocab_mapping_tensor)
        clamped_ids = torch.clamp(input_ids_flat, 0, len(self.vocab_mapping_tensor)-1)
        
        bert_input_ids_flat = torch.where(
            valid_mask,
            self.vocab_mapping_tensor[clamped_ids],
            torch.tensor(self.tokenizer.unk_token_id, device=input_ids.device, dtype=input_ids.dtype)
        )
        
        bert_input_ids = bert_input_ids_flat.view(input_ids.shape)
            
        # BERT forward
        outputs = self.bert(input_ids=bert_input_ids, attention_mask=attention_mask)
        
        # CLSãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        cls_output = outputs.last_hidden_state[:, 0, :]
        
        # åˆ†é¡
        logits = self.classifier(cls_output)
        
        return logits

def create_model(model_config, vocab_size, num_classes, max_seq_length, mutation_vocab=None):
    """ãƒ¢ãƒ‡ãƒ«è¨­å®šã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    
    if model_config['use_pretrained_bert']:
        print("ğŸ¤— äº‹å‰å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
        print(f"ãƒ¢ãƒ‡ãƒ«: {model_config['bert_model_name']}")
        print(f"ãƒ•ãƒªãƒ¼ã‚ºå±¤æ•°: {model_config['freeze_bert_layers']}")
        
        if mutation_vocab is None:
            raise ValueError("äº‹å‰å­¦ç¿’æ¸ˆã¿BERTã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€mutation_vocabãŒå¿…è¦ã§ã™")
        
        try:
            model = PretrainedBERTModel(
                bert_model_name=model_config['bert_model_name'],
                num_classes=num_classes,
                mutation_vocab=mutation_vocab,
                freeze_layers=model_config['freeze_bert_layers']
            )
            
            if not model.use_pretrained:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                print("ğŸ”„ ã‚ªãƒªã‚¸ãƒŠãƒ«BERTãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                model = MutationBERTModel(
                    vocab_size=vocab_size,
                    d_model=model_config['d_model'],
                    nhead=model_config['nhead'],
                    num_layers=model_config['num_layers'],
                    num_classes=num_classes,
                    max_seq_length=max_seq_length
                )
                
        except Exception as e:
            print(f"âš ï¸ äº‹å‰å­¦ç¿’æ¸ˆã¿BERTã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            print("ğŸ”„ ã‚ªãƒªã‚¸ãƒŠãƒ«BERTãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            model = MutationBERTModel(
                vocab_size=vocab_size,
                d_model=model_config['d_model'],
                nhead=model_config['nhead'],
                num_layers=model_config['num_layers'],
                num_classes=num_classes,
                max_seq_length=max_seq_length
            )
    else:
        print("ğŸ”§ ã‚ªãƒªã‚¸ãƒŠãƒ«BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
        model = MutationBERTModel(
            vocab_size=vocab_size,
            d_model=model_config['d_model'],
            nhead=model_config['nhead'],
            num_layers=model_config['num_layers'],
            num_classes=num_classes,
            max_seq_length=max_seq_length
        )
    
    return model

# %%
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã¨è¨“ç·´è¨­å®š
# =============================================================================

print("ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = create_model(
    model_config=model_config,
    vocab_size=len(mutation_vocab),
    num_classes=train_dataset.num_classes,
    max_seq_length=train_dataset.max_length,
    mutation_vocab=mutation_vocab
).to(device)

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
criterion = nn.CrossEntropyLoss()

# äº‹å‰å­¦ç¿’æ¸ˆã¿BERTã®å ´åˆã¯å­¦ç¿’ç‡ã‚’èª¿æ•´
if model_config['use_pretrained_bert'] and hasattr(model, 'use_pretrained') and model.use_pretrained:
    # BERTãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨åˆ†é¡ãƒ˜ãƒƒãƒ‰ã§ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®š
    bert_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if 'bert' in name:
            bert_params.append(param)
        else:
            classifier_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': bert_params, 'lr': model_config['learning_rate'] * 0.1},  # BERTã¯å°ã•ã„å­¦ç¿’ç‡
        {'params': classifier_params, 'lr': model_config['learning_rate']}    # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¯é€šå¸¸ã®å­¦ç¿’ç‡
    ], weight_decay=model_config['weight_decay'])
    
    print(f"å·®åˆ†å­¦ç¿’ç‡è¨­å®š:")
    print(f"  BERTå±¤: {model_config['learning_rate'] * 0.1:.2e}")
    print(f"  åˆ†é¡ãƒ˜ãƒƒãƒ‰: {model_config['learning_rate']:.2e}")
else:
    optimizer = optim.AdamW(model.parameters(), lr=model_config['learning_rate'], weight_decay=model_config['weight_decay'])
    print(f"çµ±ä¸€å­¦ç¿’ç‡: {model_config['learning_rate']:.2e}")

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)

print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(mutation_vocab):,}")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
print(f"æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {train_dataset.max_length}")

# ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®è¡¨ç¤º
if hasattr(model, 'use_pretrained') and model.use_pretrained:
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: äº‹å‰å­¦ç¿’æ¸ˆã¿BERT ({model_config['bert_model_name']})")
    if hasattr(model, 'd_model') and model.d_model:
        print(f"éš ã‚Œå±¤ã‚µã‚¤ã‚º: {model.d_model}")
    print(f"ãƒ•ãƒªãƒ¼ã‚ºå±¤æ•°: {model_config.get('freeze_bert_layers', 0)}")
else:
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: ã‚ªãƒªã‚¸ãƒŠãƒ«BERT")
    print(f"d_model: {model_config['d_model']}, nhead: {model_config['nhead']}, num_layers: {model_config['num_layers']}")

# %%
# è¨“ç·´é–¢æ•°ã®å®šç¾©
# =============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    return total_loss / len(train_loader), correct / total

def evaluate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())
    
    return total_loss / len(val_loader), correct / total, all_preds, all_targets

# %%
# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
# =============================================================================

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
best_val_acc = 0
best_model_state = None
train_losses = []
val_losses = []
train_accs = []
val_accs = []
epoch_times = []

print("è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
training_start_time = time.time()

try:
    for epoch in range(model_config['num_epochs']):
        epoch_start_time = time.time()
        force_print(f"\nEpoch {epoch+1}/{model_config['num_epochs']}")
        
        # è¨“ç·´
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # æ¤œè¨¼
        val_loss, val_acc, val_preds, val_targets = evaluate(model, val_loader, criterion, device)
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ›´æ–°
        scheduler.step(val_loss)
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚é–“è¨ˆç®—
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)
        
        # çµæœã‚’è¨˜éŒ²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        # æ™‚é–“æƒ…å ±ã‚’å«ã‚€å‡ºåŠ›
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print(f"Epoch Time: {epoch_duration:.2f}s ({epoch_duration/60:.1f}min)")
        
        # ç´¯ç©æ™‚é–“ã¨æ¨å®šæ®‹ã‚Šæ™‚é–“
        total_elapsed = sum(epoch_times)
        avg_epoch_time = total_elapsed / len(epoch_times)
        remaining_epochs = model_config['num_epochs'] - (epoch + 1)
        estimated_remaining = avg_epoch_time * remaining_epochs
        
        print(f"Elapsed: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min), "
              f"ETA: {estimated_remaining:.1f}s ({estimated_remaining/60:.1f}min)")
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            print(f"æ–°ã—ã„æœ€è‰¯ãƒ¢ãƒ‡ãƒ« (Val Acc: {val_acc:.4f})")

    # è¨“ç·´å®Œäº†æ™‚ã®çµ±è¨ˆ
    training_end_time = time.time()
    total_training_time = training_end_time - training_start_time
    
    print(f"\n=== è¨“ç·´å®Œäº†! ===")
    print(f"æœ€è‰¯æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f}")
    print(f"ç·è¨“ç·´æ™‚é–“: {total_training_time:.1f}s ({total_training_time/60:.1f}min)")
    print(f"å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {np.mean(epoch_times):.2f}s")
    print(f"æœ€é€Ÿã‚¨ãƒãƒƒã‚¯: {min(epoch_times):.2f}s")
    print(f"æœ€é…ã‚¨ãƒãƒƒã‚¯: {max(epoch_times):.2f}s")

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    if best_model_state:
        model.load_state_dict(best_model_state)
        
except Exception as e:
    print(f"è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    import traceback
    traceback.print_exc()

# %%
# è¨“ç·´çµæœã®åˆ†æã¨å¯è¦–åŒ–
# =============================================================================

# è¨“ç·´çµæœã®å¯è¦–åŒ–
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 3, 3)
# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡
val_loss, val_acc, val_preds, val_targets = evaluate(model, val_loader, criterion, device)
print(f"æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")

# ã‚¯ãƒ©ã‚¹åã¨ãƒ©ãƒ™ãƒ«ã®å¯¾å¿œã‚’ç¢ºèª
class_names = train_dataset.classes_
print(f"å…¨ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¯ãƒ©ã‚¹æ•°: {len(set(val_targets))}")
print(f"äºˆæ¸¬ã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¯ãƒ©ã‚¹æ•°: {len(set(val_preds))}")

# å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã®ã¿ã‚’å–å¾—
unique_labels = sorted(set(val_targets) | set(val_preds))
actual_class_names = [class_names[i] for i in unique_labels]

print(f"å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹: {actual_class_names}")

# åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã®ã¿ï¼‰
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(
    val_targets, 
    val_preds, 
    labels=unique_labels,
    target_names=actual_class_names, 
    zero_division=0
))

plt.tight_layout()
plt.show()

# %%
# ç°¡æ˜“ãƒ†ã‚¹ãƒˆè©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜
# =============================================================================

# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
os.makedirs(save_dir, exist_ok=True)

print("=== ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜ã‚’é–‹å§‹ ===")

# 1. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
model_save_path = os.path.join(save_dir, "best_model.pth")
save_data = {
    'model_state_dict': best_model_state,
    'model_config': model_config,
    'mutation_vocab': mutation_vocab,
    'protein_vocab': protein_vocab,
    'label_encoder': train_dataset.protein_to_idx,
    'num_classes': train_dataset.num_classes,
    'max_length': train_dataset.max_length,
    'model_type': 'pretrained_bert' if (hasattr(model, 'use_pretrained') and model.use_pretrained) else 'original_bert'
}

# äº‹å‰å­¦ç¿’æ¸ˆã¿BERTã®å ´åˆã¯è¿½åŠ æƒ…å ±ã‚’ä¿å­˜
if hasattr(model, 'use_pretrained') and model.use_pretrained:
    save_data['bert_model_name'] = model_config['bert_model_name']
    save_data['freeze_layers'] = model_config['freeze_bert_layers']

torch.save(save_data, model_save_path)
print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_save_path}")

# 2. è¨“ç·´å±¥æ­´ã®ä¿å­˜
import json

results = {
    'train_losses': train_losses,
    'val_losses': val_losses,
    'train_accs': train_accs,
    'val_accs': val_accs,
    'best_val_acc': best_val_acc,
    'total_training_time': total_training_time,
    'model_config': model_config,
    'data_config': data_config,
    'dataset_config': dataset_config
}

results_path = os.path.join(save_dir, "training_results.json")
with open(results_path, 'w') as f:
    json.dump(results, f, indent=2)
print(f"è¨“ç·´çµæœä¿å­˜å®Œäº†: {results_path}")

# 3. è¨“ç·´ã‚°ãƒ©ãƒ•ã®ä¿å­˜
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.tight_layout()
plot_path = os.path.join(save_dir, "training_history.png")
plt.savefig(plot_path, dpi=300, bbox_inches='tight')
plt.show()
print(f"è¨“ç·´ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†: {plot_path}")

print(f"\n=== å…¨ã¦ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ ===")
print(f"ä¿å­˜å…ˆ: {os.path.abspath(save_dir)}")
print(f"æœ€è‰¯æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f}")
print(f"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {model_config['num_epochs']}")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(mutation_vocab):,}")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")

print("\n=== å®Ÿé¨“å®Œäº† ===")

# %%
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡æ©Ÿèƒ½ã®è¿½åŠ 
# =============================================================================

def evaluate_test_data_timestep(model, test_x, test_y, mutation_vocab, protein_vocab, train_dataset, device):
    """
    ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡
    train4ã®evaluate_test_data_timestepã¨åŒæ§˜ã®è¨ˆç®—æ–¹æ³•ã‚’å®Ÿè£…
    
    Args:
        model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        test_x: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å…¥åŠ›ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—è¾æ›¸ï¼‰
        test_y: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—è¾æ›¸ï¼‰
        mutation_vocab: å¤‰ç•°èªå½™è¾æ›¸
        protein_vocab: ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³èªå½™è¾æ›¸
        train_dataset: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¯ãƒ©ã‚¹æƒ…å ±å«ã‚€ï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹
    """
    print("\n=== ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ†ã‚¹ãƒˆè©•ä¾¡ ===")
    
    timestep_results = {}
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’å–å¾—
    train_classes = set(train_dataset.classes_)
    train_protein_to_idx = train_dataset.protein_to_idx
    
    # ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°
    def extract_protein_from_test_labels(test_labels, codon_df, bunpu_df):
        proteins = []
        for mutations in test_labels:
            seq_proteins = []
            for mutation in mutations:
                _, _, protein, _, _ = gfea.Mutation_features(mutation, codon_df, bunpu_df)
                seq_proteins.append(protein)
            proteins.append(seq_proteins)
        return proteins
    
    for timestep in sorted(test_x.keys()):
        print(f"\nã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— {timestep} ã®è©•ä¾¡ä¸­...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        test_sequences = test_x[timestep]
        test_labels = test_y[timestep]
        
        if len(test_sequences) == 0:
            print(f"  ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— {timestep}: ãƒ‡ãƒ¼ã‚¿ãªã—")
            continue
        
        # ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³åã‚’æŠ½å‡º
        test_y_protein = extract_protein_from_test_labels(test_labels, codon_df, bunpu_df)
        test_x_expanded, test_y_expanded = mds.add_x_by_y(test_sequences, test_y_protein)
        
        # æœªçŸ¥ã®ã‚¯ãƒ©ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_x = []
        filtered_y = []
        filtered_protein_labels = []
        
        for i, (x, y, orig_label) in enumerate(zip(test_x_expanded, test_y_expanded, test_y_protein)):
            label = y[0] if isinstance(y, list) and len(y) > 0 else y
            if str(label) in train_classes:
                filtered_x.append(x)
                filtered_y.append(y)
                if i < len(test_y_protein):
                    filtered_protein_labels.append(orig_label)
        
        if len(filtered_x) == 0:
            print(f"  ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— {timestep}: æ—¢çŸ¥ã®ã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")
            continue
        
        print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(test_x_expanded)} -> {len(filtered_x)} ã‚µãƒ³ãƒ—ãƒ«")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        test_dataset = MutationBERTDataset(
            filtered_x, 
            filtered_y, 
            mutation_vocab, 
            protein_vocab, 
            train_dataset.max_length
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        
        # è©•ä¾¡å®Ÿè¡Œ
        model.eval()
        total_loss = 0.0
        all_predictions = []
        all_targets = []
        
        criterion = torch.nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs  # PretrainedBERTModelã¯æ—¢ã«logitsã‚’è¿”ã—ã¦ã„ã‚‹
                
                loss = criterion(logits, labels)
                total_loss += loss.item()
                
                predictions = torch.argmax(logits, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(labels.cpu().numpy())
        
        test_loss = total_loss / len(test_loader)
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å˜ä½ã§ã®äºˆæ¸¬ã¨çœŸã®ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™
        predictions_per_sequence = []
        true_labels_per_sequence = []
        
        for seq_idx, orig_labels in enumerate(filtered_protein_labels):
            if isinstance(orig_labels, list):
                true_labels_set = set(orig_labels)
            else:
                true_labels_set = {orig_labels}
            
            true_labels_per_sequence.append(true_labels_set)
            
            # å¯¾å¿œã™ã‚‹äºˆæ¸¬ã‚’å–å¾—ï¼ˆç¯„å›²ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if seq_idx < len(all_predictions):
                pred_idx = all_predictions[seq_idx]
                if 0 <= pred_idx < len(train_dataset.classes_):
                    pred_class_name = train_dataset.classes_[pred_idx]
                    predictions_per_sequence.append([pred_class_name])
                else:
                    print(f"è­¦å‘Š: äºˆæ¸¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {pred_idx} ãŒç¯„å›²å¤– (0-{len(train_dataset.classes_)-1})")
                    predictions_per_sequence.append(['<UNK>'])
            else:
                predictions_per_sequence.append(['<UNK>'])
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å˜ä½ã®ç²¾åº¦è¨ˆç®—ï¼ˆ2ã¤ã®æŒ‡æ¨™ï¼‰
        strict_sequence_accuracy = calculate_strict_sequence_accuracy(
            predictions_per_sequence, true_labels_per_sequence
        )
        flexible_sequence_accuracy = calculate_flexible_sequence_accuracy(
            predictions_per_sequence, true_labels_per_sequence
        )
        
        # çµæœã‚’ä¿å­˜
        timestep_results[timestep] = {
            'samples': len(test_sequences),
            'expanded_samples': len(test_x_expanded),
            'filtered_samples': len(filtered_x),
            'loss': test_loss,
            'strict_sequence_accuracy': strict_sequence_accuracy,
            'flexible_sequence_accuracy': flexible_sequence_accuracy,
            'predictions': all_predictions,
            'targets': all_targets,
            'original_labels': filtered_protein_labels,
            'predictions_per_sequence': predictions_per_sequence,
            'true_labels_per_sequence': [list(labels) for labels in true_labels_per_sequence]
        }
        
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(test_sequences)} (å±•é–‹å¾Œ: {len(test_x_expanded)}, ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_x)})")
        print(f"  Loss: {test_loss:.4f}")
        print(f"  å³å¯†ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç²¾åº¦: {strict_sequence_accuracy:.4f}")
        print(f"  æŸ”è»Ÿã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç²¾åº¦: {flexible_sequence_accuracy:.4f}")
    
    return timestep_results

def calculate_strict_sequence_accuracy(predictions_per_sequence, true_labels_per_sequence):
    """
    å³å¯†æ­£è§£ç‡ï¼šå˜ä¸€ãƒ©ãƒ™ãƒ«ã®å ´åˆã®ã¿æ­£è§£ã€è¤‡æ•°ãƒ©ãƒ™ãƒ«ã¯å¿…ãšä¸æ­£è§£
    """
    correct_sequences = 0
    total_sequences = len(true_labels_per_sequence)
    
    for pred_list, true_set in zip(predictions_per_sequence, true_labels_per_sequence):
        pred = pred_list[0]  # å˜ä¸€äºˆæ¸¬
        
        # çœŸã®ãƒ©ãƒ™ãƒ«ãŒ1ã¤ã®å ´åˆã®ã¿æ­£è§£ã®å¯èƒ½æ€§ã‚ã‚Š
        if len(true_set) == 1 and pred in true_set:
            correct_sequences += 1
        # è¤‡æ•°ãƒ©ãƒ™ãƒ«ã®å ´åˆã¯å¿…ãšä¸æ­£è§£
    
    return correct_sequences / total_sequences if total_sequences > 0 else 0.0

def calculate_flexible_sequence_accuracy(predictions_per_sequence, true_labels_per_sequence):
    """
    æŸ”è»Ÿæ­£è§£ç‡ï¼šäºˆæ¸¬ãŒçœŸã®ãƒ©ãƒ™ãƒ«ã®ã„ãšã‚Œã‹ã«å«ã¾ã‚Œã¦ã„ã‚Œã°æ­£è§£
    """
    correct_sequences = 0
    total_sequences = len(true_labels_per_sequence)
    
    for pred_list, true_set in zip(predictions_per_sequence, true_labels_per_sequence):
        pred = pred_list[0]  # å˜ä¸€äºˆæ¸¬
        
        # äºˆæ¸¬ãŒçœŸã®ãƒ©ãƒ™ãƒ«ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚Œã°æ­£è§£
        if pred in true_set:
            correct_sequences += 1
    
    return correct_sequences / total_sequences if total_sequences > 0 else 0.0

def report_timestep_results(timestep_results):
    """
    ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    """
    print("\n=== ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ ===")
    
    total_samples = 0
    total_filtered = 0
    weighted_strict_acc = 0.0
    weighted_flexible_acc = 0.0
    
    print(f"{'Timestep':<10} {'Samples':<8} {'Filtered':<8} {'Loss':<8} {'Strict Acc':<12} {'Flexible Acc':<12}")
    print("-" * 70)
    
    for timestep in sorted(timestep_results.keys()):
        result = timestep_results[timestep]
        samples = result['filtered_samples']
        loss = result['loss']
        strict_acc = result['strict_sequence_accuracy']
        flexible_acc = result['flexible_sequence_accuracy']
        
        print(f"{timestep:<10} {result['samples']:<8} {samples:<8} {loss:<8.4f} {strict_acc:<12.4f} {flexible_acc:<12.4f}")
        
        total_samples += result['samples']
        total_filtered += samples
        weighted_strict_acc += strict_acc * samples
        weighted_flexible_acc += flexible_acc * samples
    
    if total_filtered > 0:
        weighted_strict_acc /= total_filtered
        weighted_flexible_acc /= total_filtered
    
    print("-" * 70)
    print(f"{'Total':<10} {total_samples:<8} {total_filtered:<8} {'---':<8} {weighted_strict_acc:<12.4f} {weighted_flexible_acc:<12.4f}")
    print()
    print(f"å…¨ä½“åŠ é‡å¹³å‡ - å³å¯†ç²¾åº¦: {weighted_strict_acc:.4f}, æŸ”è»Ÿç²¾åº¦: {weighted_flexible_acc:.4f}")

# %%
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# =============================================================================

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã®å®Ÿè¡Œ
print("\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã®å®Ÿè¡Œ ===")

# ãƒ¢ãƒ‡ãƒ«ã‚’æœ€è‰¯ã®é‡ã¿ã«å¾©å…ƒ
model.load_state_dict(best_model_state)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡
timestep_results = evaluate_test_data_timestep(
    model=model,
    test_x=test_x,
    test_y=test_y,
    mutation_vocab=mutation_vocab,
    protein_vocab=protein_vocab,
    train_dataset=train_dataset,
    device=device
)

# çµæœãƒ¬ãƒãƒ¼ãƒˆ
report_timestep_results(timestep_results)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£è§£ç‡ã®å¯è¦–åŒ–
def plot_test_accuracy_by_timestep(timestep_results, save_dir):
    """
    ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£è§£ç‡ã‚’ã‚°ãƒ©ãƒ•åŒ–
    """
    if not timestep_results:
        print("ãƒ†ã‚¹ãƒˆçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    timesteps = sorted(timestep_results.keys())
    strict_accuracies = []
    flexible_accuracies = []
    sample_counts = []
    filtered_counts = []
    
    for ts in timesteps:
        result = timestep_results[ts]
        strict_accuracies.append(result['strict_sequence_accuracy'])
        flexible_accuracies.append(result['flexible_sequence_accuracy'])
        sample_counts.append(result['samples'])
        filtered_counts.append(result['filtered_samples'])
    
    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. æ­£è§£ç‡ã®æ¨ç§»
    ax1.plot(timesteps, strict_accuracies, 'o-', label='Strict Accuracy', color='red', linewidth=2)
    ax1.plot(timesteps, flexible_accuracies, 's-', label='Flexible Accuracy', color='blue', linewidth=2)
    ax1.set_xlabel('Timestep')
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Test Accuracy by Timestep')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # 2. ã‚µãƒ³ãƒ—ãƒ«æ•°ã®æ¨ç§»
    ax2.bar(timesteps, sample_counts, alpha=0.7, label='Original Samples', color='lightblue')
    ax2.bar(timesteps, filtered_counts, alpha=0.9, label='Filtered Samples', color='darkblue')
    ax2.set_xlabel('Timestep')
    ax2.set_ylabel('Sample Count')
    ax2.set_title('Sample Count by Timestep')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ­£è§£ç‡ã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã®ç›¸é–¢ï¼ˆå³å¯†æ­£è§£ç‡ï¼‰
    ax3.scatter(filtered_counts, strict_accuracies, alpha=0.7, s=60, color='red')
    ax3.set_xlabel('Filtered Sample Count')
    ax3.set_ylabel('Strict Accuracy')
    ax3.set_title('Sample Count vs Strict Accuracy')
    ax3.grid(True, alpha=0.3)
    
    # å„ç‚¹ã«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    for i, ts in enumerate(timesteps):
        ax3.annotate(f'T{ts}', (filtered_counts[i], strict_accuracies[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    # 4. æ­£è§£ç‡ã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã®ç›¸é–¢ï¼ˆæŸ”è»Ÿæ­£è§£ç‡ï¼‰
    ax4.scatter(filtered_counts, flexible_accuracies, alpha=0.7, s=60, color='blue')
    ax4.set_xlabel('Filtered Sample Count')
    ax4.set_ylabel('Flexible Accuracy')
    ax4.set_title('Sample Count vs Flexible Accuracy')
    ax4.grid(True, alpha=0.3)
    
    # å„ç‚¹ã«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    for i, ts in enumerate(timesteps):
        ax4.annotate(f'T{ts}', (filtered_counts[i], flexible_accuracies[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.tight_layout()
    
    # ã‚°ãƒ©ãƒ•ã®ä¿å­˜
    test_plot_path = os.path.join(save_dir, "test_accuracy_by_timestep.png")
    plt.savefig(test_plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ãƒ†ã‚¹ãƒˆæ­£è§£ç‡ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†: {test_plot_path}")
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print(f"\n=== ãƒ†ã‚¹ãƒˆæ­£è§£ç‡çµ±è¨ˆ ===")
    print(f"å³å¯†æ­£è§£ç‡ - æœ€é«˜: {max(strict_accuracies):.4f}, æœ€ä½: {min(strict_accuracies):.4f}, å¹³å‡: {np.mean(strict_accuracies):.4f}")
    print(f"æŸ”è»Ÿæ­£è§£ç‡ - æœ€é«˜: {max(flexible_accuracies):.4f}, æœ€ä½: {min(flexible_accuracies):.4f}, å¹³å‡: {np.mean(flexible_accuracies):.4f}")
    print(f"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(timesteps)}")
    print(f"ç·ãƒ•ã‚£ãƒ«ã‚¿å¾Œã‚µãƒ³ãƒ—ãƒ«æ•°: {sum(filtered_counts)}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£è§£ç‡ã®å¯è¦–åŒ–ã‚’å®Ÿè¡Œ
plot_test_accuracy_by_timestep(timestep_results, save_dir)

# ===== train4ã¨åŒç­‰ã®ä¿å­˜æ©Ÿèƒ½ã‚’è¿½åŠ  =====

# çµæœã®ä¿å­˜ï¼ˆè©³ç´°ç‰ˆï¼‰
import json
import pickle

# JSONä¿å­˜ç”¨ã«numpyé…åˆ—ã‚’å¤‰æ›
for timestep in timestep_results:
    result = timestep_results[timestep]
    if 'predictions' in result:
        result['predictions'] = [int(x) for x in result['predictions']]
    if 'targets' in result:
        result['targets'] = [int(x) for x in result['targets']]

test_results_path = os.path.join(save_dir, "test_results_timestep.json")
with open(test_results_path, 'w') as f:
    json.dump(timestep_results, f, indent=2, ensure_ascii=False)
print(f"ãƒ†ã‚¹ãƒˆçµæœä¿å­˜å®Œäº†: {test_results_path}")

# 4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ (config.json)
config_data = {
    'model_config': model_config,
    'data_config': data_config,
    'dataset_config': dataset_config,
    'feature_mask': None,  # BERTé¢¨ãƒ¢ãƒ‡ãƒ«ã§ã¯ä½¿ç”¨ã—ãªã„
    'model_type': 'bert_style',
    'mutation_vocab_size': len(mutation_vocab),
    'protein_vocab_size': len(protein_vocab),
    'num_classes': train_dataset.num_classes,
    'max_length': train_dataset.max_length,
    'class_names': train_dataset.classes_,
    'training_statistics': {
        'best_val_acc': best_val_acc,
        'total_training_time': total_training_time,
        'final_train_acc': train_accs[-1] if train_accs else 0,
        'final_val_acc': val_accs[-1] if val_accs else 0,
        'epochs_completed': len(train_accs)
    }
}

config_path = os.path.join(save_dir, "config.json")
with open(config_path, 'w') as f:
    json.dump(config_data, f, indent=2, ensure_ascii=False)
print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {config_path}")

# 5. èªå½™è¾æ›¸ã®ä¿å­˜ (vocabularies.pkl)
vocab_data = {
    'mutation_vocab': mutation_vocab,
    'protein_vocab': protein_vocab,
    'label_encoder': train_dataset.protein_to_idx,
    'reverse_label_encoder': {v: k for k, v in train_dataset.protein_to_idx.items()},
    'class_names': train_dataset.classes_,
    'vocab_type': 'bert_style'
}

vocab_path = os.path.join(save_dir, "vocabularies.pkl")
with open(vocab_path, 'wb') as f:
    pickle.dump(vocab_data, f)
print(f"èªå½™è¾æ›¸ä¿å­˜å®Œäº†: {vocab_path}")

# 6. ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ä¿å­˜ (label_encoder.pkl) 
label_encoder_path = os.path.join(save_dir, "label_encoder.pkl")
with open(label_encoder_path, 'wb') as f:
    pickle.dump(train_dataset.protein_to_idx, f)
print(f"ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜å®Œäº†: {label_encoder_path}")

# 7. README.mdã®ç”Ÿæˆã¨ä¿å­˜
# f-stringç”¨ã®å€¤ã‚’äº‹å‰è¨ˆç®—
final_train_acc = train_accs[-1] if train_accs else 0.0
final_val_acc = val_accs[-1] if val_accs else 0.0

readme_content = f"""# BERTé¢¨å¤‰ç•°äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å®Ÿé¨“çµæœ

## å®Ÿé¨“æ¦‚è¦
- **å®Ÿé¨“æ—¥æ™‚**: {current_time}
- **ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—**: {'äº‹å‰å­¦ç¿’æ¸ˆã¿BERT' if (hasattr(model, 'use_pretrained') and model.use_pretrained) else 'ã‚ªãƒªã‚¸ãƒŠãƒ«BERT'}
- **å¯¾è±¡å¤‰ç•°æ ª**: {', '.join(dataset_config['strains'])}
- **äºˆæ¸¬å¯¾è±¡**: ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³å¤‰ç•° (36ã‚¯ãƒ©ã‚¹)

## ãƒ¢ãƒ‡ãƒ«è¨­å®š
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: BERTé¢¨Transformer
- **èªå½™ã‚µã‚¤ã‚º**: {len(mutation_vocab):,} (å¤‰ç•°èªå½™)
- **d_model**: {model_config['d_model']}
- **num_heads**: {model_config['nhead']}
- **num_layers**: {model_config['num_layers']}
- **æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·**: {train_dataset.max_length}

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: {len(train_dataset):,} ã‚µãƒ³ãƒ—ãƒ«
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: {len(val_dataset):,} ã‚µãƒ³ãƒ—ãƒ«  
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: {len(test_x)} ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
- **ã‚¯ãƒ©ã‚¹æ•°**: {train_dataset.num_classes}

## è¨“ç·´çµæœ
- **ã‚¨ãƒãƒƒã‚¯æ•°**: {model_config['num_epochs']}
- **æœ€è‰¯æ¤œè¨¼ç²¾åº¦**: {best_val_acc:.4f}
- **æœ€çµ‚è¨“ç·´ç²¾åº¦**: {final_train_acc:.4f}
- **æœ€çµ‚æ¤œè¨¼ç²¾åº¦**: {final_val_acc:.4f}
- **ç·è¨“ç·´æ™‚é–“**: {total_training_time/60:.1f}åˆ†

## ãƒ†ã‚¹ãƒˆè©•ä¾¡çµæœ
"""

# ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
if timestep_results:
    total_samples = sum(result['samples'] for result in timestep_results.values())
    total_filtered = sum(result['filtered_samples'] for result in timestep_results.values())
    
    if total_filtered > 0:
        weighted_strict = sum(result['strict_sequence_accuracy'] * result['filtered_samples'] 
                             for result in timestep_results.values()) / total_filtered
        weighted_flexible = sum(result['flexible_sequence_accuracy'] * result['filtered_samples'] 
                               for result in timestep_results.values()) / total_filtered
        
        readme_content += f"""
- **è©•ä¾¡ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°**: {len(timestep_results)}
- **ç·ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°**: {total_samples:,} â†’ {total_filtered:,} (ãƒ•ã‚£ãƒ«ã‚¿å¾Œ)
- **å…¨ä½“å³å¯†æ­£è§£ç‡**: {weighted_strict:.4f}
- **å…¨ä½“æŸ”è»Ÿæ­£è§£ç‡**: {weighted_flexible:.4f}
"""

readme_content += f"""

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
- `best_model.pth`: æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿
- `training_results.json`: è¨“ç·´å±¥æ­´ãƒ‡ãƒ¼ã‚¿
- `training_history.png`: è¨“ç·´ãƒ»æ¤œè¨¼ã®Loss/Accuracyæ¨ç§»
- `test_results_timestep.json`: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ãƒ†ã‚¹ãƒˆçµæœ
- `test_accuracy_by_timestep.png`: ãƒ†ã‚¹ãƒˆæ­£è§£ç‡ã®å¯è¦–åŒ–
- `config.json`: å®Ÿé¨“è¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `vocabularies.pkl`: èªå½™è¾æ›¸
- `label_encoder.pkl`: ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

## å®Ÿé¨“ã®ç‰¹å¾´
- **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å¤‰ç•°ã‚’è‡ªç„¶è¨€èªã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã†BERTé¢¨ãƒ¢ãƒ‡ãƒ«
- **èªå½™æ§‹ç¯‰**: å¡©åŸºå¤‰ç•°ãƒ‘ã‚¿ãƒ¼ãƒ³ (ä¾‹: A1234C, G5678T)
- **ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³**: <PAD>, <UNK>, <CLS>, <SEP>
- **åˆ†é¡æ–¹æ³•**: CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”¨ã„ãŸåˆ†é¡

## å¾“æ¥ãƒ¢ãƒ‡ãƒ«ã¨ã®é•ã„
- **å¾“æ¥**: å¤šæ¬¡å…ƒç‰¹å¾´é‡ (8ç¨®é¡) â†’ æ§‹é€ åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **BERTé¢¨**: å˜ä¸€å¤‰ç•°èªå½™ â†’ è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **ãƒ¡ãƒ¢ãƒª**: ã‚ˆã‚Šå¤§ããªèªå½™ã‚µã‚¤ã‚º ({len(mutation_vocab):,} vs æ•°åƒ)
- **è§£é‡ˆæ€§**: æš—é»™çš„ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ vs æ˜ç¤ºçš„ç”Ÿç‰©å­¦çš„ç‰¹å¾´
"""

readme_path = os.path.join(save_dir, "README.md")
with open(readme_path, 'w', encoding='utf-8') as f:
    f.write(readme_content)
print(f"READMEä¿å­˜å®Œäº†: {readme_path}")

# 8. å®Ÿé¨“ã‚µãƒãƒªãƒ¼ã®ä¿å­˜ (experiment_summary.json)
summary_data = {
    'experiment_name': current_time,
    'model_type': 'bert_style_transformer',
    'dataset': {
        'strains': dataset_config['strains'],
        'train_samples': len(train_dataset),
        'val_samples': len(val_dataset),
        'test_timesteps': len(test_x),
        'num_classes': train_dataset.num_classes,
        'class_names': train_dataset.classes_
    },
    'model_architecture': {
        'type': 'original_bert' if not (hasattr(model, 'use_pretrained') and model.use_pretrained) else 'pretrained_bert',
        'vocab_size': len(mutation_vocab),
        'd_model': model_config['d_model'],
        'nhead': model_config['nhead'],
        'num_layers': model_config['num_layers'],
        'max_seq_length': train_dataset.max_length,
        'total_parameters': sum(p.numel() for p in model.parameters()),
        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)
    },
    'training_config': {
        'epochs': model_config['num_epochs'],
        'batch_size': model_config['batch_size'],
        'learning_rate': model_config['learning_rate'],
        'weight_decay': model_config['weight_decay']
    },
    'results': {
        'best_val_accuracy': best_val_acc,
        'final_train_accuracy': train_accs[-1] if train_accs else 0,
        'final_val_accuracy': val_accs[-1] if val_accs else 0,
        'training_time_minutes': total_training_time / 60,
        'epochs_completed': len(train_accs)
    }
}

# ãƒ†ã‚¹ãƒˆçµæœã‚’è¿½åŠ 
if timestep_results:
    total_samples = sum(result['samples'] for result in timestep_results.values())
    total_filtered = sum(result['filtered_samples'] for result in timestep_results.values())
    
    if total_filtered > 0:
        weighted_strict = sum(result['strict_sequence_accuracy'] * result['filtered_samples'] 
                             for result in timestep_results.values()) / total_filtered
        weighted_flexible = sum(result['flexible_sequence_accuracy'] * result['filtered_samples'] 
                               for result in timestep_results.values()) / total_filtered
        
        summary_data['test_results'] = {
            'evaluated_timesteps': len(timestep_results),
            'total_test_samples': total_samples,
            'filtered_test_samples': total_filtered,
            'overall_strict_accuracy': weighted_strict,
            'overall_flexible_accuracy': weighted_flexible,
            'per_timestep_results': {
                str(ts): {
                    'samples': result['samples'],
                    'filtered_samples': result['filtered_samples'],
                    'strict_accuracy': result['strict_sequence_accuracy'],
                    'flexible_accuracy': result['flexible_sequence_accuracy']
                }
                for ts, result in timestep_results.items()
            }
        }

summary_path = os.path.join(save_dir, "experiment_summary.json")
with open(summary_path, 'w') as f:
    json.dump(summary_data, f, indent=2, ensure_ascii=False)
print(f"å®Ÿé¨“ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: {summary_path}")

print(f"\n=== train4ã¨åŒç­‰ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ ===")
print(f"ä¿å­˜å…ˆ: {os.path.abspath(save_dir)}")

# æœ€çµ‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
if os.path.exists(save_dir):
    files = os.listdir(save_dir)
    print(f"ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
    print("ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    for file in sorted(files):
        file_path = os.path.join(save_dir, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            print(f"  {file}: {size:,} bytes")
    
    # ä¿å­˜å®Œäº†ã®ç¢ºèª
    expected_files = [
        "best_model.pth", "training_results.json", "training_history.png", 
        "test_results_timestep.json", "test_accuracy_by_timestep.png",
        "config.json", "vocabularies.pkl", "label_encoder.pkl", "README.md", 
        "experiment_summary.json"
    ]
    
    missing_files = [f for f in expected_files if f not in files]
    if missing_files:
        print(f"\nè­¦å‘Š: ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_files}")
    else:
        print(f"\nâœ… å…¨ã¦ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ (train4ã¨åŒç­‰)")
else:
    print(f"âŒ ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {save_dir}")

print("\n=== å®Ÿé¨“å®Œäº† ===")
print(f"å®Ÿé¨“å: {current_time}")
print(f"å¯¾è±¡å¤‰ç•°æ ª: {dataset_config['strains']}")
print(f"æœ€è‰¯æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f}")
print(f"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {model_config['num_epochs']}")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(mutation_vocab):,}")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")

# %%
