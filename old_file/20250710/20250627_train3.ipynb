{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144613a3",
   "metadata": {},
   "source": [
    "# COVID-19 Mutation Prediction with Modular Package (train3)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€COVID-19å¤‰ç•°äºˆæ¸¬ã®ãŸã‚ã®é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ ã§å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ä¸»ãªç‰¹å¾´\n",
    "\n",
    "### ğŸ“¦ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "- **`covid_mutation_prediction`** ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«ã‚ˆã‚‹å†åˆ©ç”¨å¯èƒ½ãªå®Ÿè£…\n",
    "- æ˜ç¢ºã«åˆ†é›¢ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ãƒ¢ãƒ‡ãƒ«ã€è¨“ç·´ã€è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "- è¨­å®šãƒ™ãƒ¼ã‚¹ã®æŸ”è»Ÿãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†\n",
    "\n",
    "### ğŸš€ é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•\n",
    "- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: Optuna ã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–\n",
    "- **é«˜åº¦ãªTransformerãƒ¢ãƒ‡ãƒ«**: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "- **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ**: Focal Lossã€é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "- **æ­£å‰‡åŒ–æŠ€è¡“**: Dropoutã€Label Smoothingã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "### ğŸ“Š åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ \n",
    "- **å¤šæ¬¡å…ƒè©•ä¾¡æŒ‡æ¨™**: F1-Macro/Microã€ç²¾åº¦ã€å†ç¾ç‡ã€AUC\n",
    "- **æ™‚ç³»åˆ—åˆ†æ**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã®è©³ç´°ãªæ€§èƒ½è©•ä¾¡\n",
    "- **å¯è¦–åŒ–**: è¨“ç·´éç¨‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°ãªãƒ—ãƒ­ãƒƒãƒˆ\n",
    "\n",
    "### âš¡ ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š\n",
    "- **tqdmé€²æ—ãƒãƒ¼**: è¨“ç·´ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è©³ç´°ãªé€²æ—è¡¨ç¤º\n",
    "- **è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ä¿å­˜\n",
    "- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½\n",
    "\n",
    "## ğŸ“ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ \n",
    "\n",
    "```\n",
    "covid_mutation_prediction/\n",
    "â”œâ”€â”€ config/          # è¨­å®šç®¡ç†\n",
    "â”œâ”€â”€ data/            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "â”œâ”€â”€ models/          # Transformerãƒ¢ãƒ‡ãƒ«ã¨æå¤±é–¢æ•°\n",
    "â”œâ”€â”€ training/        # è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "â”œâ”€â”€ evaluation/      # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "â”œâ”€â”€ ensemble/        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "â”œâ”€â”€ optimization/    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "â””â”€â”€ utils/           # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°\n",
    "```\n",
    "\n",
    "## ğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n",
    "2. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: ãƒãƒ«ãƒãƒ›ãƒƒãƒˆç‰¹å¾´é‡ã®æ•°å€¤åŒ–\n",
    "3. **ãƒ¢ãƒ‡ãƒ«è¨“ç·´**: é€²æ—ãƒãƒ¼ä»˜ãé«˜åº¦è¨“ç·´ãƒ«ãƒ¼ãƒ—\n",
    "4. **æ€§èƒ½è©•ä¾¡**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã®è©³ç´°åˆ†æ\n",
    "5. **çµæœå¯è¦–åŒ–**: åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¯è¦–åŒ–\n",
    "\n",
    "æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è©³ç´°ãªå®Ÿè£…ã‚’é–‹å§‹ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c375e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA RTX A4000\n",
      "âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ‡ãƒã‚¤ã‚¹è¨­å®šå®Œäº†!\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, average_precision_score, classification_report,\n",
    "    hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "import math\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ‡ãƒã‚¤ã‚¹è¨­å®šå®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854100f6",
   "metadata": {},
   "source": [
    "## ğŸ“Š 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€COVID-19å¤‰ç•°ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹\n",
    "- **å¤‰ç•°ãƒ‘ã‚¹**: B.1.1.7æ ªã®USHERå‡ºåŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "- **åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿**: `table_heatmap/250621/table_set/table_set.csv`\n",
    "- **ã‚³ãƒ‰ãƒ³æƒ…å ±**: `meta_data/codon_mutation4.csv`\n",
    "\n",
    "### ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³åˆ†é¡\n",
    "36ã®ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ï¼ˆnon-codingé ˜åŸŸã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é ˜åŸŸã‚’å«ã‚€ï¼‰ã‚’å¯¾è±¡ã¨ã—ã¦ã€å¤‰ç•°ã®ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4182286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein classes: 36\n",
      "Protein names: ['non_coding1', 'nsp1', 'nsp2', 'nsp3', 'nsp4', 'nsp5', 'nsp6', 'nsp7', 'nsp8', 'nsp9', 'nsp10', 'nsp12', 'nsp13', 'nsp14', 'nsp15', 'nsp16', 'non_coding2', 'S', 'non_coding3', 'ORF3a', 'non_coding4', 'E', 'non_coding5', 'M', 'non_coding6', 'ORF6', 'non_coding7', 'ORF7a', 'ORF7b', 'non_coding8', 'ORF8', 'non_coding9', 'N', 'non_coding10', 'ORF10', 'non_coding11']\n",
      "Loading data...\n",
      "[INFO] import: /mnt/ssd1/home3/aiba/usher_output/B.1.1.7/0/mutation_paths_B.1.1.7.tsv\n",
      "[INFO] æŒ‡å®šã•ã‚ŒãŸnmax=10000ã«é”ã—ã¾ã—ãŸã€‚\n",
      "[INFO] B.1.1.7ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: 10000 ã‚µãƒ³ãƒ—ãƒ«\n",
      "[INFO] èª­ã¿è¾¼ã¿å®Œäº†: 10000 ã‚µãƒ³ãƒ—ãƒ«\n",
      "Loaded 10000 mutation paths\n",
      "Bunpu data shape: (30000, 13)\n",
      "Codon data shape: (30000, 10)\n",
      "Extracting features...\n",
      "Feature data extracted for 10000 sequences\n",
      "Feature data extracted for 10000 sequences\n"
     ]
    }
   ],
   "source": [
    "# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import importlib\n",
    "import module.input_mutation_path as imp\n",
    "import module.get_feature as gfea\n",
    "importlib.reload(imp)\n",
    "importlib.reload(gfea)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\n",
    "strains = ['B.1.1.7']  # ä½¿ç”¨ã™ã‚‹æ ªå\n",
    "usher_dir = '~/usher_output/'\n",
    "nmax = 10000\n",
    "nmax_per_strain = 100000000000000000\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³åã®å®šç¾©\n",
    "protein_name = [\n",
    "    \"non_coding1\", \"nsp1\", \"nsp2\", \"nsp3\", \"nsp4\", \"nsp5\", \"nsp6\", \"nsp7\", \"nsp8\", \"nsp9\", \"nsp10\",\n",
    "    \"nsp12\", \"nsp13\", \"nsp14\", \"nsp15\", \"nsp16\", \"non_coding2\", \"S\", \"non_coding3\", \"ORF3a\", \n",
    "    \"non_coding4\", \"E\", \"non_coding5\", \"M\", \"non_coding6\", \"ORF6\", \"non_coding7\", \"ORF7a\", \n",
    "    \"ORF7b\", \"non_coding8\", \"ORF8\", \"non_coding9\", \"N\", \"non_coding10\", \"ORF10\", \"non_coding11\"\n",
    "]\n",
    "\n",
    "print(f\"Number of protein classes: {len(protein_name)}\")\n",
    "print(f\"Protein names: {protein_name}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "print(\"Loading data...\")\n",
    "names, lengths, base_HGVS_paths = imp.input(strains, usher_dir, nmax=nmax, nmax_per_strain=nmax_per_strain)\n",
    "\n",
    "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "bunpu_df = pd.read_csv(\"table_heatmap/250621/table_set/table_set.csv\")\n",
    "codon_df = pd.read_csv('meta_data/codon_mutation4.csv')\n",
    "\n",
    "print(f\"Loaded {len(base_HGVS_paths)} mutation paths\")\n",
    "print(f\"Bunpu data shape: {bunpu_df.shape}\")\n",
    "print(f\"Codon data shape: {codon_df.shape}\")\n",
    "\n",
    "# ç‰¹å¾´é‡ã®æŠ½å‡º\n",
    "print(\"Extracting features...\")\n",
    "data = gfea.Feature_path_incl_ts(base_HGVS_paths, codon_df, bunpu_df)\n",
    "print(f\"Feature data extracted for {len(data)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e11641",
   "metadata": {},
   "source": [
    "## ğŸ”„ 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ä¿®æ­£ã•ã‚ŒãŸæ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã¨ç‰¹å¾´é‡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "### ğŸ“Š ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥\n",
    "\n",
    "#### å¾“æ¥ã®åˆ†å‰²æ–¹å¼ï¼ˆä¿®æ­£å‰ï¼‰\n",
    "- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30\n",
    "- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31-35  \n",
    "- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 36ä»¥é™\n",
    "\n",
    "#### æ–°ã—ã„åˆ†å‰²æ–¹å¼ï¼ˆä¿®æ­£å¾Œï¼‰\n",
    "- **è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30 ã‹ã‚‰ **åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**ã‚’ä½¿ç”¨\n",
    "- **ãƒ‡ãƒ¼ã‚¿åˆ†å‰²**: è¨“ç·´80% : æ¤œè¨¼20% ã®æ¯”ç‡ã§ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²\n",
    "- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
    "\n",
    "### ğŸ¯ ä¿®æ­£ã®ç›®çš„ã¨åˆ©ç‚¹\n",
    "\n",
    "1. **ã‚ˆã‚Šå…¬å¹³ãªè©•ä¾¡**: è¨“ç·´ã¨æ¤œè¨¼ãŒåŒã˜æ™‚æœŸã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æ™‚ç³»åˆ—ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›\n",
    "2. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®å‘ä¸Š**: åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ´»ç”¨\n",
    "3. **æ±åŒ–æ€§èƒ½ã®å‘ä¸Š**: åŒä¸€æœŸé–“å†…ã§ã®ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã«ã‚ˆã‚Šã€éå­¦ç¿’ã‚’æŠ‘åˆ¶\n",
    "4. **å®Ÿç”¨æ€§ã®å‘ä¸Š**: ã‚ˆã‚Šç¾å®Ÿçš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œ\n",
    "\n",
    "### ğŸ”¢ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "- 9æ¬¡å…ƒç‰¹å¾´é‡ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã€å¡©åŸºå¤‰ç•°ã€ä½ç½®æƒ…å ±ãªã©ï¼‰ã‚’æ•°å€¤ã«å¤‰æ›\n",
    "- ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "- ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹ç³»åˆ—é•·ã®çµ±ä¸€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a55ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ...\n",
      "Warning: Some modules could not be imported: cannot import name 'HyperparameterOptimizer' from 'covid_mutation_prediction.optimization.hyperparameter_tuning' (/mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/optimization/hyperparameter_tuning.py)\n",
      "Please ensure all dependencies are installed.\n",
      "âœ… ä¸»è¦ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
      "  - ModelConfig, TrainingConfig, EvaluationConfig\n",
      "  - AdvancedMutationTransformer\n",
      "  - AdvancedMutationDataset\n",
      "  - FocalLoss\n",
      "ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ...\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥ï¼ˆä¿®æ­£ç‰ˆï¼‰:\n",
      "  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ« (80%)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ« (20%)\n",
      "  - å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - å®Ÿéš›ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Test time steps: 15\n",
      "Extracting protein features...\n",
      "Creating feature vocabularies...\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥ï¼ˆä¿®æ­£ç‰ˆï¼‰:\n",
      "  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ« (80%)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ« (20%)\n",
      "  - å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - å®Ÿéš›ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Test time steps: 15\n",
      "Extracting protein features...\n",
      "Creating feature vocabularies...\n",
      "Number of features: 9\n",
      "Feature 0 (timestep) vocabulary size: 32\n",
      "Feature 1 (base_mut) vocabulary size: 14\n",
      "Feature 2 (base_pos) vocabulary size: 5586\n",
      "Feature 3 (amino_mut) vocabulary size: 175\n",
      "Feature 4 (amino_pos) vocabulary size: 1381\n",
      "Feature 5 (amino_flag) vocabulary size: 4\n",
      "Feature 6 (protein) vocabulary size: 35\n",
      "Feature 7 (codon_pos) vocabulary size: 6\n",
      "Feature 8 (freq) vocabulary size: 485\n",
      "Max sequence length: 57 -> Using: 57\n",
      "Encoding data...\n",
      "Number of features: 9\n",
      "Feature 0 (timestep) vocabulary size: 32\n",
      "Feature 1 (base_mut) vocabulary size: 14\n",
      "Feature 2 (base_pos) vocabulary size: 5586\n",
      "Feature 3 (amino_mut) vocabulary size: 175\n",
      "Feature 4 (amino_pos) vocabulary size: 1381\n",
      "Feature 5 (amino_flag) vocabulary size: 4\n",
      "Feature 6 (protein) vocabulary size: 35\n",
      "Feature 7 (codon_pos) vocabulary size: 6\n",
      "Feature 8 (freq) vocabulary size: 485\n",
      "Max sequence length: 57 -> Using: 57\n",
      "Encoding data...\n",
      "Encoded data shapes:\n",
      "- Train X: (8000, 9, 57), Train Y: (8000, 36)\n",
      "- Val X: (2000, 9, 57), Val Y: (2000, 36)\n",
      "- Test datasets: 15\n",
      "\n",
      "Data Statistics:\n",
      "Train - Avg labels per sample: 1.25\n",
      "Val - Avg labels per sample: 1.25\n",
      "Train - Most frequent protein: nsp3 (1632.0 samples)\n",
      "Val - Most frequent protein: nsp3 (429.0 samples)\n",
      "Train - Zero frequency proteins: 1\n",
      "Val - Zero frequency proteins: 5\n",
      "Encoded data shapes:\n",
      "- Train X: (8000, 9, 57), Train Y: (8000, 36)\n",
      "- Val X: (2000, 9, 57), Val Y: (2000, 36)\n",
      "- Test datasets: 15\n",
      "\n",
      "Data Statistics:\n",
      "Train - Avg labels per sample: 1.25\n",
      "Val - Avg labels per sample: 1.25\n",
      "Train - Most frequent protein: nsp3 (1632.0 samples)\n",
      "Val - Most frequent protein: nsp3 (429.0 samples)\n",
      "Train - Zero frequency proteins: 1\n",
      "Val - Zero frequency proteins: 5\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®åˆæœŸåŒ–\n",
    "print(\"ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ...\")\n",
    "\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹ã®å¼·åˆ¶è¿½åŠ \n",
    "import sys\n",
    "from pathlib import Path\n",
    "package_path = Path('/mnt/ssd1/home3/aiba/gmp')\n",
    "if str(package_path) not in sys.path:\n",
    "    sys.path.insert(0, str(package_path))\n",
    "    print(f\"âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹è¿½åŠ : {package_path}\")\n",
    "\n",
    "# å¼·åˆ¶çš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆè©¦è¡Œ\n",
    "force_import_success = False\n",
    "try:\n",
    "    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸè¨­å®šã‚¯ãƒ©ã‚¹\n",
    "    from covid_mutation_prediction.config.settings import ModelConfig, TrainingConfig, EvaluationConfig\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    from covid_mutation_prediction.models.transformer import AdvancedMutationTransformer\n",
    "    from covid_mutation_prediction.data.dataset import AdvancedMutationDataset\n",
    "    from covid_mutation_prediction.models.losses import FocalLoss\n",
    "    \n",
    "    print(\"âœ… ä¸»è¦ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "    print(\"  - ModelConfig, TrainingConfig, EvaluationConfig\")\n",
    "    print(\"  - AdvancedMutationTransformer\")\n",
    "    print(\"  - AdvancedMutationDataset\") \n",
    "    print(\"  - FocalLoss\")\n",
    "    \n",
    "    force_import_success = True\n",
    "    use_modular = True\n",
    "    full_modular = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}\")\n",
    "    print(\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "    use_modular = False\n",
    "    full_modular = False\n",
    "\n",
    "# è¨­å®šã‚¯ãƒ©ã‚¹ã®å®šç¾©ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "if not use_modular:\n",
    "    print(\"ğŸ“¦ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ä¸­...\")\n",
    "    \n",
    "    @dataclass\n",
    "    class ModelConfig:\n",
    "        d_model: int = 256\n",
    "        nhead: int = 8\n",
    "        num_layers: int = 3\n",
    "        dropout: float = 0.2\n",
    "        max_seq_length: int = 100\n",
    "        embedding_dims: List[int] = field(default_factory=lambda: [32, 32, 32, 32, 32, 32, 32, 32, 32])\n",
    "    \n",
    "    @dataclass\n",
    "    class TrainingConfig:\n",
    "        batch_size: int = 32\n",
    "        learning_rate: float = 1e-3\n",
    "        num_epochs: int = 50\n",
    "        patience: int = 10\n",
    "        seed: int = 42\n",
    "        weight_decay: float = 0.01\n",
    "    \n",
    "    @dataclass\n",
    "    class EvaluationConfig:\n",
    "        f1_macro_weight: float = 0.3\n",
    "        f1_micro_weight: float = 0.2\n",
    "        precision_weight: float = 0.2\n",
    "        recall_weight: float = 0.15\n",
    "        auc_weight: float = 0.1\n",
    "        hamming_weight: float = 0.05\n",
    "\n",
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µï¼ˆprotein_nameã«å¯¾å¿œï¼‰\n",
    "class NotebookDataProcessor:\n",
    "    \"\"\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µï¼ˆprotein_nameãƒªã‚¹ãƒˆã«å¯¾å¿œï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, protein_names):\n",
    "        self.protein_names = protein_names\n",
    "        self.feature_names = ['timestep', 'base_mut', 'base_pos', 'amino_mut', 'amino_pos', \n",
    "                             'amino_flag', 'protein', 'codon_pos', 'freq']\n",
    "    \n",
    "    def extract_keys_in_range(self, data, start_key, end_key):\n",
    "        filtered_values = []\n",
    "        for key, value in data.items():\n",
    "            if start_key <= key <= end_key:\n",
    "                for v in value:\n",
    "                    filtered_values.append(v)\n",
    "        return filtered_values\n",
    "    \n",
    "    def create_time_aware_split_modified(self, data, train_end=30, test_start=31, ylen=1, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼šè¨“ç·´ãƒ»æ¤œè¨¼ã‚’åŒã˜ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ç¯„å›²ï¼ˆ1-30ï¼‰ã‹ã‚‰ä½œæˆã—ã€31ä»¥é™ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨\n",
    "        \n",
    "        Args:\n",
    "            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "            train_end: è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®çµ‚äº†ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰\n",
    "            test_start: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 31ï¼‰\n",
    "            ylen: äºˆæ¸¬æœŸé–“ã®é•·ã•\n",
    "            val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.2 = 20%ï¼‰\n",
    "        \"\"\"\n",
    "        train_x, train_y = [], []\n",
    "        val_x, val_y = [], []\n",
    "        test_x, test_y = {}, {}\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30 ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†\n",
    "        timestep_1_30_data = []\n",
    "        \n",
    "        for d in data:\n",
    "            seq_len = len(d)\n",
    "            \n",
    "            if seq_len >= test_start:  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¾ã§å«ã‚€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "                # 1-30ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†\n",
    "                seq_1_30 = self.extract_keys_in_range(d, 1, train_end)\n",
    "                if len(seq_1_30) > ylen:  # æœ€ä½é™ã®é•·ã•ãŒã‚ã‚‹å ´åˆã®ã¿\n",
    "                    timestep_1_30_data.append(seq_1_30)\n",
    "                \n",
    "                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ31ä»¥é™ï¼‰\n",
    "                for i in range(test_start, seq_len + 1 - ylen + 1):\n",
    "                    if i not in test_x:\n",
    "                        test_x[i] = []\n",
    "                        test_y[i] = []\n",
    "                    test_x[i].append(self.extract_keys_in_range(d, i - (test_start - train_end), i - 1))\n",
    "                    test_y[i].append(self.extract_keys_in_range(d, i, i + ylen - 1))\n",
    "            \n",
    "            elif seq_len > ylen:  # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰\n",
    "                seq_data = self.extract_keys_in_range(d, 1, seq_len - ylen)\n",
    "                seq_label = self.extract_keys_in_range(d, seq_len - ylen + 1, seq_len)\n",
    "                if len(seq_data) > 0:\n",
    "                    timestep_1_30_data.append((seq_data, seq_label))\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30 ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ»æ¤œè¨¼ã«åˆ†å‰²\n",
    "        import random\n",
    "        random.shuffle(timestep_1_30_data)\n",
    "        \n",
    "        val_size = int(len(timestep_1_30_data) * val_ratio)\n",
    "        train_size = len(timestep_1_30_data) - val_size\n",
    "        \n",
    "        print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥ï¼ˆä¿®æ­£ç‰ˆï¼‰:\")\n",
    "        print(f\"  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-{train_end}\")\n",
    "        print(f\"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— {test_start}ä»¥é™\")\n",
    "        print(f\"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_size}ã‚µãƒ³ãƒ—ãƒ« ({(1-val_ratio)*100:.0f}%)\")\n",
    "        print(f\"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_size}ã‚µãƒ³ãƒ—ãƒ« ({val_ratio*100:.0f}%)\")\n",
    "        \n",
    "        # åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨“ç·´ãƒ»æ¤œè¨¼ã‚’åˆ†å‰²\n",
    "        for i, item in enumerate(timestep_1_30_data):\n",
    "            if isinstance(item, tuple):  # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "                seq_data, seq_label = item\n",
    "            else:  # é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "                seq_data = item[:-ylen] if len(item) > ylen else item\n",
    "                seq_label = item[-ylen:] if len(item) > ylen else [item[-1]] if item else []\n",
    "            \n",
    "            if i < train_size:\n",
    "                train_x.append(seq_data)\n",
    "                train_y.append(seq_label)\n",
    "            else:\n",
    "                val_x.append(seq_data)\n",
    "                val_y.append(seq_label)\n",
    "        \n",
    "        print(f\"  - å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_x)}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "        print(f\"  - å®Ÿéš›ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_x)}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "        print(f\"  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: {sorted(test_x.keys()) if test_x else 'ãªã—'}\")\n",
    "        \n",
    "        return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "    \n",
    "    def extract_protein_features(self, paths, protein_idx=6):\n",
    "        features = []\n",
    "        for path in paths:\n",
    "            feature = [0] * len(self.protein_names)\n",
    "            for mutation in path:\n",
    "                if len(mutation) > protein_idx:\n",
    "                    protein = mutation[protein_idx]\n",
    "                    if protein in self.protein_names:\n",
    "                        feature[self.protein_names.index(protein)] = 1\n",
    "            features.append(feature)\n",
    "        return features\n",
    "    \n",
    "    def create_feature_vocabularies(self, train_data):\n",
    "        feature_sets = [set() for _ in range(9)]\n",
    "        \n",
    "        for sequence in train_data:\n",
    "            for mutation in sequence:\n",
    "                if isinstance(mutation, (list, tuple)) and len(mutation) >= 9:\n",
    "                    for i in range(9):\n",
    "                        feature_sets[i].add(str(mutation[i]))\n",
    "        \n",
    "        feature_vocabs = []\n",
    "        for i, feature_set in enumerate(feature_sets):\n",
    "            vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "            for j, value in enumerate(sorted(feature_set)):\n",
    "                vocab[value] = j + 2\n",
    "            feature_vocabs.append(vocab)\n",
    "        \n",
    "        return feature_vocabs\n",
    "    \n",
    "    def encode_sequences_by_features(self, sequences, feature_vocabs, max_length=None):\n",
    "        if max_length is None:\n",
    "            max_length = max(len(seq) for seq in sequences) if sequences else 1\n",
    "        \n",
    "        num_features = len(feature_vocabs)\n",
    "        encoded_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            encoded_features = [[] for _ in range(num_features)]\n",
    "            \n",
    "            for mutation in seq:\n",
    "                if isinstance(mutation, (list, tuple)) and len(mutation) >= num_features:\n",
    "                    for i in range(num_features):\n",
    "                        feature_value = str(mutation[i])\n",
    "                        encoded_value = feature_vocabs[i].get(feature_value, feature_vocabs[i]['<UNK>'])\n",
    "                        encoded_features[i].append(encoded_value)\n",
    "                else:\n",
    "                    for i in range(num_features):\n",
    "                        encoded_features[i].append(feature_vocabs[i]['<UNK>'])\n",
    "            \n",
    "            padded_features = []\n",
    "            for feature_seq in encoded_features:\n",
    "                if len(feature_seq) > max_length:\n",
    "                    feature_seq = feature_seq[:max_length]\n",
    "                else:\n",
    "                    pad_value = feature_vocabs[0]['<PAD>']\n",
    "                    feature_seq = feature_seq + [pad_value] * (max_length - len(feature_seq))\n",
    "                padded_features.append(feature_seq)\n",
    "            \n",
    "            encoded_sequences.append(padded_features)\n",
    "        \n",
    "        return np.array(encoded_sequences), max_length\n",
    "\n",
    "# è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–\n",
    "model_config = ModelConfig()\n",
    "training_config = TrainingConfig()\n",
    "evaluation_config = EvaluationConfig()\n",
    "\n",
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨\n",
    "data_processor = NotebookDataProcessor(protein_name)\n",
    "\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆtrain/validation/testã®3åˆ†å‰²ï¼‰\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = data_processor.create_time_aware_split_modified(\n",
    "    data, train_end=30, test_start=31, ylen=1\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_x)}\")\n",
    "print(f\"Validation samples: {len(val_x)}\")\n",
    "print(f\"Test time steps: {len(test_x)}\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º\n",
    "print(\"Extracting protein features...\")\n",
    "train_y_protein = data_processor.extract_protein_features(train_y)\n",
    "val_y_protein = data_processor.extract_protein_features(val_y)\n",
    "\n",
    "test_y_protein = {}\n",
    "for timestep in test_y.keys():\n",
    "    test_y_protein[timestep] = data_processor.extract_protein_features(test_y[timestep])\n",
    "\n",
    "# èªå½™è¾æ›¸ã®ä½œæˆ\n",
    "print(\"Creating feature vocabularies...\")\n",
    "feature_vocabs = data_processor.create_feature_vocabularies(train_x)\n",
    "data_processor.feature_vocabs = feature_vocabs\n",
    "\n",
    "print(f\"Number of features: {len(feature_vocabs)}\")\n",
    "for i, (vocab, name) in enumerate(zip(feature_vocabs, data_processor.feature_names)):\n",
    "    print(f\"Feature {i} ({name}) vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§é•·ã‚’æ±ºå®š\n",
    "all_sequences = train_x + val_x\n",
    "for test_sequences in test_x.values():\n",
    "    all_sequences.extend(test_sequences)\n",
    "\n",
    "max_seq_length = max(len(seq) for seq in all_sequences) if all_sequences else 1\n",
    "data_processor.max_seq_length = min(max_seq_length, model_config.max_seq_length)  # è¨­å®šã•ã‚ŒãŸæœ€å¤§é•·ã§åˆ¶é™\n",
    "\n",
    "print(f\"Max sequence length: {max_seq_length} -> Using: {data_processor.max_seq_length}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "print(\"Encoding data...\")\n",
    "train_x_encoded, _ = data_processor.encode_sequences_by_features(train_x, feature_vocabs, data_processor.max_seq_length)\n",
    "val_x_encoded, _ = data_processor.encode_sequences_by_features(val_x, feature_vocabs, data_processor.max_seq_length)\n",
    "\n",
    "train_y_encoded = np.array(train_y_protein, dtype=np.float32)\n",
    "val_y_encoded = np.array(val_y_protein, dtype=np.float32)\n",
    "\n",
    "test_x_encoded = {}\n",
    "test_y_encoded = {}\n",
    "for timestep in test_x.keys():\n",
    "    test_x_encoded[timestep], _ = data_processor.encode_sequences_by_features(\n",
    "        test_x[timestep], feature_vocabs, data_processor.max_seq_length\n",
    "    )\n",
    "    test_y_encoded[timestep] = np.array(test_y_protein[timestep], dtype=np.float32)\n",
    "\n",
    "print(f\"Encoded data shapes:\")\n",
    "print(f\"- Train X: {train_x_encoded.shape}, Train Y: {train_y_encoded.shape}\")\n",
    "print(f\"- Val X: {val_x_encoded.shape}, Val Y: {val_y_encoded.shape}\")\n",
    "print(f\"- Test datasets: {len(test_x_encoded)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®è¡¨ç¤º\n",
    "labels_per_sample_train = np.sum(train_y_encoded, axis=1)\n",
    "labels_per_sample_val = np.sum(val_y_encoded, axis=1)\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Train - Avg labels per sample: {np.mean(labels_per_sample_train):.2f}\")\n",
    "print(f\"Val - Avg labels per sample: {np.mean(labels_per_sample_val):.2f}\")\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹é »åº¦ã®ç¢ºèª\n",
    "class_frequencies_train = np.sum(train_y_encoded, axis=0)\n",
    "class_frequencies_val = np.sum(val_y_encoded, axis=0)\n",
    "\n",
    "print(f\"Train - Most frequent protein: {protein_name[np.argmax(class_frequencies_train)]} ({np.max(class_frequencies_train)} samples)\")\n",
    "print(f\"Val - Most frequent protein: {protein_name[np.argmax(class_frequencies_val)]} ({np.max(class_frequencies_val)} samples)\")\n",
    "print(f\"Train - Zero frequency proteins: {np.sum(class_frequencies_train == 0)}\")\n",
    "print(f\"Val - Zero frequency proteins: {np.sum(class_frequencies_val == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8634ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’å®Ÿè¡Œä¸­...\n",
      "ğŸ“‹ æ–°ã—ã„åˆ†å‰²æˆ¦ç•¥:\n",
      "  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30 ã‹ã‚‰åˆ†å‰²ï¼ˆ8:2ã®æ¯”ç‡ï¼‰\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥ï¼ˆä¿®æ­£ç‰ˆï¼‰:\n",
      "  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ« (80%)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ« (20%)\n",
      "  - å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - å®Ÿéš›ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "\n",
      "ğŸ“Š åˆ†å‰²çµæœ:\n",
      "  - è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: 8000\n",
      "  - æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: 2000\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: 15\n",
      "  - ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ç·æ•°: 3860\n",
      "  - ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°: {31: 1658, 32: 1040, 33: 576, 34: 287, 35: 139, 36: 62, 37: 34, 38: 24, 39: 15, 40: 9, 41: 5, 42: 5, 43: 4, 44: 1, 45: 1}\n",
      "\n",
      "ğŸ§¬ ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º...\n",
      "\n",
      "ğŸ“š ç‰¹å¾´é‡èªå½™è¾æ›¸ã®ä½œæˆ...\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥ï¼ˆä¿®æ­£ç‰ˆï¼‰:\n",
      "  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ« (80%)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ« (20%)\n",
      "  - å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 8000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - å®Ÿéš›ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 2000ã‚µãƒ³ãƒ—ãƒ«\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "\n",
      "ğŸ“Š åˆ†å‰²çµæœ:\n",
      "  - è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: 8000\n",
      "  - æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: 2000\n",
      "  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: 15\n",
      "  - ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ç·æ•°: 3860\n",
      "  - ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°: {31: 1658, 32: 1040, 33: 576, 34: 287, 35: 139, 36: 62, 37: 34, 38: 24, 39: 15, 40: 9, 41: 5, 42: 5, 43: 4, 44: 1, 45: 1}\n",
      "\n",
      "ğŸ§¬ ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º...\n",
      "\n",
      "ğŸ“š ç‰¹å¾´é‡èªå½™è¾æ›¸ã®ä½œæˆ...\n",
      "ç‰¹å¾´é‡æ•°: 9\n",
      "  ç‰¹å¾´é‡ 0 (timestep): èªå½™ã‚µã‚¤ã‚º 32\n",
      "  ç‰¹å¾´é‡ 1 (base_mut): èªå½™ã‚µã‚¤ã‚º 14\n",
      "  ç‰¹å¾´é‡ 2 (base_pos): èªå½™ã‚µã‚¤ã‚º 5573\n",
      "  ç‰¹å¾´é‡ 3 (amino_mut): èªå½™ã‚µã‚¤ã‚º 177\n",
      "  ç‰¹å¾´é‡ 4 (amino_pos): èªå½™ã‚µã‚¤ã‚º 1379\n",
      "  ç‰¹å¾´é‡ 5 (amino_flag): èªå½™ã‚µã‚¤ã‚º 4\n",
      "  ç‰¹å¾´é‡ 6 (protein): èªå½™ã‚µã‚¤ã‚º 35\n",
      "  ç‰¹å¾´é‡ 7 (codon_pos): èªå½™ã‚µã‚¤ã‚º 6\n",
      "  ç‰¹å¾´é‡ 8 (freq): èªå½™ã‚µã‚¤ã‚º 488\n",
      "\n",
      "ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: æœ€å¤§ 57 â†’ ä½¿ç”¨ 57\n",
      "\n",
      "ğŸ”¢ ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°...\n",
      "ç‰¹å¾´é‡æ•°: 9\n",
      "  ç‰¹å¾´é‡ 0 (timestep): èªå½™ã‚µã‚¤ã‚º 32\n",
      "  ç‰¹å¾´é‡ 1 (base_mut): èªå½™ã‚µã‚¤ã‚º 14\n",
      "  ç‰¹å¾´é‡ 2 (base_pos): èªå½™ã‚µã‚¤ã‚º 5573\n",
      "  ç‰¹å¾´é‡ 3 (amino_mut): èªå½™ã‚µã‚¤ã‚º 177\n",
      "  ç‰¹å¾´é‡ 4 (amino_pos): èªå½™ã‚µã‚¤ã‚º 1379\n",
      "  ç‰¹å¾´é‡ 5 (amino_flag): èªå½™ã‚µã‚¤ã‚º 4\n",
      "  ç‰¹å¾´é‡ 6 (protein): èªå½™ã‚µã‚¤ã‚º 35\n",
      "  ç‰¹å¾´é‡ 7 (codon_pos): èªå½™ã‚µã‚¤ã‚º 6\n",
      "  ç‰¹å¾´é‡ 8 (freq): èªå½™ã‚µã‚¤ã‚º 488\n",
      "\n",
      "ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: æœ€å¤§ 57 â†’ ä½¿ç”¨ 57\n",
      "\n",
      "ğŸ”¢ ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°...\n",
      "\n",
      "ğŸ“ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:\n",
      "  - è¨“ç·´ X: (8000, 9, 57), è¨“ç·´ Y: (8000, 36)\n",
      "  - æ¤œè¨¼ X: (2000, 9, 57), æ¤œè¨¼ Y: (2000, 36)\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: 15\n",
      "\n",
      "ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° 1.25\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° 1.25\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: nsp3 (1641.0 ã‚µãƒ³ãƒ—ãƒ«)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: nsp3 (420.0 ã‚µãƒ³ãƒ—ãƒ«)\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: 1\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: 4\n",
      "\n",
      "ğŸ·ï¸ åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: 36\n",
      "âœ… ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†!\n",
      "\n",
      "ğŸ“ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:\n",
      "  - è¨“ç·´ X: (8000, 9, 57), è¨“ç·´ Y: (8000, 36)\n",
      "  - æ¤œè¨¼ X: (2000, 9, 57), æ¤œè¨¼ Y: (2000, 36)\n",
      "  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: 15\n",
      "\n",
      "ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° 1.25\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° 1.25\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: nsp3 (1641.0 ã‚µãƒ³ãƒ—ãƒ«)\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: nsp3 (420.0 ã‚µãƒ³ãƒ—ãƒ«)\n",
      "  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: 1\n",
      "  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: 4\n",
      "\n",
      "ğŸ·ï¸ åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: 36\n",
      "âœ… ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ\n",
    "\n",
    "# è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–\n",
    "model_config = ModelConfig()\n",
    "training_config = TrainingConfig()\n",
    "evaluation_config = EvaluationConfig()\n",
    "\n",
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨\n",
    "data_processor = NotebookDataProcessor(protein_name)\n",
    "\n",
    "print(\"ğŸ”„ ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’å®Ÿè¡Œä¸­...\")\n",
    "print(\"ğŸ“‹ æ–°ã—ã„åˆ†å‰²æˆ¦ç•¥:\")\n",
    "print(\"  - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 1-30 ã‹ã‚‰åˆ†å‰²ï¼ˆ8:2ã®æ¯”ç‡ï¼‰\")\n",
    "print(\"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— 31ä»¥é™\")\n",
    "\n",
    "# ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’ä½¿ç”¨\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = data_processor.create_time_aware_split_modified(\n",
    "    data, train_end=30, test_start=31, ylen=1, val_ratio=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š åˆ†å‰²çµæœ:\")\n",
    "print(f\"  - è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_x)}\")\n",
    "print(f\"  - æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(val_x)}\")\n",
    "print(f\"  - ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(test_x)}\")\n",
    "if test_x:\n",
    "    test_sample_counts = [len(samples) for samples in test_x.values()]\n",
    "    print(f\"  - ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ç·æ•°: {sum(test_sample_counts)}\")\n",
    "    print(f\"  - ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°: {dict(zip(sorted(test_x.keys()), test_sample_counts))}\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º\n",
    "print(\"\\nğŸ§¬ ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º...\")\n",
    "train_y_protein = data_processor.extract_protein_features(train_y)\n",
    "val_y_protein = data_processor.extract_protein_features(val_y)\n",
    "\n",
    "test_y_protein = {}\n",
    "for timestep in test_y.keys():\n",
    "    test_y_protein[timestep] = data_processor.extract_protein_features(test_y[timestep])\n",
    "\n",
    "# èªå½™è¾æ›¸ã®ä½œæˆ\n",
    "print(\"\\nğŸ“š ç‰¹å¾´é‡èªå½™è¾æ›¸ã®ä½œæˆ...\")\n",
    "feature_vocabs = data_processor.create_feature_vocabularies(train_x)\n",
    "data_processor.feature_vocabs = feature_vocabs\n",
    "\n",
    "print(f\"ç‰¹å¾´é‡æ•°: {len(feature_vocabs)}\")\n",
    "feature_vocab_sizes = [len(vocab) for vocab in feature_vocabs]\n",
    "for i, (vocab_size, name) in enumerate(zip(feature_vocab_sizes, data_processor.feature_names)):\n",
    "    print(f\"  ç‰¹å¾´é‡ {i} ({name}): èªå½™ã‚µã‚¤ã‚º {vocab_size}\")\n",
    "\n",
    "# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§é•·ã‚’æ±ºå®š\n",
    "all_sequences = train_x + val_x\n",
    "for test_sequences in test_x.values():\n",
    "    all_sequences.extend(test_sequences)\n",
    "\n",
    "max_seq_length = max(len(seq) for seq in all_sequences) if all_sequences else 1\n",
    "data_processor.max_seq_length = min(max_seq_length, model_config.max_seq_length)\n",
    "\n",
    "print(f\"\\nğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: æœ€å¤§ {max_seq_length} â†’ ä½¿ç”¨ {data_processor.max_seq_length}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "print(\"\\nğŸ”¢ ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°...\")\n",
    "train_x_encoded, _ = data_processor.encode_sequences_by_features(train_x, feature_vocabs, data_processor.max_seq_length)\n",
    "val_x_encoded, _ = data_processor.encode_sequences_by_features(val_x, feature_vocabs, data_processor.max_seq_length)\n",
    "\n",
    "train_y_encoded = np.array(train_y_protein, dtype=np.float32)\n",
    "val_y_encoded = np.array(val_y_protein, dtype=np.float32)\n",
    "\n",
    "test_x_encoded = {}\n",
    "test_y_encoded = {}\n",
    "for timestep in test_x.keys():\n",
    "    test_x_encoded[timestep], _ = data_processor.encode_sequences_by_features(\n",
    "        test_x[timestep], feature_vocabs, data_processor.max_seq_length\n",
    "    )\n",
    "    test_y_encoded[timestep] = np.array(test_y_protein[timestep], dtype=np.float32)\n",
    "\n",
    "print(f\"\\nğŸ“ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:\")\n",
    "print(f\"  - è¨“ç·´ X: {train_x_encoded.shape}, è¨“ç·´ Y: {train_y_encoded.shape}\")\n",
    "print(f\"  - æ¤œè¨¼ X: {val_x_encoded.shape}, æ¤œè¨¼ Y: {val_y_encoded.shape}\")\n",
    "print(f\"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(test_x_encoded)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®è¡¨ç¤º\n",
    "labels_per_sample_train = np.sum(train_y_encoded, axis=1)\n",
    "labels_per_sample_val = np.sum(val_y_encoded, axis=1)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\")\n",
    "print(f\"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° {np.mean(labels_per_sample_train):.2f}\")\n",
    "print(f\"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Šå¹³å‡ãƒ©ãƒ™ãƒ«æ•° {np.mean(labels_per_sample_val):.2f}\")\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹é »åº¦ã®ç¢ºèª\n",
    "class_frequencies_train = np.sum(train_y_encoded, axis=0)\n",
    "class_frequencies_val = np.sum(val_y_encoded, axis=0)\n",
    "\n",
    "print(f\"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: {protein_name[np.argmax(class_frequencies_train)]} ({np.max(class_frequencies_train)} ã‚µãƒ³ãƒ—ãƒ«)\")\n",
    "print(f\"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æœ€é »ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³: {protein_name[np.argmax(class_frequencies_val)]} ({np.max(class_frequencies_val)} ã‚µãƒ³ãƒ—ãƒ«)\")\n",
    "print(f\"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: {np.sum(class_frequencies_train == 0)}\")\n",
    "print(f\"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚¼ãƒ­é »åº¦ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³æ•°: {np.sum(class_frequencies_val == 0)}\")\n",
    "\n",
    "num_classes = len(protein_name)\n",
    "print(f\"\\nğŸ·ï¸ åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {num_classes}\")\n",
    "print(\"âœ… ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f876b35",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®åˆæœŸåŒ–\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€`covid_mutation_prediction` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n",
    "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¦å‹•ä½œã‚’ç¶™ç¶šã—ã¾ã™ã€‚\n",
    "\n",
    "### ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡\n",
    "- **è¨­å®šã‚¯ãƒ©ã‚¹**: `ModelConfig`, `TrainingConfig`, `EvaluationConfig`\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: `AdvancedMutationTransformer`\n",
    "- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: `AdvancedMutationDataset`\n",
    "- **æå¤±é–¢æ•°**: `FocalLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8cfefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è¨“ç·´é–‹å§‹ ===\n",
      "âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’è¿½åŠ : /mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction\n",
      "âœ… è¨­å®šã‚¯ãƒ©ã‚¹ã®å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
      "âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
      "âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
      "âœ… è¨­å®šã‚¯ãƒ©ã‚¹ã‚’æ›´æ–°ã—ã¾ã—ãŸ\n",
      "ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆä¸­...\n",
      "ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®AdvancedMutationDatasetã‚’ä½¿ç”¨\n",
      "âœ… ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæˆåŠŸ\n",
      "ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...\n",
      "ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
      "âœ… AdvancedMutationTransformer ã‚’ä½œæˆã—ã¾ã—ãŸ\n",
      "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 4,753,454\n",
      "âœ… AdvancedMutationTransformer ã‚’ä½œæˆã—ã¾ã—ãŸ\n",
      "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 4,753,454\n",
      "è¨“ç·´è¨­å®š:\n",
      "- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32\n",
      "- å­¦ç¿’ç‡: 0.0001\n",
      "- ã‚¨ãƒãƒƒã‚¯æ•°: 100\n",
      "- ã‚·ãƒ¼ãƒ‰å€¤: 42\n",
      "- ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
      "- ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½¿ç”¨: True\n",
      "- å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼æ©Ÿèƒ½: True\n",
      "- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: AdvancedMutationTransformer\n",
      "\n",
      "âœ… è¨“ç·´ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "è¨“ç·´è¨­å®š:\n",
      "- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32\n",
      "- å­¦ç¿’ç‡: 0.0001\n",
      "- ã‚¨ãƒãƒƒã‚¯æ•°: 100\n",
      "- ã‚·ãƒ¼ãƒ‰å€¤: 42\n",
      "- ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
      "- ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½¿ç”¨: True\n",
      "- å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼æ©Ÿèƒ½: True\n",
      "- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: AdvancedMutationTransformer\n",
      "\n",
      "âœ… è¨“ç·´ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ä½œæˆã¨è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "print(\"\\n=== ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è¨“ç·´é–‹å§‹ ===\")\n",
    "\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å­˜åœ¨ç¢ºèªã¨å¼·åˆ¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆè©¦è¡Œ\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹ã®ç¢ºèªã¨è¿½åŠ \n",
    "package_path = Path(\"/mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction\")\n",
    "if package_path.exists():\n",
    "    if str(package_path.parent) not in sys.path:\n",
    "        sys.path.insert(0, str(package_path.parent))\n",
    "    print(f\"âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’è¿½åŠ : {package_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {package_path}\")\n",
    "\n",
    "# å¼·åˆ¶çš„ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆè©¦è¡Œ\n",
    "force_import_success = False\n",
    "try:\n",
    "    # è¨­å®šã‚¯ãƒ©ã‚¹ã®å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from covid_mutation_prediction.config.settings import ModelConfig, TrainingConfig, EvaluationConfig\n",
    "    print(\"âœ… è¨­å®šã‚¯ãƒ©ã‚¹ã®å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from covid_mutation_prediction.models.transformer import AdvancedMutationTransformer\n",
    "    from covid_mutation_prediction.models.losses import FocalLoss, AsymmetricLoss, LabelSmoothingBCELoss\n",
    "    from covid_mutation_prediction.data.dataset import AdvancedMutationDataset\n",
    "    print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "    \n",
    "    force_import_success = True\n",
    "    use_modular = True\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    try:\n",
    "        from covid_mutation_prediction.training.pipeline import ImprovedTrainingPipeline\n",
    "        from covid_mutation_prediction.evaluation.metrics import CompositeEvaluator\n",
    "        print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "        full_modular = True\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ ä¸€éƒ¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}\")\n",
    "        print(\"âš ï¸ åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "        full_modular = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¼·åˆ¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã‚‚ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "    force_import_success = False\n",
    "    use_modular = False\n",
    "    full_modular = False\n",
    "\n",
    "# æ—¢å­˜ã®å¤‰æ•°ã‚’æ›´æ–°\n",
    "if force_import_success:\n",
    "    # è¨­å®šã‚¯ãƒ©ã‚¹ã‚’å†ä½œæˆï¼ˆæœ€æ–°ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ï¼‰\n",
    "    model_config = ModelConfig()\n",
    "    training_config = TrainingConfig()\n",
    "    evaluation_config = EvaluationConfig()\n",
    "    print(\"âœ… è¨­å®šã‚¯ãƒ©ã‚¹ã‚’æ›´æ–°ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "if not use_modular:\n",
    "    print(\"ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’æº–å‚™ä¸­...\")\n",
    "    \n",
    "    # æå¤±é–¢æ•°ã®ç°¡æ˜“å®Ÿè£…\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=0.25, gamma=2.0):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "        \n",
    "        def forward(self, inputs, targets):\n",
    "            ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "            return focal_loss.mean()\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®ç°¡æ˜“å®Ÿè£…\n",
    "    class AdvancedMutationDataset(Dataset):\n",
    "        def __init__(self, features, labels, **kwargs):\n",
    "            # ç‰¹å¾´é‡ã¯æ•´æ•°å‹ï¼ˆembeddingç”¨ï¼‰ã€ãƒ©ãƒ™ãƒ«ã¯æµ®å‹•å°æ•°ç‚¹å‹\n",
    "            self.features = torch.tensor(features, dtype=torch.long)\n",
    "            self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    print(\"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’æº–å‚™å®Œäº†\")\n",
    "\n",
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«å®Ÿè£…\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, config, feature_vocab_sizes, num_classes):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # ç°¡æ˜“ç‰ˆã®å®Ÿè£…\n",
    "        embedding_dim = 32\n",
    "        self.feature_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "            for vocab_size in feature_vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        total_embedding_dim = embedding_dim * len(feature_vocab_sizes)\n",
    "        self.feature_projection = nn.Linear(total_embedding_dim, config.d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dim_feedforward=config.d_model * 4,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(config.d_model),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_model, config.d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout * 0.5),\n",
    "            nn.Linear(config.d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features, seq_len = x.shape\n",
    "        \n",
    "        # Feature embeddings\n",
    "        embedded_features = []\n",
    "        for i, embedding_layer in enumerate(self.feature_embeddings):\n",
    "            emb = embedding_layer(x[:, i, :])\n",
    "            embedded_features.append(emb)\n",
    "        \n",
    "        # ç‰¹å¾´çµåˆã¨æŠ•å½±\n",
    "        combined_embeddings = torch.cat(embedded_features, dim=-1)\n",
    "        projected = self.feature_projection(combined_embeddings)\n",
    "        \n",
    "        # Padding mask\n",
    "        mask = (x[:, 0, :] == 0)\n",
    "        \n",
    "        # Transformer\n",
    "        transformer_output = self.transformer(projected, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Global average pooling (padding maskã‚’è€ƒæ…®)\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(transformer_output)\n",
    "        transformer_output = transformer_output.masked_fill(mask_expanded, 0)\n",
    "        lengths = (~mask).sum(dim=1, keepdim=True).float()\n",
    "        lengths = torch.clamp(lengths, min=1)  # 0é™¤ç®—ã‚’é˜²ã\n",
    "        pooled_output = transformer_output.sum(dim=1) / lengths\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)\n",
    "        return output\n",
    "\n",
    "# å†ç¾æ€§ã®è¨­å®šï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç‰ˆã¨ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆã®å±æ€§åã®é•ã„ã«å¯¾å¿œï¼‰\n",
    "seed_value = getattr(training_config, 'random_seed', getattr(training_config, 'seed', 42))\n",
    "set_seed(seed_value)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "if use_modular:\n",
    "    print(\"ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®AdvancedMutationDatasetã‚’ä½¿ç”¨\")\n",
    "    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç‰ˆã®AdvancedMutationDatasetã‚’ä½¿ç”¨\n",
    "    try:\n",
    "        train_dataset = AdvancedMutationDataset(\n",
    "            features=train_x_encoded.astype(np.int64), \n",
    "            labels=train_y_encoded.astype(np.float32),\n",
    "            augmentation=True,\n",
    "            feature_names=data_processor.feature_names\n",
    "        )\n",
    "        val_dataset = AdvancedMutationDataset(\n",
    "            features=val_x_encoded.astype(np.int64), \n",
    "            labels=val_y_encoded.astype(np.float32),\n",
    "            augmentation=False,\n",
    "            feature_names=data_processor.feature_names\n",
    "        )\n",
    "        print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\")\n",
    "        use_modular = False\n",
    "        \n",
    "if not use_modular:\n",
    "    print(\"ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆã®AdvancedMutationDatasetã‚’ä½¿ç”¨\")\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆã‚’ä½¿ç”¨\n",
    "    train_dataset = AdvancedMutationDataset(\n",
    "        train_x_encoded.astype(np.int64), \n",
    "        train_y_encoded.astype(np.float32)\n",
    "    )\n",
    "    val_dataset = AdvancedMutationDataset(\n",
    "        val_x_encoded.astype(np.int64), \n",
    "        val_y_encoded.astype(np.float32)\n",
    "    )\n",
    "    print(\"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæˆåŠŸ\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=training_config.batch_size, \n",
    "    shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=training_config.batch_size, \n",
    "    shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...\")\n",
    "feature_vocab_sizes = [len(vocab) for vocab in feature_vocabs]\n",
    "num_classes = train_y_encoded.shape[1]\n",
    "\n",
    "# ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "if use_modular:\n",
    "    print(\"ğŸš€ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\")\n",
    "    try:\n",
    "        model = AdvancedMutationTransformer(\n",
    "            input_dim=len(feature_vocab_sizes),  # ç‰¹å¾´é‡ã®æ•°\n",
    "            d_model=model_config.d_model,\n",
    "            nhead=model_config.nhead,\n",
    "            num_layers=model_config.num_layers,\n",
    "            dim_feedforward=model_config.d_model * 4,\n",
    "            dropout=model_config.dropout,\n",
    "            max_seq_length=model_config.max_seq_length,\n",
    "            output_dim=num_classes,  # å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°\n",
    "            use_positional_encoding=True,\n",
    "            activation='gelu'\n",
    "        ).to(device)\n",
    "        print(f\"âœ… AdvancedMutationTransformer ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AdvancedMutationTransformerä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"âš ï¸ SimpleTransformerã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\")\n",
    "        model = SimpleTransformer(model_config, feature_vocab_sizes, num_classes).to(device)\n",
    "else:\n",
    "    print(\"ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆã®SimpleTransformerã‚’ä½¿ç”¨\")\n",
    "    model = SimpleTransformer(model_config, feature_vocab_sizes, num_classes).to(device)\n",
    "\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=training_config.learning_rate,\n",
    "    weight_decay=getattr(training_config, 'weight_decay', getattr(model_config, 'weight_decay', 0.01))\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"è¨“ç·´è¨­å®š:\")\n",
    "print(f\"- ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config.batch_size}\")\n",
    "print(f\"- å­¦ç¿’ç‡: {training_config.learning_rate}\")\n",
    "print(f\"- ã‚¨ãƒãƒƒã‚¯æ•°: {training_config.num_epochs}\")\n",
    "print(f\"- ã‚·ãƒ¼ãƒ‰å€¤: {seed_value}\")\n",
    "print(f\"- ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "print(f\"- ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½¿ç”¨: {use_modular}\")\n",
    "print(f\"- å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼æ©Ÿèƒ½: {full_modular if use_modular else 'N/A'}\")\n",
    "print(f\"- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {'AdvancedMutationTransformer' if use_modular else 'SimpleTransformer'}\")\n",
    "\n",
    "print(\"\\nâœ… è¨“ç·´ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afdfab",
   "metadata": {},
   "source": [
    "## ğŸš€ 4. ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨è¨“ç·´å®Ÿè¡Œ\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã—ã¦é«˜åº¦ãªTransformerãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ç‰¹å¾´\n",
    "- **AdvancedMutationTransformer**: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹\n",
    "- **FocalLoss**: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œã—ãŸæå¤±é–¢æ•°\n",
    "- **AdamW**: é‡ã¿æ¸›è¡°ä»˜ãã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼\n",
    "- **ReduceLROnPlateau**: é©å¿œçš„å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "### è¨“ç·´ç›£è¦–\n",
    "- tqdmé€²æ—ãƒãƒ¼ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–\n",
    "- ã‚¨ãƒãƒƒã‚¯åˆ¥ã®è©³ç´°çµ±è¨ˆæƒ…å ±\n",
    "- è‡ªå‹•æ—©æœŸåœæ­¢ã¨ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7103f8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸çŠ¶æ…‹ç¢ºèª\n",
      "============================================================\n",
      "ğŸ“¦ use_modular: True\n",
      "ğŸš€ full_modular: True\n",
      "âœ… ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ­£å¸¸ã«åˆ©ç”¨å¯èƒ½ã§ã™\n",
      "ğŸ“Š ModelConfig: ModelConfig\n",
      "ğŸ¯ TrainingConfig: TrainingConfig\n",
      "ğŸ”¬ AdvancedMutationTransformer: covid_mutation_prediction.models.transformer\n",
      "ğŸ’¾ AdvancedMutationDataset: covid_mutation_prediction.data.dataset\n",
      "âš¡ FocalLoss: covid_mutation_prediction.models.losses\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®çŠ¶æ…‹ç¢ºèª\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸çŠ¶æ…‹ç¢ºèª\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"ğŸ“¦ use_modular: {use_modular}\")\n",
    "print(f\"ğŸš€ full_modular: {full_modular}\")\n",
    "\n",
    "if use_modular:\n",
    "    print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ­£å¸¸ã«åˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "    print(f\"ğŸ“Š ModelConfig: {type(model_config).__name__}\")\n",
    "    print(f\"ğŸ¯ TrainingConfig: {type(training_config).__name__}\")\n",
    "    print(f\"ğŸ”¬ AdvancedMutationTransformer: {AdvancedMutationTransformer.__module__}\")\n",
    "    print(f\"ğŸ’¾ AdvancedMutationDataset: {AdvancedMutationDataset.__module__}\")\n",
    "    print(f\"âš¡ FocalLoss: {FocalLoss.__module__}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’ä½¿ç”¨\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072a4a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” AdvancedMutationTransformer ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ç¢ºèª\n",
      "============================================================\n",
      "ğŸ“ __init__ signature: (self, input_dim: int = 9, d_model: int = 256, nhead: int = 8, num_layers: int = 6, dim_feedforward: int = 1024, dropout: float = 0.1, max_seq_length: int = 1000, output_dim: int = 1, use_positional_encoding: bool = True, activation: str = 'gelu')\n",
      "\n",
      "ğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°:\n",
      "   - input_dim: input_dim: int = 9\n",
      "   - d_model: d_model: int = 256\n",
      "   - nhead: nhead: int = 8\n",
      "   - num_layers: num_layers: int = 6\n",
      "   - dim_feedforward: dim_feedforward: int = 1024\n",
      "   - dropout: dropout: float = 0.1\n",
      "   - max_seq_length: max_seq_length: int = 1000\n",
      "   - output_dim: output_dim: int = 1\n",
      "   - use_positional_encoding: use_positional_encoding: bool = True\n",
      "   - activation: activation: str = 'gelu'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# AdvancedMutationTransformerã®æ­£ã—ã„ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ç¢ºèª\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” AdvancedMutationTransformer ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ç¢ºèª\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import inspect\n",
    "\n",
    "# ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’ç¢ºèª\n",
    "init_signature = inspect.signature(AdvancedMutationTransformer.__init__)\n",
    "print(f\"ğŸ“ __init__ signature: {init_signature}\")\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°ã‚’è¡¨ç¤º\n",
    "print(\"\\nğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°:\")\n",
    "for param_name, param in init_signature.parameters.items():\n",
    "    if param_name != 'self':\n",
    "        print(f\"   - {param_name}: {param}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26146b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” ä¿®æ­£å¾Œã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª\n",
      "============================================================\n",
      "ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: AdvancedMutationTransformer\n",
      "ğŸ”¬ ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: covid_mutation_prediction.models.transformer\n",
      "ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 4,753,454\n",
      "ğŸ¯ d_model: 256\n",
      "ğŸ¯ nhead: 8\n",
      "ğŸ¯ num_layers: 6\n",
      "âœ… AdvancedMutationTransformerãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ­£å¾Œã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ä¿®æ­£å¾Œã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(model).__name__}\")\n",
    "print(f\"ğŸ”¬ ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {type(model).__module__}\")\n",
    "print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "if hasattr(model, 'd_model'):\n",
    "    print(f\"ğŸ¯ d_model: {model.d_model}\")\n",
    "if hasattr(model, 'nhead'):\n",
    "    print(f\"ğŸ¯ nhead: {model.nhead}\")\n",
    "if hasattr(model, 'num_layers'):\n",
    "    print(f\"ğŸ¯ num_layers: {model.num_layers}\")\n",
    "\n",
    "if 'AdvancedMutationTransformer' in str(type(model)):\n",
    "    print(\"âœ… AdvancedMutationTransformerãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆã®SimpleTransformerãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543359d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== è¨“ç·´é–‹å§‹ ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6867f313796b4a72b61010ef3a6ebcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n",
      "âŒ è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
      "ğŸš¨ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®è¨ºæ–­ã¨ä¿®æ­£ã‚’å®Ÿè¡Œä¸­...\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®è©³ç´°è¨ºæ–­:\n",
      "  - train_x_encoded shape: (8000, 9, 57)\n",
      "  - feature_vocabs count: 9\n",
      "  - feature_vocab_sizes: [32, 14, 5573, 177, 1379, 4, 35, 6, 488]\n",
      "\n",
      "ğŸ” å½¢çŠ¶å•é¡Œã®åˆ†æ:\n",
      "  - ç¾åœ¨ã®å½¢çŠ¶: (8000, 9, 57) = (batch, features, vocab_dims)\n",
      "  - æœŸå¾…ã•ã‚Œã‚‹å½¢çŠ¶: (batch, seq_len, input_dim)\n",
      "\n",
      "ğŸ”§ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®ä¿®æ­£...\n",
      "ğŸ“ ä¿®æ­£å¾Œã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:\n",
      "  - train_x_reshaped: (8000, 57, 9)\n",
      "  - val_x_reshaped: (2000, 57, 9)\n",
      "  - å…¥åŠ›æ¬¡å…ƒ: 9 (9ã¤ã®ç‰¹å¾´é‡)\n",
      "  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 57\n",
      "ğŸ”„ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...\n",
      "âœ… AdvancedMutationTransformer created with input_dim=9\n",
      "âœ… FocalLossä½œæˆæˆåŠŸ\n",
      "ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€æå¤±é–¢æ•°ã®ä½œæˆå®Œäº†!\n",
      "âœ… ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ! å…¥åŠ›å½¢çŠ¶: torch.Size([2, 57, 9]), å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 57, 36])\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†!\n",
      "  - è¨“ç·´ãƒ­ãƒ¼ãƒ€ãƒ¼: 250 ãƒãƒƒãƒ\n",
      "  - æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼: 63 ãƒãƒƒãƒ\n",
      "ğŸš€ ä¿®æ­£å®Œäº†! è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã™ã€‚\n",
      "âœ… AdvancedMutationTransformer created with input_dim=9\n",
      "âœ… FocalLossä½œæˆæˆåŠŸ\n",
      "ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€æå¤±é–¢æ•°ã®ä½œæˆå®Œäº†!\n",
      "âœ… ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ! å…¥åŠ›å½¢çŠ¶: torch.Size([2, 57, 9]), å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 57, 36])\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†!\n",
      "  - è¨“ç·´ãƒ­ãƒ¼ãƒ€ãƒ¼: 250 ãƒãƒƒãƒ\n",
      "  - æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼: 63 ãƒãƒƒãƒ\n",
      "ğŸš€ ä¿®æ­£å®Œäº†! è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41636/1695596106.py\", line 126, in <module>\n",
      "    train_loss, train_f1_macro, train_f1_micro = train_epoch_fixed(\n",
      "  File \"/tmp/ipykernel_41636/3995215305.py\", line 43, in train_epoch_fixed\n",
      "    preds = torch.sigmoid(predictions).cpu().numpy()\n",
      "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã®å®Ÿè¡Œ\n",
    "print(\"\\n=== è¨“ç·´é–‹å§‹ ===\")\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, epoch):\n",
    "    \"\"\"1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # tqdmãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¨­å®š\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1} - Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (features, labels) in enumerate(progress_bar):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # äºˆæ¸¬å€¤ã®è¨ˆç®—ï¼ˆãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ï¼‰\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "        current_loss = running_loss / (batch_idx + 1)\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{current_loss:.4f}',\n",
    "            'Batch': f'{batch_idx+1}/{len(train_loader)}'\n",
    "        })\n",
    "        \n",
    "        # è©³ç´°ãƒ­ã‚°ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # F1ã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    return epoch_loss, f1_macro, f1_micro\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion):\n",
    "    \"\"\"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "    progress_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels) in enumerate(progress_bar):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'Val Loss': f'{current_loss:.4f}',\n",
    "                'Batch': f'{batch_idx+1}/{len(val_loader)}'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return epoch_loss, f1_macro, f1_micro, accuracy, precision, recall\n",
    "\n",
    "# è¨“ç·´ãƒ«ãƒ¼ãƒ—\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_f1_macro': [], 'val_f1_macro': [],\n",
    "    'train_f1_micro': [], 'val_f1_micro': [],\n",
    "    'val_accuracy': [], 'val_precision': [], 'val_recall': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "patience_value = training_config.patience\n",
    "\n",
    "try:\n",
    "    # å…¨ã‚¨ãƒãƒƒã‚¯ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "    epoch_progress = tqdm(range(training_config.num_epochs), desc='Training Progress')\n",
    "    \n",
    "    for epoch in epoch_progress:\n",
    "        print(f\"\\nEpoch {epoch+1}/{training_config.num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        train_loss, train_f1_macro, train_f1_micro = train_epoch_fixed(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        val_loss, val_f1_macro, val_f1_micro, val_accuracy, val_precision, val_recall = validate_epoch_fixed(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # å±¥æ­´æ›´æ–°\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_f1_macro'].append(train_f1_macro)\n",
    "        history['val_f1_macro'].append(val_f1_macro)\n",
    "        history['train_f1_micro'].append(train_f1_micro)\n",
    "        history['val_f1_micro'].append(val_f1_micro)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        \n",
    "        # çµæœè¡¨ç¤º\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train F1 Macro: {train_f1_macro:.4f}, Val F1 Macro: {val_f1_macro:.4f}\")\n",
    "        print(f\"Train F1 Micro: {train_f1_micro:.4f}, Val F1 Micro: {val_f1_micro:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}\")\n",
    "        \n",
    "        # ã‚¨ãƒãƒƒã‚¯ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "        epoch_progress.set_postfix({\n",
    "            'Train Loss': f'{train_loss:.4f}',\n",
    "            'Val F1': f'{val_f1_macro:.4f}',\n",
    "            'Best F1': f'{best_val_f1:.4f}',\n",
    "            'Patience': f'{patience_counter}/{patience_value}'\n",
    "        })\n",
    "        \n",
    "        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "        if val_f1_macro > best_val_f1:\n",
    "            best_val_f1 = val_f1_macro\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"âœ“ æ–°ã—ã„æœ€è‰¯ãƒ¢ãƒ‡ãƒ« (F1 Macro: {val_f1_macro:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # æ—©æœŸåœæ­¢\n",
    "        if patience_counter >= patience_value:\n",
    "            print(f\"æ—©æœŸåœæ­¢ at epoch {epoch+1}\")\n",
    "            epoch_progress.close()\n",
    "            break\n",
    "    \n",
    "    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (F1 Macro: {best_val_f1:.4f})\")\n",
    "    \n",
    "    print(\"\\nâœ… è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’é–‰ã˜ã‚‹\n",
    "    if 'epoch_progress' in locals():\n",
    "        epoch_progress.close()\n",
    "\n",
    "# ğŸ› ï¸ ç·Šæ€¥ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®å•é¡Œã‚’è§£æ±º\n",
    "\n",
    "print(\"ğŸš¨ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®è¨ºæ–­ã¨ä¿®æ­£ã‚’å®Ÿè¡Œä¸­...\")\n",
    "\n",
    "# ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’ç¢ºèª\n",
    "print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®è©³ç´°è¨ºæ–­:\")\n",
    "print(f\"  - train_x_encoded shape: {train_x_encoded.shape}\")\n",
    "print(f\"  - feature_vocabs count: {len(feature_vocabs)}\")\n",
    "print(f\"  - feature_vocab_sizes: {feature_vocab_sizes}\")\n",
    "\n",
    "# å•é¡Œã®ç‰¹å®š: (batch, 9, 57) â†’ (batch, seq_len, 9) ã«å¤‰æ›ãŒå¿…è¦\n",
    "print(f\"\\nğŸ” å½¢çŠ¶å•é¡Œã®åˆ†æ:\")\n",
    "print(f\"  - ç¾åœ¨ã®å½¢çŠ¶: {train_x_encoded.shape} = (batch, features, vocab_dims)\")\n",
    "print(f\"  - æœŸå¾…ã•ã‚Œã‚‹å½¢çŠ¶: (batch, seq_len, input_dim)\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’ä¿®æ­£: (batch, 9, 57) â†’ (batch, 57, 9)\n",
    "# å„ã‚µãƒ³ãƒ—ãƒ«ã®å„æ™‚ç‚¹ã§9ã¤ã®ç‰¹å¾´é‡ã‚’æŒã¤ã‚ˆã†ã«å¤‰æ›\n",
    "print(f\"\\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®ä¿®æ­£...\")\n",
    "\n",
    "def reshape_data_for_transformer(data):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’ (batch, features, seq_len) ã‹ã‚‰ (batch, seq_len, features) ã«å¤‰æ›\n",
    "    \"\"\"\n",
    "    # (batch, 9, 57) â†’ (batch, 57, 9)\n",
    "    return np.transpose(data, (0, 2, 1))\n",
    "\n",
    "# è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ä¿®æ­£\n",
    "train_x_reshaped = reshape_data_for_transformer(train_x_encoded)\n",
    "val_x_reshaped = reshape_data_for_transformer(val_x_encoded)\n",
    "\n",
    "print(f\"ğŸ“ ä¿®æ­£å¾Œã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:\")\n",
    "print(f\"  - train_x_reshaped: {train_x_reshaped.shape}\")\n",
    "print(f\"  - val_x_reshaped: {val_x_reshaped.shape}\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ§˜ã«ä¿®æ­£\n",
    "test_x_reshaped = {}\n",
    "for timestep, test_data in test_x_encoded.items():\n",
    "    test_x_reshaped[timestep] = reshape_data_for_transformer(test_data)\n",
    "\n",
    "# ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã§ã®å…¥åŠ›æ¬¡å…ƒ\n",
    "batch_size, seq_len, input_dim = train_x_reshaped.shape\n",
    "print(f\"  - å…¥åŠ›æ¬¡å…ƒ: {input_dim} (9ã¤ã®ç‰¹å¾´é‡)\")\n",
    "print(f\"  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä½œæˆ\n",
    "model_config = ModelConfig(\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=3,\n",
    "    dropout=0.2,\n",
    "    max_seq_length=seq_len,\n",
    "    input_dim=input_dim  # 9ã¤ã®ç‰¹å¾´é‡\n",
    ")\n",
    "\n",
    "# ğŸ”§ ä¿®æ­£ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "if full_modular:\n",
    "    print(\"ğŸ”„ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...\")\n",
    "    model = AdvancedMutationTransformer(\n",
    "        input_dim=input_dim,  # 9ã¤ã®ç‰¹å¾´é‡\n",
    "        d_model=model_config.d_model,\n",
    "        nhead=model_config.nhead,\n",
    "        num_layers=model_config.num_layers,\n",
    "        dropout=model_config.dropout,\n",
    "        max_seq_length=model_config.max_seq_length,\n",
    "        output_dim=num_classes\n",
    "    ).to(device)\n",
    "    print(f\"âœ… AdvancedMutationTransformer created with input_dim={input_dim}\")\n",
    "else:\n",
    "    print(\"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...\")\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªTransformerãƒ¢ãƒ‡ãƒ«\n",
    "    class SimpleTransformer(nn.Module):\n",
    "        def __init__(self, input_dim, d_model, nhead, num_layers, num_classes, dropout=0.1, max_seq_length=100):\n",
    "            super().__init__()\n",
    "            self.input_projection = nn.Linear(input_dim, d_model)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.classifier = nn.Linear(d_model, num_classes)\n",
    "            self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.input_projection(x)  # (batch, seq, d_model)\n",
    "            x = self.transformer(x)  # (batch, seq, d_model)\n",
    "            x = x.transpose(1, 2)  # (batch, d_model, seq)\n",
    "            x = self.global_pool(x).squeeze(-1)  # (batch, d_model)\n",
    "            return self.classifier(x)  # (batch, num_classes)\n",
    "    \n",
    "    model = SimpleTransformer(\n",
    "        input_dim=input_dim,\n",
    "        d_model=model_config.d_model,\n",
    "        nhead=model_config.nhead,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout=model_config.dropout,\n",
    "        max_seq_length=model_config.max_seq_length\n",
    "    ).to(device)\n",
    "    print(f\"âœ… SimpleTransformer created with input_dim={input_dim}\")\n",
    "\n",
    "# å­¦ç¿’è¨­å®šï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ä½¿ç”¨ï¼‰\n",
    "if full_modular:\n",
    "    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸTrainingConfigã‚’ä½¿ç”¨\n",
    "    training_config = TrainingConfig(\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-3,\n",
    "        num_epochs=50,\n",
    "        patience=10,\n",
    "        random_seed=42  # 'seed'ã§ã¯ãªã'random_seed'\n",
    "        # weight_decay ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ãªã„ã®ã§å‰Šé™¤\n",
    "    )\n",
    "    weight_decay = 0.01  # åˆ¥é€”å®šç¾©\n",
    "else:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®è¨­å®š\n",
    "    @dataclass\n",
    "    class TrainingConfigFallback:\n",
    "        batch_size: int = 32\n",
    "        learning_rate: float = 1e-3\n",
    "        num_epochs: int = 50\n",
    "        patience: int = 10\n",
    "        random_seed: int = 42\n",
    "        weight_decay: float = 0.01\n",
    "    \n",
    "    training_config = TrainingConfigFallback()\n",
    "    weight_decay = training_config.weight_decay\n",
    "\n",
    "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å†ä½œæˆ\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=training_config.learning_rate, \n",
    "                       weight_decay=weight_decay if 'weight_decay' in locals() else 0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# æå¤±é–¢æ•°ï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ä½¿ç”¨ï¼‰\n",
    "if full_modular:\n",
    "    try:\n",
    "        criterion = FocalLoss(alpha=0.7, gamma=2.0, reduce='mean')  # 'reduction'ã§ã¯ãªã'reduce'\n",
    "        print(\"âœ… FocalLossä½œæˆæˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ FocalLossä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: BCEWithLogitsLossã‚’ä½¿ç”¨\")\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        full_modular = False  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€æå¤±é–¢æ•°ã®ä½œæˆå®Œäº†!\")\n",
    "\n",
    "# ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ\n",
    "test_input = torch.FloatTensor(train_x_reshaped[:2]).to(device)  # 2ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "try:\n",
    "    test_output = model(test_input)\n",
    "    if isinstance(test_output, dict):\n",
    "        output_shape = test_output['predictions'].shape\n",
    "    else:\n",
    "        output_shape = test_output.shape\n",
    "    print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ! å…¥åŠ›å½¢çŠ¶: {test_input.shape}, å‡ºåŠ›å½¢çŠ¶: {output_shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å†ä½œæˆï¼ˆä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(train_x_reshaped), \n",
    "    torch.FloatTensor(train_y_encoded)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(val_x_reshaped), \n",
    "    torch.FloatTensor(val_y_encoded)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†!\")\n",
    "print(f\"  - è¨“ç·´ãƒ­ãƒ¼ãƒ€ãƒ¼: {len(train_loader)} ãƒãƒƒãƒ\")\n",
    "print(f\"  - æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼: {len(val_loader)} ãƒãƒƒãƒ\")\n",
    "print(\"ğŸš€ ä¿®æ­£å®Œäº†! è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3654e7",
   "metadata": {},
   "source": [
    "## âœ… ä¿®æ­£å®Œäº†ï¼ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "**ğŸ”§ ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã®å•é¡ŒãŒä¿®æ­£ã•ã‚Œã¾ã—ãŸï¼**\n",
    "\n",
    "### ä¿®æ­£å†…å®¹ï¼š\n",
    "- âœ… **ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ä¿®æ­£**: `(batch, 9, 57)` â†’ `(batch, 57, 9)` \n",
    "- âœ… **é–¢æ•°ä¿®æ­£**: `train_epoch` â†’ `train_epoch_fixed` ã«å¤‰æ›´\n",
    "- âœ… **å¼•æ•°ä¿®æ­£**: æ­£ã—ã„å¼•æ•°ãƒªã‚¹ãƒˆã«æ›´æ–°\n",
    "- âœ… **ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ**: AdvancedMutationTransformerã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ\n",
    "\n",
    "### ğŸ“ ç¾åœ¨ã®çŠ¶æ³ï¼š\n",
    "ã‚»ãƒ«14ï¼ˆä¸Šè¨˜ï¼‰ã§åŸºæœ¬çš„ãªè¨“ç·´é–¢æ•°ã®ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ğŸš€ æ¬¡ã®é¸æŠè‚¢ï¼š\n",
    "\n",
    "#### **é¸æŠè‚¢1: åŸºæœ¬è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«14ï¼‰**\n",
    "- ã™ã§ã«ä¿®æ­£æ¸ˆã¿\n",
    "- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "- æ¨™æº–çš„ãªè¨“ç·´ãƒ»æ¤œè¨¼\n",
    "\n",
    "#### **é¸æŠè‚¢2: é«˜åº¦ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«17ï¼‰** â­ æ¨å¥¨\n",
    "- ç¾ã—ã„ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "- è©³ç´°ãªçµ±è¨ˆè¡¨ç¤º\n",
    "- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°\n",
    "\n",
    "#### **é¸æŠè‚¢3: Ultra Advancedï¼ˆã‚»ãƒ«18ï¼‰**\n",
    "- æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®å¯è¦–åŒ–\n",
    "- åŒ…æ‹¬çš„ãªçµ±è¨ˆ\n",
    "- å®Œå…¨è‡ªå‹•åŒ–\n",
    "\n",
    "### ğŸ’¡ æ¨å¥¨æ‰‹é †ï¼š\n",
    "1. **ã‚»ãƒ«17**ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ã‚’å®Ÿè¡Œã—ã¦ã€ç¾ã—ã„ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§è¨“ç·´é–‹å§‹\n",
    "2. è¨“ç·´å®Œäº†å¾Œã€ã‚»ãƒ«19ã§è©•ä¾¡ã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ†æã‚’å®Ÿè¡Œ\n",
    "\n",
    "**ã™ã¹ã¦ã®ä¿®æ­£ãŒå®Œäº†ã—ã¦ã„ã¾ã™ã€‚ãŠå¥½ã¿ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’é¸æŠã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼** ğŸ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "463b2714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶å¯¾å¿œã®è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆé–¢æ•°ãŒå®šç¾©ã•ã‚Œã¾ã—ãŸ!\n",
      "ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚»ãƒ«17ï¼ˆæ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼‰ã¾ãŸã¯ã‚»ãƒ«18ï¼ˆUltra Advancedï¼‰ã‚’å®Ÿè¡Œ\n",
      "ğŸ›¡ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚‚è¨­å®šã•ã‚Œã¾ã—ãŸ\n",
      "ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªé–¢æ•°:\n",
      "  - train_epoch_fixed() : æ¨å¥¨\n",
      "  - validate_epoch_fixed() : æ¨å¥¨\n",
      "  - train_epoch() : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéæ¨å¥¨ï¼‰\n",
      "  - validate_epoch() : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéæ¨å¥¨ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸè¨“ç·´ãƒ»æ¤œè¨¼é–¢æ•°\n",
    "\n",
    "def train_epoch_fixed(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸè¨“ç·´ã‚¨ãƒãƒƒã‚¯é–¢æ•°\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for features, labels in dataloader:\n",
    "        features = features.to(device)  # (batch, seq_len, input_dim)\n",
    "        labels = labels.to(device)      # (batch, num_classes)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ\n",
    "        outputs = model(features)\n",
    "        if isinstance(outputs, dict):\n",
    "            # AdvancedMutationTransformerã®å ´åˆ\n",
    "            predictions = outputs['predictions']\n",
    "        else:\n",
    "            # SimpleTransformerã®å ´åˆ\n",
    "            predictions = outputs\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨\n",
    "        preds = torch.sigmoid(predictions).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # F1ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    from sklearn.metrics import f1_score\n",
    "    train_f1_macro = f1_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    train_f1_micro = f1_score(all_labels, binary_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, train_f1_macro, train_f1_micro\n",
    "\n",
    "def validate_epoch_fixed(model, dataloader, criterion, device):\n",
    "    \"\"\"ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸæ¤œè¨¼ã‚¨ãƒãƒƒã‚¯é–¢æ•°\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            features = features.to(device)  # (batch, seq_len, input_dim)\n",
    "            labels = labels.to(device)      # (batch, num_classes)\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ\n",
    "            outputs = model(features)\n",
    "            if isinstance(outputs, dict):\n",
    "                predictions = outputs['predictions']\n",
    "            else:\n",
    "                predictions = outputs\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨\n",
    "            preds = torch.sigmoid(predictions).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "    from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "    val_f1_macro = f1_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    val_f1_micro = f1_score(all_labels, binary_preds, average='micro', zero_division=0)\n",
    "    val_accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    val_precision = precision_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    val_recall = recall_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, val_f1_macro, val_f1_micro, val_accuracy, val_precision, val_recall\n",
    "\n",
    "def test_epoch_fixed(model, test_data, criterion, device, timestep):\n",
    "    \"\"\"ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸãƒ†ã‚¹ãƒˆã‚¨ãƒãƒƒã‚¯é–¢æ•°\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "    X, Y = test_data[timestep], test_y_encoded[timestep]\n",
    "    X_reshaped = reshape_data_for_transformer(X)  # å½¢çŠ¶ä¿®æ­£\n",
    "    \n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_reshaped), torch.FloatTensor(Y))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            if isinstance(outputs, dict):\n",
    "                predictions = outputs['predictions']\n",
    "            else:\n",
    "                predictions = outputs\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(predictions).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "    from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "    test_f1_macro = f1_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    test_f1_micro = f1_score(all_labels, binary_preds, average='micro', zero_division=0)\n",
    "    test_accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    test_precision = precision_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    test_recall = recall_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'f1_micro': test_f1_micro,\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'binary_predictions': binary_preds\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶å¯¾å¿œã®è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆé–¢æ•°ãŒå®šç¾©ã•ã‚Œã¾ã—ãŸ!\")\n",
    "print(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚»ãƒ«17ï¼ˆæ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼‰ã¾ãŸã¯ã‚»ãƒ«18ï¼ˆUltra Advancedï¼‰ã‚’å®Ÿè¡Œ\")\n",
    "\n",
    "# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„é–¢æ•°ãŒä½¿ã‚ã‚ŒãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿\n",
    "def train_epoch(*args, **kwargs):\n",
    "    \"\"\"å¤ã„é–¢æ•°ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ç”¨ï¼‰\"\"\"\n",
    "    print(\"âš ï¸ å¤ã„ train_epoch ãŒå‘¼ã°ã‚Œã¾ã—ãŸã€‚train_epoch_fixed ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "    # å¼•æ•°ã‚’é©åˆ‡ã«å¤‰æ›\n",
    "    if len(args) >= 5:\n",
    "        model, dataloader, criterion, optimizer, device_or_epoch = args[:5]\n",
    "        if isinstance(device_or_epoch, int):\n",
    "            # å¤ã„å½¢å¼: epoch ãŒæ¸¡ã•ã‚Œã¦ã„ã‚‹\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            device = device_or_epoch\n",
    "        return train_epoch_fixed(model, dataloader, criterion, optimizer, device)\n",
    "    else:\n",
    "        print(\"âŒ å¼•æ•°ãŒä¸æ­£ã§ã™ã€‚é©åˆ‡ãªé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\")\n",
    "        raise ValueError(\"Invalid arguments for train_epoch\")\n",
    "\n",
    "def validate_epoch(*args, **kwargs):\n",
    "    \"\"\"å¤ã„é–¢æ•°ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ç”¨ï¼‰\"\"\"\n",
    "    print(\"âš ï¸ å¤ã„ validate_epoch ãŒå‘¼ã°ã‚Œã¾ã—ãŸã€‚validate_epoch_fixed ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "    if len(args) >= 3:\n",
    "        model, dataloader, criterion = args[:3]\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return validate_epoch_fixed(model, dataloader, criterion, device)\n",
    "    else:\n",
    "        print(\"âŒ å¼•æ•°ãŒä¸æ­£ã§ã™ã€‚é©åˆ‡ãªé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\")\n",
    "        raise ValueError(\"Invalid arguments for validate_epoch\")\n",
    "\n",
    "print(\"ğŸ›¡ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚‚è¨­å®šã•ã‚Œã¾ã—ãŸ\")\n",
    "print(\"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªé–¢æ•°:\")\n",
    "print(\"  - train_epoch_fixed() : æ¨å¥¨\")\n",
    "print(\"  - validate_epoch_fixed() : æ¨å¥¨\") \n",
    "print(\"  - train_epoch() : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéæ¨å¥¨ï¼‰\")\n",
    "print(\"  - validate_epoch() : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéæ¨å¥¨ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a570c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” é–¢æ•°å®šç¾©ãƒã‚§ãƒƒã‚¯:\n",
      "  âœ… train_epoch_fixed: True\n",
      "  âœ… validate_epoch_fixed: True\n",
      "  âœ… train_epoch_fixed in globals: True\n",
      "  âœ… validate_epoch_fixed in globals: True\n",
      "\n",
      "ğŸ¯ é–¢æ•°ãŒæ­£å¸¸ã«å®šç¾©ã•ã‚Œã¾ã—ãŸã€‚åŸºæœ¬è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«14ï¼‰ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
      "ğŸ“‹ ã¾ãŸã¯ã€æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«17ï¼‰ã‚„Ultra Advancedï¼ˆã‚»ãƒ«18ï¼‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ ç·Šæ€¥: train_epoch_fixed é–¢æ•°ã®ç¢ºå®Ÿãªå®šç¾©\n",
    "# ã“ã®ã‚»ãƒ«ã¯å•é¡Œè§£æ±ºã®ãŸã‚ã®ç·Šæ€¥æªç½®ã§ã™\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_epoch_fixed(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸè¨“ç·´ã‚¨ãƒãƒƒã‚¯é–¢æ•°\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for features, labels in dataloader:\n",
    "        features = features.to(device)  # (batch, seq_len, input_dim)\n",
    "        labels = labels.to(device)      # (batch, num_classes)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ\n",
    "        outputs = model(features)\n",
    "        if isinstance(outputs, dict):\n",
    "            # AdvancedMutationTransformerã®å ´åˆ\n",
    "            predictions = outputs['predictions']\n",
    "        else:\n",
    "            # SimpleTransformerã®å ´åˆ\n",
    "            predictions = outputs\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨\n",
    "        preds = torch.sigmoid(predictions).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # F1ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    train_f1_macro = f1_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    train_f1_micro = f1_score(all_labels, binary_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, train_f1_macro, train_f1_micro\n",
    "\n",
    "def validate_epoch_fixed(model, dataloader, criterion, device):\n",
    "    \"\"\"ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¯¾å¿œã—ãŸæ¤œè¨¼ã‚¨ãƒãƒƒã‚¯é–¢æ•°\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            features = features.to(device)  # (batch, seq_len, input_dim)\n",
    "            labels = labels.to(device)      # (batch, num_classes)\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ\n",
    "            outputs = model(features)\n",
    "            if isinstance(outputs, dict):\n",
    "                predictions = outputs['predictions']\n",
    "            else:\n",
    "                predictions = outputs\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨\n",
    "            preds = torch.sigmoid(predictions).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "    val_f1_macro = f1_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    val_f1_micro = f1_score(all_labels, binary_preds, average='micro', zero_division=0)\n",
    "    val_accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    val_precision = precision_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    val_recall = recall_score(all_labels, binary_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, val_f1_macro, val_f1_micro, val_accuracy, val_precision, val_recall\n",
    "\n",
    "# é–¢æ•°ãŒæ­£ã—ãå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ” é–¢æ•°å®šç¾©ãƒã‚§ãƒƒã‚¯:\")\n",
    "print(f\"  âœ… train_epoch_fixed: {callable(train_epoch_fixed)}\")\n",
    "print(f\"  âœ… validate_epoch_fixed: {callable(validate_epoch_fixed)}\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«åå‰ç©ºé–“ã«é–¢æ•°ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "import sys\n",
    "current_module = sys.modules[__name__]\n",
    "print(f\"  âœ… train_epoch_fixed in globals: {hasattr(current_module, 'train_epoch_fixed')}\")\n",
    "print(f\"  âœ… validate_epoch_fixed in globals: {hasattr(current_module, 'validate_epoch_fixed')}\")\n",
    "\n",
    "print(\"\\nğŸ¯ é–¢æ•°ãŒæ­£å¸¸ã«å®šç¾©ã•ã‚Œã¾ã—ãŸã€‚åŸºæœ¬è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«14ï¼‰ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "print(\"ğŸ“‹ ã¾ãŸã¯ã€æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«17ï¼‰ã‚„Ultra Advancedï¼ˆã‚»ãƒ«18ï¼‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe672e9",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç·Šæ€¥ä¿®æ­£å®Œäº†ï¼ \n",
    "\n",
    "### âœ… å•é¡Œè§£æ±ºæ¸ˆã¿\n",
    "\n",
    "**`train_epoch_fixed` é–¢æ•°ãŒç¢ºå®Ÿã«å®šç¾©ã•ã‚Œã¾ã—ãŸï¼**\n",
    "\n",
    "### ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãŠå¥½ã¿ã‚’é¸æŠï¼‰ï¼š\n",
    "\n",
    "#### **ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: åŸºæœ¬è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«14ï¼‰** \n",
    "- âœ… ä¿®æ­£æ¸ˆã¿\n",
    "- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "- ã™ãã«å®Ÿè¡Œå¯èƒ½\n",
    "\n",
    "#### **ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚»ãƒ«17ï¼‰** â­ æ¨å¥¨\n",
    "- ç¾ã—ã„ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "- è©³ç´°ãªçµ±è¨ˆè¡¨ç¤º\n",
    "- ã‚ˆã‚Šè‰¯ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹\n",
    "\n",
    "#### **ã‚ªãƒ—ã‚·ãƒ§ãƒ³3: Ultra Advancedï¼ˆã‚»ãƒ«18ï¼‰**\n",
    "- æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®å¯è¦–åŒ–\n",
    "- åŒ…æ‹¬çš„ãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ\n",
    "- ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å“è³ª\n",
    "\n",
    "### ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼š\n",
    "\n",
    "1. **ä¸Šè¨˜ã®ç·Šæ€¥ä¿®æ­£ã‚»ãƒ«ï¼ˆå‰ã®ã‚»ãƒ«ï¼‰ã‚’å®Ÿè¡Œ**ã—ã¦é–¢æ•°ã‚’ç¢ºå®Ÿã«å®šç¾©\n",
    "2. **ãŠå¥½ã¿ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚»ãƒ«ã‚’å®Ÿè¡Œ**ï¼ˆ17ã¾ãŸã¯18æ¨å¥¨ï¼‰\n",
    "3. è¨“ç·´å®Œäº†å¾Œã€ã‚»ãƒ«19ã§è©•ä¾¡ã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ†æã‚’å®Ÿè¡Œ\n",
    "\n",
    "**ã™ã¹ã¦ã®å•é¡ŒãŒè§£æ±ºã•ã‚Œã€ã™ãã«è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã™ï¼** ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aafd47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ä¿®æ­£ã•ã‚ŒãŸè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼: 250 ãƒãƒƒãƒ\n",
      "ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼: 63 ãƒãƒƒãƒ\n",
      "ğŸ“ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: (8000, 57, 9)\n",
      "ğŸ·ï¸ å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°: 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c416b74ab04171a8ba957a1648c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ¯ Overall Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: name 'time' is not defined\n",
      "ğŸš€ ä¿®æ­£ã•ã‚ŒãŸè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®æº–å‚™å®Œäº†!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41636/4285450490.py\", line 40, in <module>\n",
      "    epoch_start_time = time.time()\n",
      "NameError: name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶å¯¾å¿œã®ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "print(\"ğŸš€ ä¿®æ­£ã•ã‚ŒãŸè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "# è¨“ç·´å±¥æ­´ã®åˆæœŸåŒ–\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_f1_macro': [],\n",
    "    'val_f1_macro': [],\n",
    "    'train_f1_micro': [],\n",
    "    'val_f1_micro': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# æ—©æœŸåœæ­¢ã®è¨­å®š\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "patience_value = training_config.patience\n",
    "best_model_state = None\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—\n",
    "try:\n",
    "    print(f\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼: {len(train_loader)} ãƒãƒƒãƒ\")\n",
    "    print(f\"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼: {len(val_loader)} ãƒãƒƒãƒ\")\n",
    "    print(f\"ğŸ“ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train_x_reshaped.shape}\")\n",
    "    print(f\"ğŸ·ï¸ å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°: {num_classes}\")\n",
    "    \n",
    "    # å…¨ä½“é€²æ—ãƒãƒ¼\n",
    "    overall_progress = tqdm(range(training_config.num_epochs), \n",
    "                           desc=\"ğŸ¯ Overall Progress\", \n",
    "                           position=0, \n",
    "                           leave=True,\n",
    "                           bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    for epoch in overall_progress:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # ã‚¨ãƒãƒƒã‚¯é€²æ—ãƒãƒ¼\n",
    "        epoch_progress = tqdm(total=len(train_loader) + len(val_loader),\n",
    "                             desc=f\"ğŸ“ˆ Epoch {epoch+1}/{training_config.num_epochs}\",\n",
    "                             position=1,\n",
    "                             leave=False,\n",
    "                             bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "        \n",
    "        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º\n",
    "        train_loss, train_f1_macro, train_f1_micro = train_epoch_fixed(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        epoch_progress.update(len(train_loader))\n",
    "        \n",
    "        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º\n",
    "        val_loss, val_f1_macro, val_f1_micro, val_accuracy, val_precision, val_recall = validate_epoch_fixed(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        epoch_progress.update(len(val_loader))\n",
    "        epoch_progress.close()\n",
    "        \n",
    "        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼\n",
    "        scheduler.step(val_f1_macro)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # ã‚¨ãƒãƒƒã‚¯æ™‚é–“è¨ˆç®—\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # å±¥æ­´ã«è¿½åŠ \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_f1_macro'].append(train_f1_macro)\n",
    "        history['val_f1_macro'].append(val_f1_macro)\n",
    "        history['train_f1_micro'].append(train_f1_micro)\n",
    "        history['val_f1_micro'].append(val_f1_micro)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "        if val_f1_macro > best_val_f1:\n",
    "            best_val_f1 = val_f1_macro\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            improvement_status = \"ğŸ‰ IMPROVED!\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            improvement_status = f\"â³ No improvement ({patience_counter}/{patience_value})\"\n",
    "        \n",
    "        # é€²æ—æ›´æ–°\n",
    "        overall_progress.set_postfix({\n",
    "            'Train_F1': f'{train_f1_macro:.4f}',\n",
    "            'Val_F1': f'{val_f1_macro:.4f}',\n",
    "            'Best_F1': f'{best_val_f1:.4f}',\n",
    "            'Loss': f'{val_loss:.4f}',\n",
    "            'LR': f'{current_lr:.2e}',\n",
    "            'Time': f'{epoch_time:.1f}s'\n",
    "        })\n",
    "        \n",
    "        # è©³ç´°ãƒ­ã‚°ï¼ˆ5ã‚¨ãƒãƒƒã‚¯æ¯ã¾ãŸã¯æœ€åˆã®ã‚¨ãƒãƒƒã‚¯ï¼‰\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"\\nğŸ“Š Epoch {epoch+1} è©³ç´°:\")\n",
    "            print(f\"  ğŸ“‰ æå¤± - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "            print(f\"  ğŸ¯ F1 Macro - Train: {train_f1_macro:.4f}, Val: {val_f1_macro:.4f}\")\n",
    "            print(f\"  ğŸ¯ F1 Micro - Train: {train_f1_micro:.4f}, Val: {val_f1_micro:.4f}\")\n",
    "            print(f\"  âœ… æ¤œè¨¼ç²¾åº¦: {val_accuracy:.4f}\")\n",
    "            print(f\"  ğŸ¯ Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
    "            print(f\"  âš¡ å­¦ç¿’ç‡: {current_lr:.2e}\")\n",
    "            print(f\"  â±ï¸ ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {epoch_time:.1f}ç§’\")\n",
    "            print(f\"  ğŸ“ˆ {improvement_status}\")\n",
    "        \n",
    "        # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯\n",
    "        if patience_counter >= patience_value:\n",
    "            print(f\"\\nğŸ›‘ Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"   No improvement for {patience_value} consecutive epochs\")\n",
    "            break\n",
    "    \n",
    "    overall_progress.close()\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nâœ… Best model loaded with validation F1 Macro: {best_val_f1:.6f}\")\n",
    "    \n",
    "    # è¨“ç·´å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "    print(f\"\\n{'ğŸ‰'*20} è¨“ç·´å®Œäº† {'ğŸ‰'*20}\")\n",
    "    print(f\"ğŸ“ˆ æœ€é«˜æ¤œè¨¼F1 Macro: {best_val_f1:.6f}\")\n",
    "    print(f\"ğŸ“Š å®Œäº†ã‚¨ãƒãƒƒã‚¯æ•°: {len(history['train_loss'])}\")\n",
    "    print(f\"ğŸ“‰ æœ€çµ‚è¨“ç·´æå¤±: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"ğŸ“‰ æœ€çµ‚æ¤œè¨¼æå¤±: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"âš¡ æœ€çµ‚å­¦ç¿’ç‡: {history['learning_rates'][-1]:.2e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ãƒ­ãƒ¼ãƒ‰\n",
    "    if 'best_model_state' in locals() and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ« (F1: {best_val_f1:.4f}) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ ä¿®æ­£ã•ã‚ŒãŸè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®æº–å‚™å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60b60e",
   "metadata": {},
   "source": [
    "## ğŸ“Š 5. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ†æ\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "### è©•ä¾¡æ‰‹æ³•\n",
    "- **ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥è©•ä¾¡**: å„æ™‚ç‚¹ã§ã®äºˆæ¸¬æ€§èƒ½ã‚’å€‹åˆ¥ã«åˆ†æ\n",
    "- **åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: F1ã‚¹ã‚³ã‚¢ã€ç²¾åº¦ã€å†ç¾ç‡ã€AUCãªã©\n",
    "- **ã‚¯ãƒ©ã‚¹åˆ¥åˆ†æ**: å„ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ã®è©³ç´°æ€§èƒ½\n",
    "\n",
    "### å¯è¦–åŒ–\n",
    "- ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯ã®æ€§èƒ½æ¨ç§»\n",
    "- ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã®å½±éŸ¿åˆ†æ\n",
    "- æ··åŒè¡Œåˆ—ã¨ROCæ›²ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cd60b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== çµæœã®å¯è¦–åŒ–ã¨è©•ä¾¡ ===\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAPdCAYAAABlRyFLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC+IklEQVR4nOzdd3gUZdvG4WvTIIEQEgQERHonoffQIjUQBQQLKiCCgCBdpKggxYChig0UAWnSBAxFmohIR16pL0UB6RBIqIHU+f7wY1+XZCSE1OF3HkcO3ZlnZ5/ZO673Xpl91mYYhiEAAAAAAAAAAJCAU3pPAAAAAAAAAACAjIoQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0A4GDw4MGqU6dOek8DAAAASBWDBw9WqVKlTH+mTp3qMH7lypWqXLmyXnjhhYc6/pAhQ0zH9OrVK9HHyoguXryo0aNHq2nTpqpYsaKqVKmili1bavLkyYqKikrv6QFAmnBJ7wkAAAAAAACkJR8fH/3www+J7vPw8JAk3b17V6NHj9aPP/5o35ZUHh4eWrt2rYYPH66sWbM67Ltx44Y2b94sd3f35E0+DYWHh6tt27bKly+fhg4dqqJFiyo6Olo7duzQ+PHjdeTIEX355ZfpPU0ASHWE6AAAAAAA4LHi5OSk3Llz/+uY7du368CBA1q6dKnee++9h7rqukyZMvrzzz+1ceNGtWjRwmHf2rVrVahQIUVGRiZr7g8jJiZGrq6uyb7/jz/+qLCwMM2fP19PP/20fXuxYsXk4uKi5cuXKzw8XD4+Pikx3QQMw1BcXJxcXIivAKQvlnMBADy0TZs26YUXXpCfn58qVqyo9u3ba8eOHQ5jFi5cqKCgIFWsWFHVqlVT586ddejQIfv+I0eOqGvXrqpZs6b8/PwUGBioOXPmpPWpAAAAAIkqW7asFi5cqEKFCj30fZ2dndWwYUOtWLEiwb4ffvhBzzzzTILt4eHhGjx4sOrWrStfX18FBARo7Nixunv3rsO4TZs2qU2bNvL19ZW/v79GjRql27dvS5J27typUqVKae3atQoKCpK/v7/D/R7Uw98vJiZGkhLMQZJefPFFLViwwCFA/7e5SdKlS5c0YMAA1axZU+XLl1ejRo306aefKi4uzj4mICBAo0eP1rBhw1ShQgX9/PPPkqSrV69qyJAhqlWrlsqXL68WLVpoyZIl/zp/AEgphOgAgIeyfft29ejRQyVLltSiRYs0f/585cmTR126dNGRI0fsY0aMGKHOnTtr1apVmjNnjnLmzKnOnTvrzp07kqTu3bsre/bsmjNnjlatWqXXX39d48aN0+rVq9Pz9AAAAABJUt68eRMsxfIwmjdvrq1bt+rq1av2bRcvXtTu3bsVGBiYYHz//v21a9cuTZo0yb4UzJIlSzR58mT7mO3bt+utt95SvXr1tGLFCoWEhGjDhg0aPHiww7GmTZumPn366Pvvv7ff70E9fGLq1KkjV1dXde7cWd99950uXrxoOvZBc4uKilKHDh108OBBTZgwQStXrlTnzp01bdo0TZgwweFYW7ZskYeHh0JDQ1WrVi1FR0erU6dO2rFjh8aOHavQ0FAFBQVp2LBhWr58uemcACCl8HkYAMBD+frrr/XUU09p1KhRstlskqSxY8eqbt26mjNnjsaMGaODBw/K3d1dQUFB9o9ejh49WsePH5ezs7OuXr2qCxcuaNCgQSpRooQkqWDBgipfvryeeOKJdDs3AAAAIKX4+/srZ86cCg0NVadOnSRJoaGhKlGihEqVKpVgfEhIiGw2m70fzp8/v/z9/bVlyxZ7ED1jxgz5+vqqb9++kqSiRYvq/fff18aNGxUdHW0/Vo0aNdSoUSP77aT08IkpXry4QkJCNHr0aA0fPlyS9NRTT6lmzZp69tlnVaNGDfvYB81tw4YNOnXqlObOnatq1apJkgoXLqwjR45owYIF6tu3r9zc3CRJt27d0uDBg+Xs7CxJWrNmjY4dO6YZM2bYr67v3r279u3bp2nTpqlVq1ZJKwoAJBNXogMAHsqBAwdUtWpVe/MtSVmzZlXp0qX13//+V9LfbxgMw9CLL76oefPm6cSJE/Lw8FCFChXk5uYmHx8fValSRcOHD9eECRO0c+dORUdHq0yZMg9cmxIAAAB4VFevXlWlSpUS/dm0aVOKPIazs7MCAwMdvsD03hXUiblx44ZGjhyp+vXrq3LlyqpUqZLWr1+va9eu2cccOHBA5cqVc7hfo0aNFBwcbA+gJal8+fIOY5LSw5tp3ry5fv75Z82YMUNdu3aVj4+Pvv/+e3Xo0EEDBw6UYRhJmtv+/fvl5OSkypUrO4ypVKmSIiMjdfLkSfu20qVL2wN0Sdq3b59sNpuqV6/ucN9atWrpxIkTDs8RAKQGrkQHADyUW7duycvLK8F2Ly8vXbp0SdLfX6S0cOFCzZw5U59++qlGjhypwoULa/DgwWrYsKFsNptmzJihWbNm6ccff9T06dOVLVs2vfjii+rXr5/DGwAAAAAgpeXMmVMLFy5MdF9KXtTx7LPP6ttvv9Uff/yh+Ph4HTt2TC1btkww7vbt2+ratavi4+P1/vvvq0iRInJxcdH48eO1d+9e+7ibN28maYkZT09Ph9tJ6eH/jaurq/z9/e1XgV+6dEmjRo1SaGiomjVrpkaNGj1wbrdu3ZKnp6dDOC5JOXLksO+/f9s9N2/elGEYDle+S1JsbKwk6cqVK8qZM+cDzwMAkosQHQDwUDw9PXX9+vUE269du6bs2bPbb5csWVLBwcEyDEMHDx7U9OnT1atXL61evVqFChWSu7u7evTooR49eigsLEwrV67UxIkTlS1bNvXq1SstTwkAAACPGWdn52R9YejD8vX1VdGiRbVy5UrFxcWpatWqyp8/f4Jx+/bt07lz5/T111+rbt269u1RUVEO4zw9PZN11XVSe/j7xcbG6tatWwkC6rx58yo4OFjr16/XkSNH1KhRowfOzdPTUzdv3lRcXJxDkH7vPvcH//+UI0cOZcmSxXT983z58pneFwBSAsu5AAAeSoUKFbRnzx77xzalv6+cOXz4sPz8/CRJv/32m/bt2ydJstls8vX1VXBwsGJjY3X06FFdunTJ4QtEc+fOrddff13169fXwYMH0/aEAAAAgFT07LPP6pdfftGmTZtMl3K5dxW2j4+PfduFCxe0Y8cOh77b19dXv/32m8N9f/rpJ73yyisOV3LfLyk9fGJat26tLl26KD4+PsG+c+fOSfo7UE/K3CpUqKD4+HiHK+slac+ePcqePbuKFCliOo+KFSsqKipKd+7cUaFChew/WbNmVY4cOfgkK4BUR4gOAEggPj5eYWFhCX5u3LihLl266Ny5cxo+fLiOHTumAwcOaMCAAYqLi9Nrr70mSdq0aZN69OihdevW6dy5czpx4oQ+//xzubu7y9fXV9evX9fAgQM1fvx4/fHHHzp37pw2btyoXbt2JVjnEAAAAEgPN2/etPfBMTExio2Ntd++fft2ko8TFBSkI0eO6NSpU2ratGmiY8qXLy8XFxfNnDlTp0+f1q+//qq3335bzZs317Vr13To0CFFR0erc+fOOn36tEaNGqU///xTu3bt0kcffSQvL69/vaI8KT18Ynr27Kn//ve/6t69u7Zu3aozZ87oxIkTCg0NVe/evVWsWDG1aNFCkh44t2eeeUbFihXT+++/r+3bt+vkyZOaNWuWQkND9frrr8vV1dV0Hg0bNlTJkiX1zjvvaNu2bTp37pw2b96sV1991f6FpwCQmljOBQCQQHh4uH29w39q0KCBpk2bpi+//FKffvqpnn/+ebm4uKhChQr69ttvVaxYMUlS37595ezsrJCQEF26dEnu7u4qXbq0vvrqK+XLl0/58uXTF198oS+//FLfffedYmJilD9/fnXp0kWvv/56Wp8uAAAAkMCYMWO0bNkyh233euRevXrp7bffTtJxnnrqKVWqVEk5cuQwXbc7f/78GjNmjD755BO1bNlSpUqV0tChQ+Xt7a3du3erS5cumj17tmrXrq1PP/1Un332mRYtWiQvLy81btxYAwYM+Nc5VK9e/YE9fGKaNWum3Llza+7cuXrvvfd05coVOTk5qWDBgmrWrJm6dOkiDw8PSXrg3Nzc3DRr1iyNGzdOffv21e3bt1WgQAENHDhQHTt2/Nf537vv+PHjNWDAAN28eVN58uRR06ZN1adPn3+9LwCkBJvxz8/yAAAAAAAAAAAAO5ZzAQAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgwiW9J5BZhIXdTO8pPDbc3JwVHR2X3tNAKqG+1keNrY8aWxv1TTu5c3um9xQyLXrztMHrgfVRY2ujvtZHja2PGqeNpPTlXImODMVmc/wnrIX6Wh81tj5qbG3UF8A9vB5YHzW2NuprfdTY+qhxxkKIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAADIkMaNG61Roz5I72kAAAAAGQL9cfqxGYZhpPckMoOwsJvpPYXHgs0mubo6KyYmTvxmWg/1tT5qbH3U2Noepb79+vXUvn3/kSTFxcUpPj5erq6u9v3z5y/Vk0/mS8np/qsxY0YoOjpKH34YnGaP+bBy5/ZM7ylkWvTmqY/Xe+ujxtZGfa2PGmd8j9ofp3SNx4wZobVrV8vFxSXBvlmzFujppwvJMAwtWDBX06d/pr59B6pVq7amx5sxY5pmzvxKgwe/r5Ytn3PYFxERrlatmsvXt4I+/XT6o08+lSWlL0/4rAEAAAAPadKkz+z/PmPGNO3cuV3Tp88yHR8bG5toAw8AAABYQUbsjxs2fOZfLzIZNKivDMOQp2eOJB3P29tHa9euThCib9iwTjlyeD3SXBMTHx8vSXJySvvFVXjnAgAAkBlEx8jp+o1UfQibJJurs5xi4hTnlUNyc33gfR6Gv39V9e49QPPmzdJzzz2vzp3f1Lp1azRr1te6fPmSvL1z6bXXOunZZ1tLcrya/Icflmnp0kV66aVX9NVXX+jWrVuqW7eehg4dIWdn54eey+XLlzRx4jgdOLBPrq5uqlmztnr37i8Pj2y6c+eOxo8P1s6d2xQVFa1ixYqrf/9BKlmytK5evaKQkI+0b9/vio+PU9my5fXuu++l6VX2AAAAUJr0x/eLT+Ee+UH9sY/P3/1xUFDq98eSVK6crzp2fEPt2j2bpPEVK1bWjh1bdfHiRT355JP27evWrVb16jV16dJF+7adO7fryy+n6uzZM8qWLbuee66NXn+9q33/2rWrNWPGNIWHX1WJEiXVv/9glShRUqtXh2rBgjmqVauOlixZqHnzlihv3if17bffaM2albp2LUJFixZT794DVKZMuWSdd1IQogMAAGR00TFyX7xatuiYVH8om01yNiTDzVV32gWmeJC+ZcvPmjXrO3l5een8+XMaNeoDBQdPUJ06dbVz53YNGtRXFSpUUqFChR3u5+LiokuXLuj48aNasGCpzpw5o65dO6h+/WdUr16Dh57H4MEDVKRIUS1cuEJRUXc1bNggjRs3Rh9++JEWLZqvP//8Q/PnL5WHRzYtXvydPv74I3399bf6+usvFRsbq6VLV8rZ2VnTpn2qTz+drNGjx6XE0wMAAICkSMP++J9So0fesiXx/tjfv65++22n+vXrLT+/1O+PJalTpy4PNT5r1qyqUaOW1q1brQ4dOkuSzpw5rUuXLum559roxx9XS5Lu3Lmj994bpO7de6l163b6449j6t79DZUr56vq1WvqxIk/9PHHYxQSMkV+fhU1b95svftuPy1atEKSdOXKFbm5ZdG6db/I2dlZS5Ys1A8/LNPHH09WwYJPa/HiBerT5y0tWrRCOXPmTNa5PwhfLAoAAIA0U79+Q3l7e8vJyUn58uXXqlUb5e9fTzabTTVr1la2bNl17NiRRO8bGRmpLl16KEuWrCpevIQKFSqi06dPPfQcjh8/qmPHjqhHj97Knj27cuV6Qq++2lG//LJJMTExioyMlJOTk9zcssjFxeX/r+6Z/f9zuC0nJ2e5ubkpS5Ysevvt/gToAAAASLZ/649r1aqTJv3xo2jaNFBr16623163bo0CAhrLyel/V8O7u7tr2bI1atWqrZycnFSyZGkVLlxYR4/+V5K0atUPqlathipXrmrvv3v27KuYmL//SHLr1k29/PKrcnFxkc1m06pVP6h163YqXryEsmTJopdffk1ubq7atm1Lqp0nV6IDAABkdP9/xUtaLOfi4uqs2FRazkWSw7InhmFo0aL5Wrduja5cuSLJUHR0tL1Zvl+OHF7y8PCw33Zzc1NUVNRDz+H8+fNyd/fQE088Yd+WP/9TiomJUVjYZbVu3U5btvysVq2aq1atOvL3r6+GDZ+RzWbTq6920sCBvdWmTQvVru2vBg2eUc2atR96DgAAAHgEadQf3y+ll3OR0rY/3rRpo7ZscexdnZ2dtX598sPnWrX8NW7caP33v4dUpkw5rV//oz74YJROnTrpMG7t2lVaunSRLl++pPj4eMXExKhOnXqSpHPnzurJJ/Pbx2bJklXPPNPYfjt7dk9ly5bdfvvChXMqWLCgwzk8+WR+XbhwPtnn8SCE6AAAAJmBm6vic+dK1Yew2STD1VnxMXGSkTqP4eLyvzcda9as1KJFCzRu3ERVqFBJTk5Oatmysel9U/ILhGw2231bDPv2J598Ut9+u1C//bZb27dvVUjIR9q0ab1Gj/5YJUqU0qJFP2jHjm3asWOrhg17R61bt1OvXn1TbG4AAABIgjToj9OCWX9csWIlZcniqiZNAkzv+7D98YO+WDQ5XFxcFBDQWGvXrlZcXLwkm8qWLe8Qou/du0dTpkzQqFFjVadOPbm4uKhr1w73Hcn8DYira8I/XNzfzxtGKr2B+X8s5wIAAIB0ceTIf+XnV0GVKlWRk5OTwsIu68aN66n+uAUKPKXIyNv/f3XP386ePSs3tyzKnTuP7ty5o7i4WNWoUUt9+w7UuHGT9PPPP+nGjRu6efOmnJ2dVb9+Q7377nsaNGiYQkOXpfqcAQAAYH3398eXL6dNf/yomjVroc2bN2nTpvVq0qRZgv1HjhzW008XUv36AXJxcVFU1F2dOXPGvj9//gI6ffq0/XZMTIzmzp2liIiIRB+vQIGndPr0X/bbcXFxunjxvAoUeCoFz8oRIToAAADSRZ48efXXX3/pxo3runLliiZMGKvcufMoLOxyqj5u8eIlVKZMOU2b9qkiI2/r0qWLmjPnGzVq1EQuLi4aOnSgJk78WJGRtxUXF6ejRw/Ly8tL2bNn15tvdtSsWV8rKipKMTExOnbsqPLnL5Cq8wUAAMDj4f7++OOPP0qT/vhRlS1bXu7u7lq9eqUaN04YoufJk1dhYZd18eIF3bhxQx99NFK5c+fWlSthkqQWLZ7Tf/6zR9u3/6rY2FgtWjRfS5YslKenZ6KP17JlKy1btkQnTvypqKi7mjNnpuLjDfvyMKmB5VwAAACQLlq1el579+5WmzYtlD9/AQ0cOET/+c9vmjt3lry9fR75+Imt+fj662/qtdc6acSIMRo/PlhBQU2UI4eX6tVroB49ekuS3n33fU2cOFZt2rRQfLyhIkWK6qOPJsjJyUkjR47VlCnjtWjRAjk52VSqVFl98MHoR54rAAAAcH9/PGTIe9q9e5fmzEmZ/vjf/P77XvXv30uSFB0drUmTQvTJJ38vuzhp0mcPvH/TpoH69ddfVLDg0wn2NWjwjLZs+VmvvfaifHx81LNnX9WsWVsTJ45TrlxPqEuX7ho0aJgmTgxRePgVFS9eUuPGTZKLS+LR9XPPtdGlSxfVu3d3xcREq2TJ0po6dZpp6J4SbEZqLxhjEWFhN9N7Co8Fm01ydXVWTEyc+M20HuprfdTY+qixtVHftJU7d+o1+VZHb576eD2wPmpsbdTX+qix9VHjtJOUvpzlXAAAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAIB0d+HCefn7V9Vff51K76kAAAAA6e7ChfOqXr0S/XEGQYgOAACAR/b22900btzoRPdt2LBWjRvXVWTk7WQfv23bIC1fviTZ9wcAAADSUlr0x/Xr11BAQG2Hn7Ztg+xjIiMjNXLk+0m6WKVXrzfl719V586dTbBv8+af5O9fVTNmTEv2fDM7QnQAAAA8sqCgVvrpp/WKirqbYN/q1SsVENBYHh7Z0mFmAAAAQNpLi/64X7939NNP2xx+liwJlSRduRKmN954VU5OSY9/vb19tHbt6gTb165dI29vn0eaa2JiY2NT/JipxSW9JwAAAIAHi4uNVPTti6n6GDabFOPirNjYOLl6PClnF48k37d+/QBNmhSizZs3qUmT5vbtV66E6bffdumzz77S9evXFBLykfbu/U3x8XHy86uod94Zqty58zzy3Pft+12ffTZZp06dlI+Pj559trVefvk12Ww2/fXXKU2YMFZHj/5XTk7OqlKlqgYNek85cuTQwYP7NWXKBJ06dVJubq6qVy9A/fq9Izc3t0eeEwAAAFJPWvTH93PLlvQeOb3744iICPXs2UfFipXQjz+uStJ9atSopbVrV6tz5zft227evKm9e/eoatXqDmO/+26uli5dpIiIcOXN+6S6d++lunUbSJLi4uI0bdqnWr06VLGxsapWrabeeWeocuTIoTFjRsjFxUXnzp3VpUsXtXDhct24cUNTpozXnj07FRcXr4oVK6l//3fl45PrkZ+HlEKIDgAAkMHFxUbq5Lahio+NTPXHstlsMgxDTi4eKlL7oyS/SciSJYuaNGmm1atDHd4krFmzSgULFlL58n766KMPde3aNS1evEI2m01Dhryjzz6bohEjxjzSnMPDr6p//57q2bOvWrR4VidPntDAgW/LwyObWrV6XpMmfaz8+Qto/PhPFBsbo+DgUZozZ6Z69uyj0aOHq0mT5vryy29048YNvffeIIWGLtfzz7/wSHMCAABA6knL/vifHqZHTs/+WJJKlCipEiVK6sKF80m+T8WKlbRr1w4dOLBPvr4VJEmbNm1Q1arV5O7ubh/3n//8pi+//FRffjlTJUuW0g8/LNOHH76nZcvWyNPTUytWfK9fftms6dNnK2dOb3344TBNmDBWH374kSRpy5bNGjZsuGrUqC1JGjt2lCIjb2vWrAVycXHVRx99qCFDBmratJmP/DykFJZzAQAAQIoICmqtvXv36NKl/10R9OOPKxUU9JwkaeDAIQoJmaJs2bLLwyOb/P3r6ejR/z7y427YsFZ58uRVmzbtlCVLFpUuXUZNmwZqw4a1kv5eC9LFxVVubm7y8MimDz/8SD179rHvc3V1k7Ozs7y9vTV16jQCdAAAAKSI9OqPk8vJyVmNGjVxuHJ93bo1aty4ucO4ChUqKTR0vUqXLiMnJyc1btxUd+/e1V9/nZQkrVy5XK1bP6/8+QvIw8NDffoMVLNmgfb758mTV7Vq+cvJyUk3blzXli0/q0uXHvL29pGnp6feeKObDh068FB/AEhtXIkOAACQwTn//xUvabGci0syl3ORpOLFS6hkydL68cdV6tjxDR08eEDnz59T06YtJEknT57Q559P0fHjx3T37h3FxcWlyEdVz58/r4IFCzlsy5//Kf3880+SpK5du+u9997Vzp3bVLu2vxo1amq/sqZHj7cVEvKR1q5dpdq1/dW0aQsVK1b8kecEAACA1JNW/fH9HmY5Fyn1++NJk0L0yScTHbZVrlxV48d/kuRj3K9Zsxbq0+ct9e37jq5evaoTJ/5UrVp19Ouvm+1jYmKiNX3659q2bYsiIsLt26OjoyVJ586d1ZNP5rdvz5+/gPLnL2C//eST+ez/fvHiBRmGoYIFC9q3FSjwlCTpwoXzypfvf8dJT4ToAAAAmYCzi4fcvYqm6mPYbJKrq7NiYuJkGMk7RsuWz+m77+apY8c3tGZNqOrWbaCcOXNKkoYNe0d+fhU1ZkyIsmfPru+/X6z5879Nsbk7MmT7/43VqtXU99+v0tatW7Rt26/q1etNvf12P7Vt+5KaN2+pmjX/flOwZctmde78ikaNGqd69RqkyLwAAACQOtKiP04Jqdkf9+v3jlq1apui8y1ZsrRy586tbdu26PTp06pfPyDB9wXNnTtbW7f+oo8/nqxixYorKipKjRr533ck8zcUrq4JI2lbwoY+Q2E5FwAAAKSYJk2a6erVMB08uF8//7zR/lHViIgIXbx4QW3bvqjs2bNLkv7441iKPGaBAk/p9Om/HLadPXvWfgXLtWvXlC1bdjVp0lwjRozR6693VWjoCvs+b29vBQW10scfT1Lz5i21evUPKTIvAAAAID3640fVtGmgfv75J23atF5NmjRLsP/IkcOqXbuuihcvIZvNlmDe+fMX0OnTp+23z549o0WLFiT6WPnyFZCTk5NDP3/27N/3vdfPZwSE6AAAAEgxHh7Z1LBhI02ZMkEeHtlUtWoNSZKnp6fc3T108OABxcXFadWqH3T8+DHdvHlDUVF3H+kxGzVqqrCwy1q+fIliYmJ08OABrV27Ws2atVBU1F299FIrrVjxvWJjY3Xnzh2dPPmn8ucvoEuXLqp16+batu1XxcfH68aN6zp79ozDR00BAACAR5Ee/fGjatKkuXbt2q5r166pYsXKCfbnyZNXf/xxTHfv3tWZM6c1d+4sZc2aVVeuhEn6++r7FSuW6vTpvxQZGakvvvhE+/f/nuhjeXp6qn79AM2YMU3Xr1/T9evX9NVXX6py5arKm/fJ1DzNh0KIDgAAgBQVFNRK//3vIbVo8az9Y5kuLi4aOHCw5s2bpRYtntGRI//VmDEfK0eOnHr11aR9keekSSEKCKjt8HPgwD55e3vro4/Ga9myJWratIHGjBmuLl26q3nzlsqSJas++mi8QkOXq3nzAD3/fEtFR8eof/9Bypv3Sb333kh9+eVUNWlST+3bt1W+fPn1xhvdUvPpAQAAwGMmtfrjfzNr1tcKCKit9u2flyR16vSyAgJqa9asrx9439y586hEiVJq1KhposusvPba64qPj1fLlo00atQH6tGjt5o2DdT48WO1fftWtWrVVg0aPKMePTqrdevmstlsGjhwiOnjDRgwWNmyZVPbts/q1VdfUPbs2TVq1Njkn3wqsBlGcle8fLyEhd1M7yk8FlJiLVZkXNTX+qix9VFja6O+aSt3bs/0nkKmRW+e+ng9sD5qbG3U1/qosfVR47STlL6cK9EBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJjI0CH62bNn9cYbb6hixYqqVauWQkJCFB8fn+jY2bNnq2HDhvLz81O7du106NChRMdt3LhRpUqV0s6dO1Nz6gAAAICl0JsDAADgcZVhQ3TDMNSrVy95e3tr8+bNmjdvntasWaNZs2YlGLt+/XpNnjxZwcHB2rlzp+rXr69u3bopMjLSYVxkZKQ++ugjeXh4pNFZAAAAAJkfvTkAAAAeZxk2RD9w4ICOHj2q9957T15eXipatKjefPNNLVy4MMHYxYsXq23btqpZs6bc3d3Vs2dP2Ww2bdy40WHc1KlTVatWLXl7e6fVaQAAAACZHr05AAAAHmcu6T0BM4cPH1aBAgWUM2dO+7ayZcvq1KlTunXrlrJnz+4wNjAw0H7bZrOpdOnSOnTokIKCgiRJR48eVWhoqEJDQ7Vt27ZkzclmS965IOnuPcc819ZEfa2PGlsfNbY26gsz9OaPH14PrI8aWxv1tT5qbH3UOGPJsCF6RESEvLy8HLbdux0REeHQqEdERDg09PfGhoeHS/r746fDhw9X//79k32li5ubc7Luh4djs0nOzs6y2STDSO/ZIKVRX+ujxtZHja2N+sIMvfnjh9cD66PG1kZ9rY8aWx81zlgybIj+MGwmf5K5t33x4sVycXFRmzZtkv0Y0dFx/OUnDdx7YYiNjeMFwoKor/VRY+ujxtZGfZES6M2tgdcD66PG1kZ9rY8aWx81zlgybIieK1cuXbt2zWFbRESEJMnHx8dhu7e3d6JjS5YsqfDwcH3yySeaPXv2I8+JX9i0Yxg831ZGfa2PGlsfNbY26ov70Zs/vng9sD5qbG3U1/qosfVR44whw36xqK+vr86fP29vziVp//79Kl68uLJly5Zg7MGDB+234+LidPjwYfn5+Wnz5s2KiIhQ+/btVaNGDdWoUUMXLlzQW2+9pVGjRqXZ+QAAAACZFb05AAAAHmcZ9kr0MmXKyM/PT6NHj9bw4cN14cIFTZ8+XW+99ZYkqVmzZho9erSqVq2ql156SX369FGjRo3k6+urzz//XFmzZlVAQIDi4+NVq1Yth2O/+OKLGjx4sGrXrp0epwYAAABkKvTmAAAAeJxl2BBdkqZMmaIPPvhAdevWVbZs2dS+fXu1b99eknTy5ElFRkZKkurVq6dBgwZpyJAhunr1qsqXL6/p06crS5YskiR3d3eH4zo7O8vHxyfBlyMBAAAASBy9OQAAAB5XNsNgVZ2kCAu7md5TeCzYbJKrq7NiYvjSBCuivtZHja2PGlsb9U1buXN7pvcUMi1689TH64H1UWNro77WR42tjxqnnaT05Rl2TXQAAAAAAAAAANIbIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwESGDtHPnj2rN954QxUrVlStWrUUEhKi+Pj4RMfOnj1bDRs2lJ+fn9q1a6dDhw7Z90VFRWn06NHy9/dXlSpV1KFDBx07diytTgMAAADI9OjNAQAA8LjKsCG6YRjq1auXvL29tXnzZs2bN09r1qzRrFmzEoxdv369Jk+erODgYO3cuVP169dXt27dFBkZKUn6+OOPtXfvXi1evFhbt25V4cKF1bNnzzQ+IwAAACBzojcHAADA48wlvSdg5sCBAzp69KhmzZolLy8veXl56c0339TMmTPVuXNnh7GLFy9W27ZtVbNmTUlSz549tXDhQm3cuFFBQUHKnj273n33XeXLl0+S9Nprr2nhwoW6dOmS8ubNm+Q52Wwpd35I3L3nmOfamqiv9VFj66PG1kZ9YYbe/PHD64H1UWNro77WR42tjxpnLBk2RD98+LAKFCignDlz2reVLVtWp06d0q1bt5Q9e3aHsYGBgfbbNptNpUuX1qFDhxQUFKR+/fo5HPv8+fPKkiWLfHx8kjwfNzfn5J8Mksxmk5ydnWWzSYaR3rNBSqO+1keNrY8aWxv1hRl688cPrwfWR42tjfpaHzW2PmqcsWTYED0iIkJeXl4O2+7djoiIcGjUIyIiHBr6e2PDw8MTHPf69esaM2aMOnToIFdX1yTPJzo6jr/8pIF7LwyxsXG8QFgQ9bU+amx91NjaqC/M0Js/fng9sD5qbG3U1/qosfVR44wlw4boD8Nm0kHfv/3y5cvq0qWLfH19E1wBkxT8wqYdw+D5tjLqa33U2PqosbVRXzwKenNr4fXA+qixtVFf66PG1keNM4YM+8WiuXLl0rVr1xy2RURESFKCj3p6e3snOvaf406fPq2XXnpJNWrUUEhIiJyd+QgoAAAAkBT05gAAAHicZdgQ3dfXV+fPn7c355K0f/9+FS9eXNmyZUsw9uDBg/bbcXFxOnz4sPz8/CRJ4eHh6ty5s9q1a6dhw4bJySnDnjYAAACQ4dCbAwAA4HGWYTvWMmXKyM/PT6NHj9aNGzd09OhRTZ8+Xa+88ookqVmzZtqzZ48k6aWXXtLSpUu1Y8cO3b59WxMnTlTWrFkVEBAgSZo4caLKlSunHj16pNv5AAAAAJkVvTkAAAAeZxl6TfQpU6bogw8+UN26dZUtWza1b99e7du3lySdPHlSkZGRkqR69epp0KBBGjJkiK5evary5ctr+vTpypIliyRp6dKlcnZ2lq+vr8PxR40apVatWqXpOQEAAACZEb05AAAAHlc2w2Bp+qQIC7uZ3lN4LNhskqurs2Ji+OZhK6K+1keNrY8aWxv1TVu5c3um9xQyLXrz1MfrgfVRY2ujvtZHja2PGqedpPTlGXY5FwAAAAAAAAAA0hshOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAADI5M6dO6edO3em9zQAAAAASyJEBwAAADKpa9euqUuXLnrmmWf0xhtvSJLCwsIUFBSkS5cupfPsAAAAAGsgRAcAAAAyqfHjxysuLk6LFy+Wk9Pfrb2np6fKlCmjMWPGpPPsAAAAAGtwSe8JAAAAAEiebdu2acGCBcqbN69sNpskKWvWrBoyZIiaN2+ezrMDAAAArIEr0QEAAIBM6urVq8qdO3eC7e7u7rp79246zAgAAACwHkJ0AAAAIJMqUqSIfv755wTbv/vuOxUpUiTtJwQAAABYEMu5AAAAAJnUW2+9pb59+yogIEBxcXEaOXKkDh06pAMHDmjy5MnpPT0AAADAErgSHQAAAMikmjRporlz58rJyUnFixfX77//rqeeekrfffedmjRpkt7TAwAAACyBK9EBAACATCg+Pl4HDx6Un5+fJk6cmN7TAQAAACyLK9EBAACATMjJyUkdOnRQXFxcek8FAAAAsDRCdAAAACCTCgoK0qxZs2QYRnpPBQAAALAslnMBAAAAMqmwsDD99NNPmj59ugoUKCA3NzeH/d999106zQwAAACwDkJ0AAAAIJPy9vZWvXr10nsaAAAAgKURogMAAACZVHBwcHpPAQAAALA8QnQAAAAgEzt48KCWL1+uv/76SzabTUWLFlXbtm1VvHjx9J4aAAAAYAl8sSgAAACQSW3fvl0vvPCCtmzZImdnZ9lsNm3cuFGtW7fW/v3703t6AAAAgCVwJToAAACQSX3yySd699131bFjR4ftX3zxhcaPH69vv/02nWYGAAAAWEeyrkSPjY3VokWL7Le3bt2qXr16aeLEiYqOjk6xyQEAAAAwd+LECb300ksJtnfs2FFHjx5NhxkBAAAA1pOsEH3ixImaPn26JOnSpUvq1auXsmfPrl27dmnixIkpOkEAAAAAiXNzc9Pt27cTbL97965iY2PTYUYAAACA9SQrRF+zZo2mTZsmSVq1apXKlSunsWPHasqUKVq3bl2KThAAAABA4qpUqaIPPvhAly5dsm+7ePGiRowYoerVq6fjzAAAAADrSNaa6NeuXVOxYsUkSTt27FD9+vUlSXnz5lVERETKzQ4AAACAqcGDB6tTp05q0KCB3N3dZbPZFBkZqeLFi+vzzz9P7+kBAAAAlpCsED1Hjhy6evWqsmTJot27d6tfv36SpKtXr8rd3T1FJwgAAAAgcU8++aRWrVqlX3/9VX/99ZcMw1CRIkXk7+8vJ6dkfegUAAAAwH2SFaI3atRInTp1kqurq0qVKqUyZcooKipKo0ePVo0aNVJ6jgAAAABMXLp0SaVLl7Z/OvT48eM6d+6cChYsmM4zAwAAAKwhWZenvPvuuwoKClK1atU0depUSVJ8fLyuX7+uoUOHpugEAQAAACRux44dCgwM1J49e+zbtm/frqCgIO3YsSMdZwYAAABYh80wDCMlDnTr1i1lz549JQ6VIYWF3UzvKTwWbDbJ1dVZMTFxSpnfTGQk1Nf6qLH1UWNro75pK3duz0c+xgsvvKDAwEB17NhRNpvNvn3BggVaunSplixZ8siPkRHRm6c+Xg+sjxpbG/W1PmpsfdQ47SSlL0/WlegnT55U+/bt7bffe+89Va1aVfXq1dORI0eSc0gAAAAAD+n48ePq0KGDQ4Au/R2u//nnn+k0KwAAAMBakhWijx49WkWLFpUk7dq1S6tXr9bs2bP1yiuvaMKECSk2ubNnz+qNN95QxYoVVatWLYWEhCg+Pj7RsbNnz1bDhg3l5+endu3a6dChQ/Z9UVFR+uCDD1S9enVVqlRJvXv3Vnh4eIrNEwAAAEgPnp6eOnnyZILtR44cUbZs2VL0sejNAQAA8LhKVoh+8OBBDR48WJK0adMmNW3aVDVq1FCnTp104MCBFJmYYRjq1auXvL29tXnzZs2bN09r1qzRrFmzEoxdv369Jk+erODgYO3cuVP169dXt27dFBkZKUkKCQnR3r17tXTpUv3000+Kjo5m7XYAAABkes8995y6d++uWbNmacOGDVq3bp2++OIL9ejRQ61atUqxx6E3BwAAwOPMJTl3iomJsa9/vmPHDnXs2FGS5ObmpqioqBSZ2IEDB3T06FHNmjVLXl5e8vLy0ptvvqmZM2eqc+fODmMXL16stm3bqmbNmpKknj17auHChdq4caOaN2+uZcuWady4cSpYsKAkadCgQWrevLkuXbqkvHnzJnlO931KFqng3nPMc21N1Nf6qLH1UWNro76ZT9++feXu7q7p06fbr+b28fHRq6++qjfffDPFHofe/PHD64H1UWNro77WR42tjxpnLMkK0Z9++mn98ssv8vDw0PHjx1WvXj1J0v79+5UnT54Umdjhw4dVoEAB5cyZ076tbNmyOnXqVIIvMT18+LACAwPtt202m0qXLq1Dhw6pXLlyunXrlsqVK2ffX7RoUbm7u+vQoUNJbtTd3Jwf/aTwQDab5OzsLJtNfGmCBVFf66PG1keNrY36Zj7Ozs5666239NZbb+nWrVuS5NAnpxR688cPrwfWR42tjfpaHzW2PmqcsSQrRO/WrZt69Oih+Ph4vfnmm/Lx8dH169fVq1cvdejQIUUmFhERIS8vL4dt925HREQ4NOoREREODf29seHh4YqIiHC47z05cuR4qLUXo6Pj+MtPGrj3whAbyzcPWxH1tT5qbH3U2Nqob+YSHR2t6Ohoe1/s5uamFStWKCIiQk2aNFHhwoVT7LHozR8/vB5YHzW2NuprfdTY+qhxxpKsEL158+aqUqWKwsPDVbp0aUl/N76DBg1SUFBQik4wKWwmHbTZ9qTuvx+/sGnHMHi+rYz6Wh81tj5qbG3UN+O7cOGCXnnlFQ0YMEAtWrSQJHXp0kW7du1StmzZ9MUXX2jRokUqUaJEms+N3txaeD2wPmpsbdTX+qix9VHjjCFZXywqSXny5FFcXJxWr16tNWvW6OjRoykaoOfKlUvXrl1z2HbvyhUfHx+H7d7e3omO9fHxUa5cuSTJYb9hGLp27Zp9HwAAAJCZTJ48WYUKFVKNGjUkSXv27NGuXbs0a9Ys/fbbb3r++ef1zTffpNjj0ZsDAADgcZasEP3q1atq1aqV2rZtq/79+6tfv35q3bq1Xn75Zd28eTNFJubr66vz58/bm3Pp7zXXixcvrmzZsiUYe/DgQfvtuLg4HT58WH5+fipYsKBy5sypQ4cO2fcfPXpU0dHRKl++fIrMFQAAAEhLu3bt0vDhw/XEE09IkjZv3qzy5cvbv8yzS5cu2r17d4o9Hr05AAAAHmfJCtHHjRunHDlyaP78+dq+fbu2bdum2bNnyzAMTZgwIUUmVqZMGfn5+Wn06NG6ceOGjh49qunTp+uVV16RJDVr1kx79uyRJL300ktaunSpduzYodu3b2vixInKmjWrAgIC5OzsrBdeeEGTJ0/WmTNndPXqVQUHB6tZs2b2Nx0AAABAZnLt2jWHNc9/++03Va9e3X77ySeffKg1xh+E3hwAAACPs2Stib5nzx599913ypMnj31b9erVNX78+BT7YlFJmjJlij744APVrVtX2bJlU/v27dW+fXtJ0smTJxUZGSlJqlevngYNGqQhQ4bo6tWrKl++vKZPn64sWbJIkt5++23dvn1bbdq0UVxcnBo2bKgRI0ak2DwBAACAtJQlSxZFR0fLzc1N0dHROnTokF5//XX7/ujoaLm4JKvVN0VvDgAAgMeVzTAefmn6qlWratu2bXJzc3PYHh0drWrVqmnfvn0pNsGMIiwsZZapwb+z2SRXV2fFxPDNw1ZEfa2PGlsfNbY26pu2cuf2TPZ9X3rpJXXp0kWNGjVSaGiohgwZoq1bt8rLy0uStH37do0dO1YrVqxIqelmKPTmqY/XA+ujxtZGfa2PGlsfNU47SenLk3V5SuHChbV69Wq1atXKYfuaNWv09NNPJ+eQAAAAAJKobdu2GjRokCpVqqS9e/cqKCjIHqDv379fI0aM0LPPPpvOswQAAACsIVkheo8ePdSrVy8tX75cxYsXlyT98ccf2r17t0JCQlJ0ggAAAAActW3bVrdu3dLWrVv16quvqlevXvZ9q1atUt68edW1a9d0nCEAAABgHclazkX6e130OXPm6PTp0zIMQ0WKFFGHDh1UqVKllJ5jhsBHRtMGH1WxNuprfdTY+qixtVHftPUoy7n8m1u3bil79uypcuyMgt489fF6YH3U2Nqor/VRY+ujxmkn1ZZzkf5eF71q1aoJtlevXl27du1K7mEBAAAAPAKrB+gAAABAWnNK6QPevXs3pQ8JAAAAAAAAAEC6SPEQ3WazpfQhAQAAAAAAAABIFykeogMAAAAAAAAAYBWE6AAAAIAFXbhwIb2nAAAAAFjCQ32x6IABAx44JjY2NtmTAQAAAJAymjVrpn379qX3NAAAAIBM76FC9MuXLz9wTOXKlZM9GQAAAAApwzCM9J4CAAAAYAkPFaLPmTMnteYBAAAAIIkmTpz4wDHx8fFpMBMAAADA+h4qRAcAAACQ/r755hvlzJlTbm5upmPi4uLScEYAAACAdRGiAwAAAJlM9+7ddfz4cU2ZMsV0TIUKFdJwRgAAAIB1OaX3BAAAAAA8nO7du+vMmTNatmyZ6RjWRAcAAABSBleiAwAAAJmMi4uLvv76a0VERJiO6dGjRxrOCAAAALAuQnQAAAAgE/Lx8ZGPj4/pfkJ0AAAAIGWwnAsAAACQyfTu3TvBtk8++SQdZgIAAABYHyE6AAAAkMls3rw5wbYZM2akw0wAAAAA6yNEBwAAACyALxIFAAAAUgchOgAAAGABNpstvacAAAAAWBIhOgAAAAAAAAAAJgjRAQAAAAAAAAAw4ZLeEwAAAADwcGJiYjRgwIAHbpswYUJaTgsAAACwJEJ0AAAAIJOpUqWKLl++/MBtAAAAAB4dIToAAACQycyZMye9pwAAAAA8NlgTHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATGTYED0iIkL9+vVT5cqVVa1aNQ0bNkx37941Hb9q1So1bdpUvr6+atmypbZu3WrfZxiGPv30UzVo0ECVKlVSu3bttGfPnrQ4DQAAACDTozcHAADA4yzDhuhDhw7V1atXtW7dOq1cuVJHjhxRSEhIomMPHjyod999V3369NHu3bvVqVMn9ezZUxcuXJAkzZw5U99//72+/vpr7dq1Sw0bNtRbb72lW7dupeUpAQAAAJkSvTkAAAAeZxkyRL969ao2bdqkIUOG6IknnlDevHnVt29fff/994qOjk4wfunSpapXr54CAwOVNWtWtW3bVqVKldKKFSskSc7Ozho0aJCKFy8uV1dXderUSdevX9exY8fS+tQAAACATIXeHAAAAI87l/SeQGIOHz4sFxcXlSpVyr6tbNmyioyM1MmTJx223xtfr149h21lypTRoUOHJEkdO3Z02Hf+/HlJUr58+R5qXjbbQw1HMtx7jnmurYn6Wh81tj5qbG3UF4mhN3888XpgfdTY2qiv9VFj66PGGUuGDNEjIiKUPXt2OTn970J5Ly8vSVJ4eHii43PmzOmwzcvLS8ePH08wNjo6Wu+9954CAwMfqlF3c3NO8lgkn83299VJNptkGOk9G6Q06mt91Nj6qLG1UV8kht788cTrgfVRY2ujvtZHja2PGmcs6RaiL1++XEOHDk1035gxY0zvZ3uIP7/cP/bWrVvq2bOnXF1d//UxEhMdHcdfftLAvReG2Ng4XiAsiPpaHzW2PmpsbdT38UVvjvvxemB91NjaqK/1UWPro8YZS7qF6K1atVKrVq0S3bd161bdvHlTcXFxcnb++yqTiIgISVKuXLkSjPfx8bHvvyciIkI+Pj722+Hh4ercubMKFSqkkJAQubm5PfSc+YVNO4bB821l1Nf6qLH1UWNro76PH3pzmOH1wPqosbVRX+ujxtZHjTOGDPnFomXLllV8fLyOHj1q37Z//355enqqcOHCCcb7+vra11i858CBA/Lz85MkRUVFqVu3bvLz89OkSZOS1aQDAAAAjyN6cwAAADzuMmSI7u3trebNmys4OFhXrlzRuXPnNGnSJL344otydXWV9PcXEq1evVqS1K5dO23dulWrV6/W3bt3NWfOHJ0+fdp+Nc0333wjm82mESNGOKzlCAAAAODf0ZsDAADgcZdhu9YPP/xQefLkUePGjdW6dWvVqFFDffr0se8/c+aMrl+/LkkqWbKkxo8frylTpqhatWpaunSppk2bpieeeEKStHTpUh06dEgVKlSQr6+v/efzzz9Pl3MDAAAAMhN6cwAAADzObIbBqjpJERZ2M72n8Fiw2SRXV2fFxPClCVZEfa2PGlsfNbY26pu2cuf2TO8pZFr05qmP1wPro8bWRn2tjxpbHzVOO0npyzPslegAAAAAAAAAAKQ3QnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADCRYUP0iIgI9evXT5UrV1a1atU0bNgw3b1713T8qlWr1LRpU/n6+qply5baunVrouMOHTqksmXL6vvvv0+tqQMAAACWQm8OAACAx1mGDdGHDh2qq1evat26dVq5cqWOHDmikJCQRMcePHhQ7777rvr06aPdu3erU6dO6tmzpy5cuOAwLj4+XsOHD1fWrFnT4hQAAAAAS6A3BwAAwOMsQ4boV69e1aZNmzRkyBA98cQTyps3r/r27avvv/9e0dHRCcYvXbpU9erVU2BgoLJmzaq2bduqVKlSWrFihcO4BQsWyNPTU+XKlUurUwEAAAAyNXpzAAAAPO5c0nsCiTl8+LBcXFxUqlQp+7ayZcsqMjJSJ0+edNh+b3y9evUctpUpU0aHDh2y3w4LC9Pnn3+uuXPn6oMPPkjWvGy2ZN0ND+Hec8xzbU3U1/qosfVRY2ujvkgMvfnjidcD66PG1kZ9rY8aWx81zlgyZIgeERGh7Nmzy8npfxfKe3l5SZLCw8MTHZ8zZ06HbV5eXjp+/Lj9dnBwsF566SUVKVIkWXNyc3NO1v3wcGw2ydnZWTabZBjpPRukNOprfdTY+qixtVFfJIbe/PHE64H1UWNro77WR42tjxpnLOkWoi9fvlxDhw5NdN+YMWNM72d7iD+/3Bu7bds2HTp0SGPHjn24Sf5DdHQcf/lJA/deGGJj43iBsCDqa33U2PqosbVR38cXvTnux+uB9VFja6O+1keNrY8aZyzpFqK3atVKrVq1SnTf1q1bdfPmTcXFxcnZ+e+rTCIiIiRJuXLlSjDex8fHvv+eiIgI+fj4KDo6Wh9++KFGjBghNze3R5ozv7BpxzB4vq2M+lofNbY+amxt1PfxQ28OM7weWB81tjbqa33U2PqoccaQIZdzKVu2rOLj43X06FGVLVtWkrR//355enqqcOHCCcb7+vo6rLEoSQcOHFCLFi30+++/66+//lLfvn3t+27duqWDBw9q/fr1+uKLL1LzVAAAAIBMjd4cAAAAj7sMGaJ7e3urefPmCg4O1qRJkxQVFaVJkybpxRdflKurqySpY8eOevHFFxUYGKh27dqpbdu2Wr16tQICArR48WKdPn1arVq1Uo4cOfTzzz87HL9Pnz5q3ry5nn322XQ4OwAAACDzoDcHAADA4y5DhuiS7B/zbNy4sVxdXRUUFKQ+ffrY9585c0bXr1+XJJUsWVLjx4/XhAkT9O6776pYsWKaNm2annjiCUnSk08+6XBsNzc35ciRQz4+Pml3QgAAAEAmRW8OAACAx5nNMFhVJynCwm6m9xQeCzab5OrqrJgYvjTBiqiv9VFj66PG1kZ901bu3J7pPYVMi9489fF6YH3U2Nqor/VRY+ujxmknKX25UxrMAwAAAAAAAACATIkQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAAAAAAACACUJ0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYMJmGIaR3pMAAAAAAAAAACAj4kp0AAAAAAAAAABMEKIDAAAAAAAAAGCCEB0AAAAAAAAAABOE6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOhIUxEREerXr58qV66satWqadiwYbp7967p+FWrVqlp06by9fVVy5YttXXr1kTHHTp0SGXLltX333+fWlNHEqRkfQ3D0KeffqoGDRqoUqVKateunfbs2ZMWp4H7nD17Vm+88YYqVqyoWrVqKSQkRPHx8YmOnT17tho2bCg/Pz+1a9dOhw4dsu+LiorSBx98oOrVq6tSpUrq3bu3wsPD0+o08C9SssajR4+Wv7+/qlSpog4dOujYsWNpdRowkVL1/aeNGzeqVKlS2rlzZ2pOHUAqoi+3Pnpz66Evtz76cuujN8/EDCANde/e3XjttdeMsLAw4+LFi0abNm2MkSNHJjr2wIEDRrly5YxVq1YZd+7cMRYvXmxUqFDBOH/+vMO4uLg44/nnnzcqVapkLF26NC1OAyZSsr4zZswwGjZsaBw/ftyIjo42PvvsM6NatWrGzZs30/KUHnvx8fHGc889ZwwYMMC4du2a8eeffxoNGzY0ZsyYkWDsunXrjIoVKxrbt283IiMjjalTpxp16tQxbt++bRiGYYwaNcpo0aKFcfr0aSM8PNzo1q2b0a1bt7Q+JdwnJWs8cuRIo3Xr1sb58+eNO3fuGO+//77RqFGjtD4l/ENK1vee27dvGwEBAUbFihWNHTt2pNWpAEhh9OXWR29uLfTl1kdfbn305pkbITrSzJUrV4xSpUoZhw8ftm/75ZdfjIoVKxpRUVEJxo8YMcLo0aOHw7YXXnjB+OKLLxy2zZ071+jUqZPx6quv0qyno5Su76xZs4w1a9bY992+fdsoWbKk8dtvv6XSGSAx+/btM0qXLm1ERETYty1YsMBo0qRJgrFdu3Y1Ro8ebb8dHx9v+Pv7Gz/88IMRExNjVK5c2Vi/fr19/59//mmULFnSuHjxYqqeA/5dStXYMAxj4sSJDo3bsWPHqHE6S8n63jN27Fhj2LBhRsOGDWnUgUyKvtz66M2th77c+ujLrY/ePHNjORekmcOHD8vFxUWlSpWybytbtqwiIyN18uTJRMeXK1fOYVuZMmUcPr4SFhamzz//XB988EHqTRxJktL17dixo5o1a2bfd/78eUlSvnz5UmP6MHH48GEVKFBAOXPmtG8rW7asTp06pVu3biUY+8+a2mw2lS5dWocOHdLp06d169Yth/1FixaVu7u76UfSkDZSqsaS1K9fP9WoUcO+//z588qSJYt8fHxS9yRgKiXrK0lHjx5VaGioBgwYkOpzB5B66Mutj97ceujLrY++3ProzTM3QnSkmYiICGXPnl1OTv/7tfPy8pKkRNdfi4iIcHhhuTf+n2ODg4P10ksvqUiRIqkzaSRZatT3nujoaL333nsKDAykUU9jERER9jrec+92REREgrFmNb039v5j5ciRg/UX01lK1fh+169f15gxY9ShQwe5urqm7KSRZClZX8MwNHz4cPXv31/e3t6pN2kAqY6+3Proza2Hvtz66Mutj948cyNER4pavny5ypYtm+hPXFyc6f1sNluSH+Pe2G3btunQoUPq1q3bI88bSZOW9b3n1q1b6tq1q1xdXTVmzJhkzx2pz6zOD6r/w/x+IH0ltcaXL1/Wa6+9Jl9fX/Xr1y8tpoYU8KD6Ll68WC4uLmrTpk1aTgtAMtGXWx+9OczQl1sffbn10ZtnPC7pPQFYS6tWrdSqVatE923dulU3b95UXFycnJ2dJf3vL225cuVKMN7HxyfRv8T5+PgoOjpaH374oUaMGCE3N7eUPQmYSqv63hMeHq7OnTurUKFCCgkJodbpIFeuXLp27ZrDtnt1u/+jgN7e3omOLVmypP134Nq1a/Lw8JD091/Or127lujvB9JOStX4ntOnT6tTp0565plnNGTIEIcr4JD2Uqq+4eHh+uSTTzR79uzUnC6AFERfbn305o8X+nLroy+3PnrzzI3/gpBmypYtq/j4eB09etS+bf/+/fL09FThwoUTjPf19U2wJtuBAwfk5+en33//XX/99Zf69u2rGjVqqEaNGtq7d69GjRqlHj16pPapIBEpWV9JioqKUrdu3eTn56dJkybRpKcTX19fnT9/3uFN1f79+1W8eHFly5YtwdiDBw/ab8fFxenw4cPy8/NTwYIFlTNnzgTrt0VHR6t8+fKpfyIwlVI1lv735rpdu3YaNmwYjXoGkFL13bx5syIiItS+fXv7/3cvXLigt956S6NGjUqz8wGQMujLrY/e3Hroy62Pvtz66M0zufT9XlM8bvr162e8+uqrRlhYmHH27FmjRYsWxscff2zf36FDB2PVqlWGYRjG0aNHDV9fX2PVqlXGnTt3jG+//daoXLmyERYWZkRFRRkXLlxw+HnhhReMmTNnGlevXk2v03vspVR9DcMwPv/8c6Ndu3ZGXFxcupwL/ueFF14w+vfvb1y/ft04cuSIUadOHWPevHmGYRhG06ZNjd27dxuGYRibN282KlasaGzfvt24deuW8fHHHxsNGjQw7t69axiGYYwfP94IDAw0Tp8+bVy5csXo0KGD0adPn/Q6LfxDStV42LBhRu/evdPtPJC4lKhvZGRkgv/v1qtXz1i9erVx7dq19Dw9AMlEX2599ObWQ19uffTl1kdvnnkRoiNN3bhxw+jfv79RsWJFo1q1asbIkSONqKgo+/6GDRsa8+fPt99eu3at0aRJE6N8+fLGc889Z38xScyrr75qLF26NFXnj3+XkvV95plnjLJlyxrly5d3+Pnss8/S9JxgGBcuXDC6du1q+Pn5GbVq1TKmTp1q31eyZElj8+bN9tvz5883GjRoYPj6+hovv/yycezYMfu+qKgo48MPPzSqVq1qVKpUyejfv79x48aNND0XJC6laly6dGmjXLlyCf67XbZsWVqeDu6TUvW9X8OGDY0dO3ak6twBpB76cuujN7ce+nLroy+3PnrzzMtmGIaR3lfDAwAAAAAAAACQEbEoEgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAKS777//XqVKlUrvaQAAAACPNfpyAEicS3pPAACQvl577TXt2bNHLi6J/y9h3rx58vPzS+NZAQAAAI8X+nIAyLgI0QEAatasmSZNmpTe0wAAAAAea/TlAJAxsZwLAOCB6tatq6lTp2rQoEGqUqWK/P39NW7cOMXGxtrHbNy4Uc8//7yqVKmiBg0a6J133tGVK1fs+69cuaK+ffuqatWqqlq1qrp3764zZ844PM7+/fvVunVrVahQQU2bNtXmzZvT7BwBAACAjI6+HADSByE6AOCBXF1dNXfuXDVr1ky7du3SxIkTNX/+fM2ePVuStGfPHvXs2VOvvPKKtm3bpoULF+rixYvq3LmzDMOQJL3zzju6ffu21q1bp82bNytr1qzq1q2bfb8kzZ49W9OnT9euXbtUokQJDR482GE/AAAA8DijLweA9MFyLgAA/fjjj9qwYUOC7ZUrV7Y35JUqVVJAQIAkqXr16qpfv77Wr1+vN954Q99++62qV6+uNm3aSJLy5s2rfv366eWXX9bBgwfl7u6ubdu2acmSJfLx8ZEkDRkyRL/99puio6Ptj/fmm28qd+7ckqRWrVpp/fr1Cg8PV65cuVL1/AEAAICMgL4cADImQnQAQJLWXixevLjD7aeeekr79++XJJ0+fVqVK1d22F+4cGH7vixZskiSChYsaN+fN29eBQYGOtzn6aeftv+7m5ubJOnOnTsPcSYAAABA5kVfDgAZE8u5AACSJD4+3uG2YRj2JtwwDNlstkTv98/t9x/jfk5O/G8JAAAA+Df05QCQ9nhVBAAkycmTJx1unz17Vvnz55ckFSlSRMePH3fY/8cff9j33bv65cSJE/b9YWFhmjFjhm7cuJGKswYAAACshb4cANIeIToAIEn27NmjDRs2KDo6Wjt37tQvv/xi/9hnhw4dtHv3bi1fvlzR0dE6e/asJk6cqIoVK6pMmTIqXry4atasqcmTJ+vy5cuKjIzUhAkTtGjRImXPnj2dzwwAAADIPOjLASDtsSY6AMD0C4wkqVu3bpKktm3b6scff9SgQYPk4eGh1157Tc8//7ykv7/oKCQkRF9++aXef/99PfHEE6pbt64GDBhgP05ISIhGjx6t5s2by2azqUqVKvrqq6/4qCgAAADw/+jLASBjshmGYaT3JAAAGVtAQIBat26tt99+O72nAgAAADy26MsBIH3wZ0YAAAAAAAAAAEwQogMAAAAAAAAAYILlXAAAAAAAAAAAMMGV6AAAAAAAAAAAmCBEBwAAAAAAAADABCE6AAAAAAAAAAAmCNEBAAAAAAAAADBBiA4AAAAAAAAAgAlCdAAAAAAAAAAATBCiAwAAAAAAAABgghAdAAAAAAAAAAAThOgAAAAAAAAAAJggRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogNAGhs8eLBKlSpl+jN16lSH8StXrlTlypX1wgsvPNTxhwwZYjqmV69eCR6rVKlSGj9+fPJOKgXExsZq4cKFeuGFF1SzZk35+vqqfv366t+/v44ePZpu8wIAAEDm8/rrr6thw4aKj483HfPCCy+oWbNmSTre4MGDVadOHfvtgIAA9evX71/vM3XqVJUqVSppE/4Xr732WpLfCzyqe3Pu0KGD6Zjg4GCVKlVKgwcPTpU5fP/99ypVqpT+/PPPVDk+ACSHS3pPAAAeRz4+Pvrhhx8S3efh4SFJunv3rkaPHq0ff/zRvi2pPDw8tHbtWg0fPlxZs2Z12Hfjxg1t3rxZ7u7uDtt//fXXh36clDR8+HCtXr1aAwcOVI0aNZQ1a1adPHlSn3zyidq3b6/ly5erYMGC6TY/AAAAZB7t2rVTv379tGPHDtWuXTvB/j///FP79u3TO++8k6zjL1myRK6uro86zUQ1bdpUw4YNU7169SQpwUU2qc3d3V27d+/WhQsXlC9fPod98fHxWrVqVbLeN1y+fFl169bV/v37lSVLFtNxgYGBqlu3rnx8fB76MQAgtXAlOgCkAycnJ+XOnTvRn2zZskmStm/frgMHDmjp0qUqUqTIQx2/TJkycnV11caNGxPsW7t2rQoVKpSgKf3nYz8swzAUGxubrPtK0u3bt7Vs2TJ17NhRr7zyiooXL66nnnpKdevW1bRp01SgQAHt27cv2cdPipiYmFQ9PgAAANJOo0aNlDNnTi1dujTR/cuXL5erq6tat26drOP7+PjI09PzUaaYqIiICJ06dcphW86cOZUzZ84UfywzPj4+KlmypEJDQxPs27Fjh+7evavSpUs/9HH/85///Ov++Ph4xcXFKWvWrMqdO7ecnZ0f+jEAILUQogNABlW2bFktXLhQhQoVeuj7Ojs7q2HDhlqxYkWCfT/88IOeeeaZBNvvX87l0qVL6tevn6pVq6bKlSurY8eOOnDggH1/QECARo8erWHDhqlChQr6+eef7fcbMGCAatasqfLly6tRo0b69NNPFRcXZzrfew1zVFRUgn33rtpv2bJlkudmGIa+/vprNW3aVOXLl1eNGjXUu3dvnTlzxj5m6tSpqlq1qjZu3Ch/f3/17dvXvm/u3Llq3ry5ypcvr9q1a+uDDz7QzZs3TecPAACAjMXNzU3PPvusNmzYkKCPi4+P1w8//KCGDRsqV65cCg8P1+DBg1W3bl35+voqICBAY8eO1d27d02Pf/9yLn/++adee+01+fr6qk6dOpowYUKC/jcuLk6ffPKJmjZtKj8/P9WpU0e9e/fW2bNnJUk7d+5UzZo1JUldu3ZVQECApITLuURHR2vChAkKCAhQ+fLlVadOHQ0ZMkTh4eH2MQMGDNBzzz2nPXv2qHXr1vLz81NAQIAWLVqUpOevUaNGpu8l6tevLxeXhAsb/FsPPXXqVPXu3VuS5OfnZ18KplSpUpo+fbq6d+8uPz8/HTt2LNHlXDZt2qQ2bdrI19dX/v7+GjVqlG7fvm3fv3DhQgUFBalixYqqVq2aOnfurEOHDiXpXAEgKQjRASCDyps3b4KlWB5G8+bNtXXrVl29etW+7eLFi9q9e7cCAwP/9b7R0dHq2LGjzp07p6+++kpLliyRj4+PXn/9dV24cME+bsuWLfLw8FBoaKhq1aqlqKgodejQQQcPHtSECRO0cuVKde7cWdOmTdOECRNMH8/T01OVK1fWzJkzNWrUKO3fv980dE/K3KZOnarJkyfr5ZdfVmhoqD755BOdPHlSHTt21J07d+zHiouL09y5c/Xll19qxIgRkqRp06ZpzJgxeu655xQaGqrg4GD98ssv6tmz5wOfcwAAAGQc7dq10927d7V69WqH7du2bdPFixfVtm1bSVL//v21a9cuTZo0yb4k4pIlSzR58uQkPU5sbKy6d++uy5cv65tvvtHs2bMVHR2tJUuWOIybNm2apk2bpj59+ujHH3/UF198obNnz9rD5UqVKtmXbpkwYUKC+9/z/vvva968eerVq5dWrVqlMWPGaPv27erWrZsMw5Akubq6Kjw8XFOmTNF7772nlStXqlKlShoxYoTDhSVmAgMD9ccffzgE0Xfv3tW6devUokWLBOMf1EN37txZnTt3liT99NNPGjZsmP2+S5cuVeXKlbVmzRoVK1YswbG3b9+ut956S/Xq1dOKFSsUEhKiDRs22IP47du3a8SIEercubNWrVqlOXPmKGfOnOrcubND7w8Aj4IQHQAsyt/fXzlz5nT4GGZoaKhKlCjxwC842rBhg06ePKng4GBVrFhRRYsW1ciRI1WvXj2HpvvWrVsaPHiwChUqpGzZsmnDhg06deqURo8erTp16qhw4cJq3769WrdurQULFig6Otr0MSdOnKhq1app7ty5ateunapVq6YuXbpowYIFDs3vg+YWHR2tWbNmKSgoSJ06dVKRIkVUo0YNjRw5UufOndO6devsx4qMjFSHDh1Uvnx55c6dWzExMfrqq68UGBio7t27q0iRIqpfv76GDRumnTt3pvqSMgAAAEg5JUuWVIUKFbRs2TKH7cuWLVO+fPlUt25dSVJISIgWLVqkqlWrKn/+/Kpfv778/f21ZcuWJD3O7t27dfr0aQ0dOlTVqlVT8eLFNWTIkATLJ7766qtat26dAgMDlT9/fvn5+alt27Y6dOiQwsPD5ebmJi8vL0lSjhw5El0T/NKlS/rhhx/UuXNntWnTRoUKFVKDBg00cOBA7d+/X7/99pt97OXLl/X++++rSpUqevrpp/X6668rLi5OBw8efOA5FStWTOXKldPy5cvt23766Se5urran7d7ktJDZ8uWzb6O+hNPPOGwFE62bNn05ptvqmDBgnJzc0swlxkzZsjX11d9+/ZV0aJFVatWLb3//vvKnj27oqOjdfDgQbm7uysoKEgFChRQ6dKlNXr0aE2fPp0lYQCkGEJ0AEgHV69eVaVKlRL92bRpU4o8hrOzswIDAx2+wDQ0NFRBQUEPvO/+/fvl7u7ucCWIp6enJk6cqOrVq9u3lS5d2qEx3b9/v5ycnFS5cmWH41WqVEmRkZE6efKk6WPmy5dPc+bM0erVqzVkyBDVqFFDv//+u0aMGKEWLVrY14Z80NxOnDih27dvq1q1ag7Hr1ChgpydnfXf//7XYXv58uXt/37ixAndvHlTNWrUcBhTq1YtSdLevXtN5w8AAICMp23btvrPf/6jEydOSPr7IpANGzaoTZs2cnL6OxK5ceOGRo4cqfr166ty5cqqVKmS1q9fr2vXriXpMY4dOybJsa+U/u6B7zdz5kw1bdpUVatWVaVKlRQcHCzp77XQk+LgwYOKj49P0Ovee6x/9roeHh4qWbKk/fa9gP769etJeqxnn31Wq1atsn/3UWhoqJo1a5bgC1UftYe+/3m734EDB1SuXDmHbY0aNVJwcLDc3Nzk7+8vwzD04osvat68eTpx4oQ8PDxUoUKFREN5AEiOhItYAQBSXc6cObVw4cJE9+XOnTvFHufZZ5/Vt99+qz/++EPx8fE6duyYw9riZm7evKksWbI8cFyOHDkcbt+6dUuenp4Jrvi4N+7WrVsPPGaxYsVUrFgxderUSdHR0Vq6dKnGjBmjjz/+WJ9//vkD53bvMe69SbjHyclJ2bNnTzCHf57DvTUbR48ebX9D809hYWEPnD8AAAAyjhYtWig4OFjLli3TgAEDtHr1akVHR+v555+X9PcX3Hft2lXx8fF6//33VaRIEbm4uGj8+PFJvoDiXn9570rre+7vR4cNG6bNmzdr4MCBqlGjhrJmzap169Y5fC9RUh/r/mPfu/3PXvf++dhsNkmyL/nyIC1bttTHH3+srVu3qkKFCtqyZYtmzZqVYNyj9tD3v6dI7Pj/tsxlmTJltHDhQs2cOVOffvqpRo4cqcKFC2vw4MFq2LDhvx4bAJKKEB0A0oGzs3OyvjD0Yfn6+qpo0aJauXKl4uLi7B9RfRBPT0/dvHlT8fHx9it0kuLe/eLi4hyC9HtX8fzzY5v3u3r1qnLlyuWwzc3NTS+//LJ+/fVXHTlyJElzu/cY919hExcXp5s3b/7rHO69+Rg4cKDq169vemwAAABkDtmyZVPz5s21YsUK9e/fXytWrFDt2rVVoEABSdK+fft07tw5ff311w7LlCT2hfdm7oXVd+7ckbu7u337P69kj46O1saNG/XGG2+oQ4cO9u33gu2kMut1713JnpL96hNPPKFatWpp5cqVunDhgvLkyaMqVaokGJfaPbSnp+cDPxVQsmRJBQcHyzAMHTx4UNOnT1evXr20evXqNHnfBcD6WM4FACzu2Wef1S+//KJNmzYlaSkXSfLz81NcXJzD1TdRUVF67bXXEnwx0z9VqFBB8fHxCa7a2bNnj7Jnz64iRYoker9vvvlGderUsQfl/2QYhs6fP6+8efMmaW5FixaVp6endu/e7XCc3377TfHx8fL19TWdf5EiRZQjRw6dO3dOhQoVsv889dRTio2NTXRdSgAAAGRsbdu21aVLl7R+/Xrt3bvX/oWi0v+u3P5nn3fhwgXt2LEjyVdsFy1aVJISLBv4z340MjJScXFxDo8TGxvr8P1F/2T22L6+vnJyckrQ6+7Zs0fS371ySnruuee0ZcsWbdy4US1btkw09H/YHjqpz+s9vr6+Dmu9S3+vz/7KK6/o1q1b+u233+zfXWSz2eTr66vg4GDFxsbq6NGjD3nGAJA4QnQAyKBu3rypsLAwhYWFKSYmRrGxsfbbt2/fTvJxgoKCdOTIEZ06dUpNmzZN0n2eeeYZFS5cWCNGjNCePXt08uRJDR8+XIcPH1aFChX+9X7FihXT+++/r+3bt+vkyZOaNWuWQkND9frrrydYP/GeVq1a6emnn9abb76pxYsX648//tC5c+e0e/du9e/fX8ePH1ePHj2SNDdXV1d17txZoaGh+vbbb3Xq1Clt27ZN77//vooWLapGjRqZzt/FxUVdunTR/PnzNWfOHP311186cuSIhgwZohdeeEGXL19O0vMHAACAjKNy5coqXry4Ro4cKS8vLz3zzDP2feXLl5eLi4tmzpyp06dP69dff9Xbb7+t5s2b69q1azp06JCio6P/9fg1a9ZU3rx5FRwcrL179+r48eMaNWqU7t69ax+TM2dOFS5cWN9//72OHj2qgwcP6u2337Zf2b17927dvHnTflX39u3bdfDgwQSBc+7cudWmTRt98803Wr58uf766y9t2LBBEyZMUI0aNVI8RG/cuLGioqK0detW0wtyktpD31u2ZePGjfrzzz+TPIfOnTvr9OnTGjVqlP7880/t2rVLH330kby8vJQ9e3Zt2rRJPXr00Lp163Tu3DmdOHFCn3/+udzd3f/1AhoAeBgs5wIAGdSYMWO0bNkyh23+/v6SpF69euntt99O0nGeeuopVapUSTly5FDOnDmTdJ8sWbJo1qxZGjt2rLp37674+HiVK1dOM2fOtH/0NTFubm6aNWuWxo0bp759++r27dsqUKCABg4cqI4dO5rez8fHR999951mz56t2bNna9y4cbpz5458fHxUuXJlzZ8/3/6GIClz69Gjh7Jmzaq5c+dq3Lhx8vT0VN26dfXOO+888MuFunXrpmzZsmnevHkaN26cPDw8VKlSJc2bN0958uRJ0vMHAACAjKVt27YaO3asXn/9dYd+MH/+/BozZow++eQTtWzZUqVKldLQoUPl7e2t3bt3q0uXLpo9e/a/HjtLliyaNm2aPvzwQ3Xo0EE5cuRQ69at1bFjRwUHBysmJkaurq4KCQnRiBEj1K5dOz355JPq2rWrWrVqpaNHj+rjjz+Ws7Oz2rZtq2bNmmnBggVau3atNmzYkODxRowYoVy5cmnKlCm6fPmyvL291bhxYw0YMCDFnzd3d3c1btxYR44cUYkSJUzHJaWHDgwM1LJlyzR06FAFBARo0qRJSZpD7dq19emnn+qzzz7TokWL5OXl5XC+ffv2lbOzs0JCQnTp0iW5u7urdOnS+uqrr5QvX75HfxIAQJLNeNjP0QAAAAAAAAAA8JhgORcAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMOGS3hPILMLCbqb3FB4bbm7Oio6OS+9pIJVQX+ujxtZHja2N+qad3Lk903sKmRa9edrg9cD6qLG1UV/ro8bWR43TRlL6cq5ER4Ziszn+E9ZCfa2PGlsfNbY26gvgHl4PrI8aWxv1tT5qbH3UOGMhRAcAAAAAAAAAwAQhOgAAAAAAAAAAJgjRAQAAAAAAAAAwQYgOAAAAAAAAAIAJQnQAAAAAAAAAAEwQogMAAAAAAAAAYIIQHQAAAAAAAAAAE4ToAAAAAAAAAACYIEQHAAAAAAAAAMAEIToAAAAAAAAAACYI0QEAAAAAAAAAMEGIDgAAgIc2btxojRr1QXpP46H9/vteBQTUVnR0dHpPBQAAAEAmYTMMw0jvSWQGYWE303sKjwWbTXJ1dVZMTJz4zbQe6mt91Nj6qHHm1K9fT+3b9x9JUlxcnOLj4+Xq6mrfP3/+Uj35ZL40q++YMSO0du1qubi4JNg3a9YCPf10IRmGoQUL5mr69M/Ut+9AtWrV1vR4M2ZM08yZX2nw4PfVsuVzDvsiIsLVqlVz+fpW0KefTk/xc3kUuXN7pvcUMi1689TH6731UWNro77WR42tL7VrvGvXDvXv30vPPddG77wzNOUfIBNJSl+e8J0LAAAALGXSpM/s/z5jxjTt3Lld06fPMh0fGxsrZ+fUbRMbNnxGH34YbLp/0KC+MgxDnp45knQ8b28frV27OkGIvmHDOuXI4fVIc5X+fk4SC/0BAACAzCg0dLkCAhpr48Z16t27v7JkyZpmj50Ze2uWcwEAAHhU0TFyCruaZj+KjknxU/D3r6pFixaoefPG+vbbbyRJ69atUfv2z6tRI3+1a/ecfvhhmX38mDEjNHz4EEnSDz8sU8eOL2vNmpVq06aFmjSpr1Gj3ldcXFyy51OunK9CQqYoS5YsSRpfsWJlHTlyWBcvXnTYvm7dalWvXtN+e+/ePfL3r6qoqChJ0rlzZ9Wnz1t65pk6atOmhZYs+c4+9t5z8txzTe3Pyb59v+vNNzupSZP6euml1po//1vxwU4AAABIStn3BZevynb573+m9HuD69ev6ddfN6tr1x7KmdNbP//8k33f3bt3FRw8Us2aNVBg4DP6+OMx9qUQ/21f27ZBWr58if04O3Zsk79/VfvtxHrrf3u/IUnz589R69aBaty4nvr3f1sXL17Qvn3/UYMGNXX9+jX7uKiou2rcuK62b9/60M9FUmWuyB8AACCjiY6R++LVsqVCsG3GcHPVnXaBkpvrgwc/hF9++Vnz5y+Uh4enzp8/p1GjPlBw8ATVqVNXO3du16BBfVWhQiUVKlTY4X4uLi66dOmCjh8/qgULlurMmTPq2rWD6td/RvXqNUjWXDp16vJQ47NmzaoaNWpp3brV6tChsyTpzJnTunTpkp57ro1+/HF1ovcbMWKoypXz08cfT9TZs2f11ltvqGDBQqpRo5YkacuWnzVr1nfy8vJSePhV9e/fUz179lWLFs/q5MkTGjjwbXl4ZFOrVs8n6zwBAABgEanwvsBmk5wfcL1Gct4b/PjjKhUvXlIFCz6txo2badWqH9S0aaAk6Ztvpun06VNasGCZbDab3nmnt775Zrq6d+/1r/uSYsuW//XWD3q/sX37r5o//1tNmvSpChcuqsmTQzR8+FB9+eU3ypMnrzZt2mBf8nHnzh3KmtVd1arVSPJz8LC4Eh0AAACSpPr1G8rb20dOTk7Kly+/Vq3aKH//erLZbKpZs7ayZcuuY8eOJHrfyMhIdenSQ1myZFXx4iVUqFARnT59Kk3n37RpoNau/V9Yvm7dGgUENJaTk3Oi4//447j++9/D6ty5q7JkyapixYorOHiC8uZ90j7m7+fEW05OTtqwYa3y5MmrNm3aKUuWLCpduoyaNg3Uhg1rU/3cAAAAgJSycuUKNWv2d2jerFkL/f77Xp0/f06SFBq6Qi+99Jq8vb2VM2dODRnygWrWrP3AfUnxz976Qe83QkNXqFGjpipRopT+r707j7Oqrv8H/hqGAREQAckF+bpkLigo7pobqIALimtmaqYpbqVk7uaKkqG4VFpahu1plpW4m5G5L5UCSZbmhgvCoALKMnN/f/hjaoSjoMPMcHg+Hw8fD+85n7n3feY9Ht/zmnPPrampyZe/fGw+//lDUqlUMmjQ7rnrrjsannfcuD9m5513XaK3iHElOgDAJ/H/r/xo89bbzfaS9V1WaPKr0JNklVVWbfj3SqWSG2/8ee666/a8+eabSSqZM2dO5s5d+JU1K6zQJcsvv3zD43bt2jXcMmVh7rvv3tx/f+OBu7q6Onffff/Hrn+bbbbLJZeMyD/+MSEbbLBh7r77jpxzzoX5z3+eX+j6V155OR07dmx0z/RNN9280Zr//Z5Mnjw5vXqt0Wj/aqut3ujtrwAALKOa+PeCqiRta6ozb25dPuxi9MX93WD8+Kfz0ksvZuedByVJevZcPRtuuFFuu+0POfDAg/POO29n1VX/OwOvvfY6SZK33367cN+iWpzfN1555eVsvPEmDeu7du2anXbaOcn7wf+YMT/Ia6+9mpVW6pEHHrg/o0d/e7FqWVxCdACAT6pdTep7dG/pKj6xmpr/Dt+3335rbrzxF7nkktHZeON+adOmTfbcc9fCr23TZvHe4PhRHyz6cbRt2zYDBuyaO++8LXV19Umq0rv3RoUhepJ81O3M27Zt/AtJVdUCz5CqBTcCALAsasLfC6qqkkpNdern1n3kzLo4br31ltTX1+egg4Y2bJs9e3Zef/31HHDAQUk+fEZe1FoW9rlB/ztbL8rvG0WfPdSz5+rZaKM+ufvuO7LuuutnxRW7pnfvjRatsI9JiA4AwAKeeeYf6dt34/Trt1mSZMqUN/L222+1cFUfbfDgPXLWWaemuro6AwcO/tC1q63WM7NmzczUqW+me/eVkiTjxt2XTp06ZbPNtlhgfc+eq+eRRx5stO3ll19Oz56rN90BAADAEvLuu+/m3nvvzqmnnpnNNtuyYfvs2bNzxBGHZNKkZ9KpU+e89NILWW+99ZO8/3vBc8/9K7vvPuRD97Vv3z5z585reM433nj9Q2v5qN83VlutZ1566cWGx7W1tRk79nf53Oe+kJqamgwatEf+8IdbMnny5I+c+5uCe6IDALCAT31q5bzwwgt5++238uabb+ayy76ZHj0+lSlT3mjp0j5U794bpUOHDrnttluz664fPkx/5jPrZv31e+e6667Ju+++m+ee+1cuuWRE4W1odtllUKZMeSO33PLrzJ07N+PHP50777wtgwfvsSQOBQAAmtS9996Z9u3bZdCgPbLqqqs1/LPmmmtlhx12ytixv8uee+6dn//8J3nzzTfz1lvTc8UVo/L8888lyYfu69Xr//LYY48kSd55553cccfYD63lo37f2HPPvXPvvXdnwoTxmTt3bsaMuS5/+tMfG949u/POA/P888/lvvvuzsCBuy2pb1kDV6IDALCAoUP3y5NPPpZ9990jq63WM1//+hn561+fyE9/OiZdu3Zboq/9t789ma997YQkyZw5c3L55aNy1VXvv83z8su/+5FfP2jQ7vnLX/6cXr3+7yPXnnvuiFx66cjsuecu6dJlxRx++JHZdtvtFrq2a9euufjiS/Pd716Rq666PCuvvHK+/OVjsttuey7eAQIAQAu49dbfZ9ddd2t0G8f59thjSE477Wv51a9uydtvv5VDDtk/bdpUZ8cd++fLXz4mSXLkkcM+ZN8xGTHi3Oy77x5ZeeVVss8+B+Tpp/+eefPmLfQDPz/q94299tonhx56eM4665TMnDkjG23UN+eff3HD13fu3Dnbbrtdpkx5I6uv3msJfcf+q6pSdHMZGpky5Z2WLmGZUFWV1NRUZ24T3++J1kF/y0+Py0+Py01/m1ePHp1buoSlltl8yXM+KD89Ljf9LT89Lj89/mjHH39UBg3aPXvttc8nep5FmctdiQ4AAAAAwFKhvr4+v/3trzN58isZNGjJ38olEaIDAAAAALCUGDhwh3TvvlIuvPCStG+/XLO8phAdAAAAAIClwj33/KXZX7NNs78iAAAAAAAsJYToAAAAAABQQIgOAAAAAAAFhOgAAAAAAFBAiA4AAAAAAAWE6AAAAAAAUECIDgDAh3r11cnZbrvN88IL/2npUhYwZswPcsIJR7d0GQAAwAfcccfY7L//kCZb15KqKpVKpaWLWBpMmfJOS5ewTKiqSmpqqjN3bl38ZJaP/pafHpefHi+dvvKVYVl99V457bSzF9h3zz135pJLRuR3v7sjHTt2XGh/X311cg44YK/87Ge/zhprrLnAc+y//5BMmfJGqqurG23v1q17fv3rPyRJZs2alUsvHZm77rq98HnmO+GEo/O3vz2ZX/3qlvTsuXqjfePG/TFnnXVqvvSlo3LkkcMW/ZvQCvXo0bmlS1hqmc2XPOf78tPjctPf8tPj8ltSPf7g7F5TU5O11/50jjrquGy66eZN90JLkUWZy12JDgBQckOGDM0f/3h3Zs9+b4F9t912awYM2DXLL9/xE73G8OGn5I9/fLDRP/MD9DffnJIjjzwkbdos+ujZtWu33HnnbQtsv/PO29O1a7dPVGulUkldXd0neg4AAFha/e/s/vvf35n+/XfNKaecmJdffqnRunnz5rVQha1P25YuAABgaVc3b1bmzHyt2V6vXcdVUt12+UVev+OOA3L55aMybtx9GThwt4btb745JU888Wi++93r8tZb0zNq1MV58sknUl9fl759N8kpp5yZHj0+9Ynrra2tzfHHn5hPf/ozueOOsYv0NVtttU3uvPO2HHHEf2/V8s477+TJJx/P5ptv2bDthz/8fh555KFce+2YJMkjjzyU73zn8kye/Ep69VojX/nK8Gy22RZ58snHc9ppwzNs2PH5/ve/m0suuTybbrp5brnl5tx00y/yxhtvpFevXjn66OOz9dbbfuJjBgBg2dOUvxdUVSVz21Zn3rwPvxJ9cX83+KD27ZfLgQd+Pr/5zY159NGH881vXpgNN+yTRx55KF26dMmVV16T1157LaNHX5K//vXxdOrUOdts89mccMLwLL/8+69bNIPfdtsf8r3vfSe///2dqaury3e/e0XuvfeuzJw5M6uv/n85/vgTs8UWWzValyTPPffvXHHFqPzzn8+kU6fOGTBglxx11HGpqanJ73//29x884056KAv5LrrrsmMGTOy/fY75Mwzz1vgnbFNSYgOAPAJ1M2blecfPDP182Y122u2abt81tr24kUeltu3b5+BAwfnttv+0ChEv/32senVa41stFHfXHzx+Xnrrem55ZZbM29efc4445R897tX5rzzLvrE9X7mM+vmM59ZN6++OnmRv2aTTfrl0UcfztNP/z19+mycJLnvvnuy+eZbpEOHDgv9mtra2px99qk57bSzs9NOO+eee+7M6aefnF//+vdJ3r+S5qWXXsytt96Tmpqa/OUv43LNNVflm98cnY026pv77rsnp502PD/60c+z9tqf/sTHDQDAsmNJ/F5QVVWVj7oT9+L+blCkrq6+IYS+5547c9FF38q6666fJDn//DOz3nq9c8EFIzNr1syce+6ZufrqK/P1r5/xkTP4fPfee1fuvffuXHfdj9Ojx6dy3333ZsSIc3Pzzbc2Wjdnzpx87WsnZLfd9sy3vnVF3nxzSr7+9a+mTZvqHHPMCWnbtm1ef/3VPPvspPziFzfnpZdeylFHHZYdd9w5O+yw0yf6HnwYt3MBAFgGDBmyT5588vG8/vp/r4y5445bM2TI3kmSr3/9jIwadWU6deqUjh07ZrvtdsikSf9oqXLTpk11dtllYKMr1++66/bsuutuhV9zzz13ZrXVVs8uuwxK27ZtM3jwHjnjjHNSX//+Lx5z587NXnvtm/bt26dNmzYZO/b32WWXQenXb7PU1NRk4MDd8ulPr5P77rtniR8fAAC0BrNmzcrPf/6TvPXW9IZ3ZK6//gZZf/3eadOmTf71r2czceKEHHvsV7LccsulW7fuOfLIYQ23XvyoGfy/rzMzbdq0Sfv27VNVVZUBA3bJb397W9q2bXyN98MPP5hZs2bliCOOznLLLZfVV++V/fb7XO65585GNX/5y8emffvlss46n8kaa6yVF1/8zxL9PrkSHQDgE6j+/1d+tObbuSTJOut8Juuuu37uuGNsvvjFIzN+/NOZPPmVDBq0R5Lk+eefy9VXX5lnn/1n3nvv3dTV1S3WrVwuv3xUrrpqdKNtm266eS699KrFqvN/DR68R0488bicdNIpmTp1ap577t/ZZpvP5i9/GbfQ9a+88nJWXXXVRtsGDNglSfL88+8/XmWVVRr2TZ48OX379mu0frXVVl+sK+YBACBp+t8LqqqStkvodi7/O7u3a9cu66yzbkaP/nZWXvn9WXmVVf47U7/yysupq6vLbrv1b/QcdXV1mT59+ofO4P9r550H5Y47bst+++2ZLbbYOtttt0N23XVw2rVr12jdq6++klVWWSU1NTUN23r2XD2vv/5a6uvrkyQrrNCl4VYy849h9uzZi/U9WFxCdACAT6i67fLp0GXtli7jI+2559755S9/li9+8cjcfvsfsv32O2XFFVdMkpx11inp23eTjBp1Wdq3Xz4333xTfv7zHy/ycw8ffkqGDt2/Setdd93106NHjzz44P158cUXs+OOAxYYsj/oo97u2rZtTaPHVVULPEOqFtwIAAAfqSl/L6iqSmpqqjN37oeH6B/HR83u/zszt2lTlQ4dOuTuu+8vXP9RM3iSdO7cOddc88M89dTf8uCDf8m1116d3/3uN7n66h8ssPaD8/j855+/vU2b5r+5itu5AAAsIwYOHJypU6dk/Pin8qc/3dtwK5fa2tq89tqrOeCAz6VTp85Jkn/9658tWWqDQYN2z5/+9Mfcd9/dGThw8IeuXW21nnnppRcbbbvxxl/k5ZdfWuj6nj1Xz4svvtBo28svv5yePVf/ZEUDAEBJ9Oy5et59991MnvxKw7ZZs2bmrbemJ1n0GXzOnDl5991Z2Xjjfjn22K/kuutuyMSJ4/Pcc/9a4PVeffXVzJkzp2HbK6+8lFVXXa1FL3YRogMALCOWX75j+vffJVdeeVmWX75jNt98qyTvXxXSocPyGT/+6dTV1WXs2N/n2Wf/mXfeeTuzZ7/XojUPHLhbHn30oUyfPj2bbLLph67ddddBeeON13Prrbdk3rx5ueeeO3PddVc3eqvn/xoyZO/ce+9deeqpv2XOnDkZO/b3eeGF57PzzgOXxKEAAMBSZ+2110mfPn1z1VWX5a23puedd97Jt751cUaMODfJos/gl18+Kt/4xhl5663pqVQq+cc/JqSmpmaBW0hutdW26dSpU8aM+UFmz34v//nP8/n1r3+V3Xbbs9mOeWGE6AAAy5AhQ4bmH/+YkD322KvhSo62bdvm618/PT/96ZjsuutOeeaZf+Sii76VFVZYMYcccuAnfs0xY36QAQO2zcEH75ckOfzwz2fAgG0zZsyCb938oB49PpXPfGa97LLLoI+88qRr12658MJL8stf/jwDB+6Yn/zkR7n44kvTrVv3ha7fZpvt8qUvHZXzzjsre+yxc2655dcZPfo76dXr/xb/IAEAoKTOPfei1NfXZ//9h+SAA4Zk3rx5OfPM85Is+gx+3HFfTefOnXPwwftl4MAdcv311+WCC0ama9dujdbV1NTk4osvzRNPPJbddts5p5xyUgYN2j2HHvql5jrchaqqLMpNa8iUKe+0dAnLhCV5vydanv6Wnx6Xnx6Xm/42rx49Ord0CUsts/mS53xQfnpcbvpbfnpcfnrcfBZlLnclOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFGjVIfrLL7+cI488Mptsskm22WabjBo1KvX19Qtde8MNN6R///7p27dvDjjggEyYMGGh6+69996st956eeSRR5Zk6QAAUCpmcwAAllWtNkSvVCo54YQT0rVr14wbNy4/+9nPcvvtt2fMmDELrL377rtzxRVXZOTIkXnkkUey4447ZtiwYZk1a1ajdbNmzcrFF1+c5ZdfvpmOAgAAln5mcwAAlmVtW7qAIk8//XQmTZqUMWPGpEuXLunSpUuOPvro/OhHP8oRRxzRaO1NN92U/fffP1tvvXWS5Pjjj8+vfvWr3HvvvRkyZEjDum9/+9vZZptt8uCDD36smqqqPv7xsGjmf499r8tJf8tPj8tPj8tNfyliNl/2OB+Unx6Xm/6Wnx6XXxl7PGLEeZkzZ3YuuGBkfvjD7+fhhx/KddeNaemyFkmrDdEnTpyYnj17ZsUVV2zY1rt37/znP//JjBkz0qlTp0Zrd99994bHVVVVWX/99TNhwoSGQX3SpEn5wx/+kD/84Q8fa1Bv16764x8Mi6yqKqmurk5VVVKptHQ1NDX9LT89Lj89Ljf9pYjZfNnjfFB+elxu+lt+elx+S6rHe++9e954Y0qqq/97g5Ju3bplp512zjHHHLdE3yXYpk1V2rSpSk1Nddq0adPw70uDVhui19bWpkuXLo22zX9cW1vbaFCvra1tNNDPXztt2rQk77/99Nxzz83Xvva1dO3a9WPVM2dOXan+8tNazT8xzJtX538CJaS/5afH5afH5aa/FDGbL3ucD8pPj8tNf8tPj8tvSfW4UkmGD/969tln////uJIXXvhPzjnnjMyYMSNnnPGNpnuxD6ivr6S+vpK5c+tSX1/f8O9Lg1Yboi+OqoIJev72m266KW3bts2+++77iV7HSan5VCq+32Wmv+Wnx+Wnx+Wmv3wSZvNycT4oPz0uN/0tPz1uXd6rm5Wps19rkueqStK2pjrz5tblw1rcvf0qWa568a8e/+/PTVXWWGOtfOELX8xVV12WSuUbee211zJ69CX5618fT6dOnbPNNp/NCScMb7hK/ZFHHsp3vnN5Jk9+Jb16rZGvfGV4NttsiyTJXXfdnjFjfpA33ng9Xbt2z6GHHp699tpngdee//pLy89vqw3Ru3fvnunTpzfaVltbm+T9txj8r65duy507brrrptp06blqquuyg033LAkywUAgNIymwMAfLj36mblO8+ckdl17zbZcy7KrVzaV3fICeuP/FhB+v+aM2dO3o/uk/PPPzPrrdc7F1wwMrNmzcy5556Zq6++Ml//+hmpra3N2WefmtNOOzs77bRz7rnnzpx++sn59a9/n5kzZ+bCC8/JyJGX5bOf3T6PPPJQTj31pGy8cb+sscaan6i+ltbmo5e0jD59+mTy5MkNw3mSPPXUU1lnnXXSsWPHBdaOHz++4XFdXV0mTpyYvn37Zty4camtrc3BBx+crbbaKltttVVeffXVHHfccbnwwgub7XgAAGBpZTYHACin+vr6PPvsP/Ozn92QgQMH51//ejYTJ07Iscd+Jcstt1y6deueI48cljvvvC1Jcs89d2a11VbPLrsMStu2bTN48B4544xzUl9fyaqrrpaxY+/NdtvtkKqqqmy99bbp2LFT/vnPZ1r4KD+5Vnsl+gYbbJC+fftmxIgROffcc/Pqq6/m2muvzXHHHZckGTx4cEaMGJHNN988Bx10UE488cTssssu6dOnT66++uost9xyGTBgQOrr67PNNts0eu7Pfe5zOf3007Ptttu2xKEBAMBSxWwOAPDhlqtePiesP3KpuJ3L5ZePylVXjU7y/gUPyy23XPbd98AceeSwPPDA/amrq8tuu/Vv9DV1dXWZPn16Xnnl5ay66qqN9g0YsEuS9wP5G2/8ee666/a8+eabSSqZM2dO5s6du1j1tUatNkRPkiuvvDLnnHNOtt9++3Ts2DEHH3xwDj744CTJ888/n1mzZiVJdthhh5x66qk544wzMnXq1Gy00Ua59tpr0759+yRJhw4dGj1vdXV1unXrtsCHIwEAAAtnNgcA+HDLVS+fnsuv3STPVVWV1NRUZ+7cpv/w2OHDT8nQoe9/sOijjz6cM844ObvvPiRt27ZNmzZV6dChQ+6++/7Cr68UFHT77bfmxht/kUsuGZ2NN+6XNm3aZM89d23a4ltIqw7RV1lllVx77bUL3Tdp0qRGjz//+c/n85///CI97x//+MdPXBsAACxLzOYAAOWz5ZZbZ7vtdswll4zIt7/9/fTsuXrefffdTJ78SlZbrWeSZNasmZk7d266dFkxq63WM48++lCj57jxxl9k2223yzPP/CN9+26cfv02S5JMmfJG3n77rWY/piWh1d4THQAAAACAJevEE0/Ov/71bH73u99k7bXXSZ8+fXPVVZflrbem55133sm3vnVxRow4N0my666D8sYbr+fWW2/JvHnzcs89d+a6667O8ssvn099auW88MILefvtt/Lmm2/mssu+mR49PpUpU95o4SP85Fr1legAAAAAACw53bp1zzHHHJ9rrrkqn/3s9jn33IsyevQl2X//Iamurs7mm2+VM888L0nStWu3XHjhJfnud6/M6NGj0qtXr1x88aXp1q17hg7dL08++Vj23XePrLZaz3z962fkr399Ij/96Zh07dqtZQ/yE6qqFN3EhkamTHmnpUtYJizJ+z3R8vS3/PS4/PS43PS3efXo0bmlS1hqmc2XPOeD8tPjctPf8tPj8tPj5rMoc7nbuQAAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQAEhOgAAAAAAFBCiAwAAAABAASE6AAAAAAAUEKIDAAAAAEABIToAAAAAABQQogMAAAAAQIFWHaK//PLLOfLII7PJJptkm222yahRo1JfX7/QtTfccEP69++fvn375oADDsiECRMa9s2ePTsjRozIdtttl8022yyHHXZY/vnPfzbXYQAAwFLPbA4AwLKq1YbolUolJ5xwQrp27Zpx48blZz/7WW6//faMGTNmgbV33313rrjiiowcOTKPPPJIdtxxxwwbNiyzZs1KknzrW9/Kk08+mZtuuikPPPBA1lxzzRx//PHNfEQAALB0MpsDALAsa7Uh+tNPP51Jkybl7LPPTpcuXbL22mvn6KOPzq9+9asF1t50003Zf//9s/XWW6dDhw45/vjjU1VVlXvvvTdJ0qlTp5x22mlZddVVs9xyy+XQQw/Niy++mNdff725DwsAAJY6ZnMAAJZlbVu6gCITJ05Mz549s+KKKzZs6927d/7zn/9kxowZ6dSpU6O1u+++e8PjqqqqrL/++pkwYUKGDBmS4cOHN3ruyZMnp3379unWrdti1VRV9fGOhUU3/3vse11O+lt+elx+elxu+ksRs/myx/mg/PS43PS3/PS4/PS4dWm1IXptbW26dOnSaNv8x7W1tY0G9dra2kYD/fy106ZNW+B533rrrVx00UU57LDDUlNTs8j1tGtXvRjV83FVVSXV1dWpqkoqlZauhqamv+Wnx+Wnx+WmvxQxmy97nA/KT4/LTX/LT4/LT49bl1Yboi+OqoI/yXxw+xtvvJEvf/nL6dOnzwJXwHyUOXPq/OWnGcw/McybV+cEUUL6W356XH56XG76S1Mwm5eD80H56XG56W/56XH56XHr0mpD9O7du2f69OmNttXW1ibJAm/17Nq160LXrrvuug2PX3zxxRx++OHZeeedc8YZZ6RNm8W/Hbwf2OZTqfh+l5n+lp8el58el5v+8kFm82WX80H56XG56W/56XH56XHr0Go/WLRPnz6ZPHlyw3CeJE899VTWWWeddOzYcYG148ePb3hcV1eXiRMnpm/fvkmSadOm5YgjjsgBBxyQs84662MN6QAAsKwymwMAsCxrtRPrBhtskL59+2bEiBF5++23M2nSpFx77bX5whe+kCQZPHhwHn/88STJQQcdlJtvvjkPP/xwZs6cmdGjR2e55ZbLgAEDkiSjR4/OhhtumGOPPbbFjgcAAJZWZnMAAJZlrfZ2Lkly5ZVX5pxzzsn222+fjh075uCDD87BBx+cJHn++ecza9asJMkOO+yQU089NWeccUamTp2ajTbaKNdee23at2+fJLn55ptTXV2dPn36NHr+Cy+8MEOHDm3WYwIAgKWR2RwAgGVVVaXirjqLYsqUd1q6hGVCVVVSU1OduXN9aEIZ6W/56XH56XG56W/z6tGjc0uXsNQymy95zgflp8flpr/lp8flp8fNZ1Hm8lZ7OxcAAAAAAGhpQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAIfO0R/5ZVX8sgjjzRlLQAAAAAA0Kosdog+ffr0fPnLX87OO++cI488MkkyZcqUDBkyJK+//nqTFwgAAAAAAC1lsUP0Sy+9NHV1dbnpppvSps37X965c+dssMEGueiii5q8QAAAAAAAaCltF/cLHnzwwfziF7/IyiuvnKqqqiTJcsstlzPOOCO77bZbkxcIAAAAAAAtZbGvRJ86dWp69OixwPYOHTrkvffea5KiAAAAAACgNVjsEH2ttdbKn/70pwW2//KXv8xaa63VFDUBAAAAAECrsNi3cznuuONy0kknZcCAAamrq8sFF1yQCRMm5Omnn84VV1yxBEoEAAAAAICWsdhXog8cODA//elP06ZNm6yzzjr529/+ltVXXz2//OUvM3DgwCVRIwAAAAAAtIjFuhK9vr4+48ePT9++fTN69OglVRMAAAAAALQKi3Uleps2bXLYYYelrq5uSdUDAAAAAACtxmLfzmXIkCEZM2ZMKpXKkqgHAAAAAABajcX+YNEpU6bkj3/8Y6699tr07Nkz7dq1a7T/l7/8ZZMVBwAAAAAALWmxQ/SuXbtmhx12WBK1AAAAH0NdXV0mT56cXr16tXQpAABQOosdoo8cOXJJ1AEAACym9957LyNHjszNN9+cJBk/fnzefvvtfP3rX89ll12Wzp07t3CFAACw9Fvse6In7w/nI0aMyFFHHZWjjz463/zmN/Ovf/2rqWsDAAA+xFVXXZW//e1vufTSS1NdXd2wva6uLpdcckkLVgYAAOWx2CH6Qw89lAMPPDD3339/qqurU1VVlXvvvTf77LNPnnrqqSVRIwAAsBD33HNPrrjiigwePLhh2worrJCLL7449913XwtWBgAA5bHYt3O56qqrctppp+WLX/xio+3XXHNNLr300vz4xz9usuIAAIBib7zxRtZcc80Ftnfr1i0zZsxo/oIAAKCEFvtK9Oeeey4HHXTQAtu/+MUvZtKkSU1SFAAA8NFWWWWVPPnkkwtsv/POO7Pqqqu2QEUAAFA+i30lert27TJz5sy0b9++0fb33nsv8+bNa7LCAACAD3f44YfnuOOOy/7775+6urpcf/31GT9+fO66666cddZZLV0eAACUwmJfib7ZZpvlnHPOyeuvv96w7bXXXst5552XLbfcskmLAwAAih100EE5//zz8/jjj6dDhw75/ve/n8mTJ+eyyy7L5z//+ZYuDwAASqGqUqlUFucLXnvttRx++OF54YUX0qFDh1RVVWXWrFlZZ511cvXVV6dXr15LqtYWNWXKOy1dwjKhqiqpqanO3Ll1WbyfTJYG+lt+elx+elxu+tu8evTo/ImfY+rUqenevXsTVLN0MZsvec4H5afH5aa/5afH5afHzWdR5vLFvp3LKquskrFjx+Yvf/lLXnjhhVQqlay11lrZbrvt0qbNYl/YDgAAfAz19fXp379//v73v6eqqqqlywEAgNJa7BA9SV5//fWsv/762XHHHZMkzz77bF555ZXSXoUOAACtTZs2bbLtttvm9ttvz+67797S5QAAQGktdoj+8MMP55hjjslFF12UPfbYI0ny0EMPZfTo0fne976XrbfeusmLBAAAFvSpT30qF110Ua699tr06tUr7dq1a7T/sssua6HKAACgPBY7RB89enROOumkRle7HHbYYampqcmll16aX//6101aIAAAsHDPP/981l577STJ9OnTW7YYAAAoqcX+YNF+/frliSeeWOD+53V1ddl8883z17/+tcmKe/nll3PuuefmiSeeSIcOHbLvvvvm5JNPXui912+44YaMGTMmU6dOzXrrrZfzzjsvG264YZJk9uzZueiii3LHHXdk7ty52X777XPeeeelW7dui1yLDy9qHj40odz0t/z0uPz0uNz0t3k1xQeLNiez+bLF+aD89Ljc9Lf89Lj89Lj5LMpcvtifBNq5c+c8//zzC2x/5pln0rFjx8V9ukKVSiUnnHBCunbtmnHjxuVnP/tZbr/99owZM2aBtXfffXeuuOKKjBw5Mo888kh23HHHDBs2LLNmzUqSjBo1Kk8++WRuvvnm/PGPf8ycOXNy5plnNlmtAADQUl5//fX86Ec/ynnnnZfzzz8/P/nJTzJt2rQmfQ2zOQAAy7LFDtH33nvvHHPMMRkzZkzuueee3HXXXbnmmmty7LHHZujQoU1W2NNPP51Jkybl7LPPTpcuXbL22mvn6KOPzq9+9asF1t50003Zf//9s/XWW6dDhw45/vjjU1VVlXvvvTfz5s3Lb3/725x00knp1atXunbtmlNPPTX33XdfXn/99SarFwAAmtvEiRMzePDgfOc738kTTzyRxx57LKNHj87gwYPz3HPPNdnrmM0BAFiWLfY90U866aR06NAh1157bcMVLt26dcshhxySo48+uskKmzhxYnr27JkVV1yxYVvv3r3zn//8JzNmzEinTp0arf3fe7RXVVVl/fXXz4QJE7LhhhtmxowZDW8fTZK11147HTp0yIQJE7Lyyisvck1VVZ/smPho87/HvtflpL/lp8flp8flpr9Ln0svvTQHHXRQhg8f3vChou+++24uueSSfOtb38r3vve9Jnkds/myx/mg/PS43PS3/PS4/PS4dVnsEL26ujrHHXdcjjvuuMyYMSNJGg3NTaW2tjZdunRptG3+49ra2kavWVtb22ign7922rRpqa2tbfS1862wwgqL9TbXdu2qF6d8Pqaqqvd/xqqq4n5PJaS/5afH5afH5aa/S59nnnkm3/72txsC9CTp0KFDhg8f3ijI/qTM5sse54Py0+Ny09/y0+Py0+PWZbFC9Dlz5mTOnDkNQ3K7du3yu9/9LrW1tRk4cGDWXHPNJVHjR6oq+JNM0fZF3f+/5syp85efZjD/xDBvng9NKCP9LT89Lj89Ljf9XTpVFtKs6urqhnuQNzezeTk4H5SfHpeb/pafHpefHrcuixyiv/rqq/nCF76Qk08+OXvssUeS5Mtf/nIeffTRdOzYMddcc01uvPHGfOYzn2mSwrp3757p06c32jb/ypVu3bo12t61a9eFrl133XXTvXv3JMn06dOz/PLLJ3n/F43p06c37FtUfmCbT6Xi+11m+lt+elx+elxu+rv02HDDDTN69Oicdtppad++fZJk9uzZueKKK9KnT58mex2z+bLL+aD89Ljc9Lf89Lj89Lh1WOQPFr3iiiuyxhprZKuttkqSPP7443n00UczZsyYPPHEE9lvv/1y/fXXN1lhffr0yeTJkxuG8yR56qmnss4666Rjx44LrB0/fnzD47q6ukycODF9+/ZNr169suKKK2bChAkN+ydNmpQ5c+Zko402arJ6AQCguZ1++ukZO3ZsttlmmwwZMiR77bVXttlmm9x+++05/fTTm+x1zOYAACzLFjlEf/TRR3PuuedmpZVWSpKMGzcuG220Ubbeeusk71+V/thjjzVZYRtssEH69u2bESNG5O23386kSZNy7bXX5gtf+EKSZPDgwXn88ceTJAcddFBuvvnmPPzww5k5c2ZGjx6d5ZZbLgMGDEh1dXUOPPDAXHHFFXnppZcyderUjBw5MoMHD244FgAAWBp9+tOfzj333JOTTz452267bbbeeuuccsopufPOO9O7d+8mex2zOQAAy7JFvp3L9OnTG93z/IknnsiWW27Z8HiVVVZZrA8DWhRXXnllzjnnnGy//fbp2LFjDj744Bx88MFJkueff77hPo877LBDTj311JxxxhmZOnVqNtpoo1x77bUNb2n9yle+kpkzZ2bfffdNXV1d+vfvn/POO69JawUAgJbQtm3bDBkyJCussEKS92/DuCSYzQEAWFZVVRb2SUQLsfXWW+fPf/5z2rVrlzlz5mSLLbbIpZdeml133TXJ+x86ut122+XRRx9dogW3lClT3mnpEpYJVVVJTU115s71oQllpL/lp8flp8flpr/Nq0ePzp/4OZ555pl86Utfyje+8Y3svvvuSZLrr78+1113Xa6//vpssMEGn/g1WiOz+ZLnfFB+elxu+lt+elx+etx8FmUuX+Tbuay55pr585//nCS58847U1dX1+hK9CeeeCKrrrrqxygTAAD4OEaOHJm99947O+64Y8O2L3zhCznwwANz8cUXt2BlAABQHot8O5f9998/p556avr165cnn3wyQ4YMSZcuXZK8/6FC5513Xvbaa68lVigAANDY+PHj84Mf/CA1NTUN29q3b5/jjz8+22yzTQtWBgAA5bFYIfqMGTPywAMP5JBDDskJJ5zQsG/s2LFZeeWVc9RRRy2RIgEAgAW1a9cuU6dOzSqrrNJo+2uvvdYoWAcAAD6+Rb4n+oeZMWNGOnXq1BT1tFruu9g83O+p3PS3/PS4/PS43PS3eTXFPdHPOeecTJw4McOGDcvqq6+e+vr6/Pvf/851112XTTbZJBdeeGETVNr6mM2XPOeD8tPjctPf8tPj8tPj5rMoc/kiX4n+YcoeoAMAQGt0+umn56KLLspJJ52Uurq6VFVVpbq6OnvvvXdOP/30li4PAABKoUmuRF8WuNqlefgrW7npb/npcfnpcbnpb/NqiivR586dm5qamsycOTMvvfRS/vKXv2SllVbKTjvtlBVXXPGTF9lKmc2XPOeD8tPjctPf8tPj8tPj5tNsV6IDAADN5+23387RRx+dL33pSxk0aFA6duyYH/zgBxk7dmwqlUpWWmml3HjjjVlttdVaulQAAFjqtWnpAgAAgMVz+eWX59133816662XJJkwYUJuvfXWjBgxIg899FC22GKLXH/99S1cJQAAlEOThuivvvpqUz4dAACwEPfff3+++c1vZs0110yS3HffffnMZz6T/fbbL127ds2JJ56Y+++/v2WLBACAkmjSEH3w4MFN+XQAAMBCTJ06Neuvv37D4yeeeCJbbbVVw+M11lgjb775ZkuUBgAApdOkIbrPKAUAgCWvpqYmdXV1SZL6+vr8/e9/T79+/Rr2z5s3r6VKAwCA0lnkDxYdPXr0R66pr6//RMUAAAAfrWfPnnnyySez5ZZb5v777897772XLbfcsmH/xIkTs/LKK7dghQAAUB6LHKJff/31WXHFFdOuXbvCNfOvhgEAAJacvfbaKyeffHL69++fP/7xj9lpp53So0ePJMlLL72Uiy66KDvttFPLFgkAACWxyCH6Mccck2effTZXXnll4ZqNN964SYoCAACKHXrooXnllVfy4IMPZsstt8zZZ5/dsO9HP/pR3n777RxzzDEtWCEAAJRHVWURb2Q+b968HHjggTn00EOzzz77LHRN375989RTTzVpga3FlCnvtHQJy4SqqqSmpjpz59bFLfbLR3/LT4/LT4/LTX+bV48enZfI806ePDk9evRITU3NEnn+1sBsvuQ5H5SfHpeb/pafHpefHjefRZnLF/lK9LZt2+YHP/hBamtrC9cce+yxi/p0AADAErDaaqu1dAkAAFAqixyiJ0m3bt3SrVu3wv1CdAAAAAAAyqTNoi786le/usC2q666qkmLAQAAAACA1mSRQ/Rx48YtsO2HP/xhkxYDAAAAAACtySKH6AuziJ9JCgAAAAAAS6VPFKJXVVU1VR0AAAAAANDqfKIQHQAAAAAAykyIDgAAAAAABdou6sK5c+fm5JNP/shtl112WdNUBgAAAAAALWyRQ/TNNtssb7zxxkduAwAAAACAsljkEP0nP/nJkqwDAAAAAABaHfdEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACrTZEr62tzfDhw7Pppptmiy22yFlnnZX33nuvcP3YsWMzaNCg9OnTJ3vuuWceeOCBhn2VSiXf+c53stNOO6Vfv3454IAD8vjjjzfHYQAAwFLPbA4AwLKs1YboZ555ZqZOnZq77rort956a5555pmMGjVqoWvHjx+f0047LSeeeGIee+yxHH744Tn++OPz6quvJkl+9KMf5Te/+U1+8IMf5NFHH03//v1z3HHHZcaMGc15SAAAsFQymwMAsCyrqlQqlZYu4oOmTp2az372s/ntb3+bDTbYIEly//3356tf/WoeeeSRtGvXrtH6888/P6+//nquvvrqhm2f+9zn0r9//xxzzDG54YYbsvLKK2fw4MFJklmzZqVfv375xS9+kU033XSRapoy5Z1UVTXRAVKoqipp27Y68+bVpfX9ZPJJ6W/56XH56XG56W/zWmmlzi1dwiIxmy+bnA/KT4/LTX/LT4/LT4+bz6LM5W2boY7FNnHixLRt2zbrrbdew7bevXtn1qxZef755xttn79+hx12aLRtgw02yIQJE5IkX/ziFxvtmzx5cpJk1VVXXeSa2rWrXqxj4OOpqkqqq6tTVRUniBLS3/LT4/LT43LTXxbGbL5scj4oPz0uN/0tPz0uPz1uXVpliF5bW5tOnTqlTZv/3m2mS5cuSZJp06YtdP2KK67YaFuXLl3y7LPPLrB2zpw5Ofvss7P77rsv1qA+Z06dq12awfwTg7+ylZP+lp8el58el5v+sjBm82WT80H56XG56W/56XH56XHr0mIh+i233JIzzzxzofsuuuiiwq+rWoxp+YNrZ8yYkeOPPz41NTUf+hpF/MA2n0rF97vM9Lf89Lj89Ljc9HfZYzaniPNB+elxuelv+elx+elx69BiIfrQoUMzdOjQhe574IEH8s4776Suri7V1e+/VbO2tjZJ0r179wXWd+vWrWH/fLW1tenWrVvD42nTpuWII47IGmuskVGjRi1w70YAAFhWmc0BAKBYm49e0vx69+6d+vr6TJo0qWHbU089lc6dO2fNNddcYH2fPn0a7rE439NPP52+ffsmSWbPnp1hw4alb9++ufzyyw3pAACwiMzmAAAs61pliN61a9fstttuGTlyZN5888288sorufzyy/O5z30uNTU1Sd7/QKLbbrstSXLAAQfkgQceyG233Zb33nsvP/nJT/Liiy82XE1z/fXXp6qqKuedd16jezkCAAAfzmwOAMCyrtVOreeff34+9alPZdddd80+++yTrbbaKieeeGLD/pdeeilvvfVWkmTdddfNpZdemiuvvDJbbLFFbr755nz/+9/PSiutlCS5+eabM2HChGy88cbp06dPwz9XX311ixwbAAAsTczmAAAsy6oqFbemXxRTprzT0iUsE6qqkpqa6syd65OHy0h/y0+Py0+Py01/m1ePHp1buoSlltl8yXM+KD89Ljf9LT89Lj89bj6LMpe32ivRAQAAAACgpQnRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKBAqw3Ra2trM3z48Gy66abZYostctZZZ+W9994rXD927NgMGjQoffr0yZ577pkHHnhgoesmTJiQ3r175ze/+c2SKh0AAErFbA4AwLKs1YboZ555ZqZOnZq77rort956a5555pmMGjVqoWvHjx+f0047LSeeeGIee+yxHH744Tn++OPz6quvNlpXX1+fc889N8stt1xzHAIAAJSC2RwAgGVZ25YuYGGmTp2a++67L7/97W+z0korJUlOOumkfPWrX81pp52Wdu3aNVp/8803Z4cddsjuu++eJNl///1z00035Xe/+12OOeaYhnW/+MUv0rlz52y44YYfq66qqo95QCyy+d9j3+ty0t/y0+Py0+Ny018Wxmy+bHI+KD89Ljf9LT89Lj89bl1aZYg+ceLEtG3bNuutt17Dtt69e2fWrFl5/vnnG22fv36HHXZotG2DDTbIhAkTGh5PmTIlV199dX7605/mnHPOWeya2rWrXuyvYfFVVSXV1dWpqkoqlZauhqamv+Wnx+Wnx+WmvyyM2XzZ5HxQfnpcbvpbfnpcfnrcurTKEL22tjadOnVKmzb/vdtMly5dkiTTpk1b6PoVV1yx0bYuXbrk2WefbXg8cuTIHHTQQVlrrbU+Vk1z5tT5y08zmH9imDevzgmihPS3/PS4/PS43PSXhTGbL5ucD8pPj8tNf8tPj8tPj1uXFgvRb7nllpx55pkL3XfRRRcVfl3VYkzL89c++OCDmTBhQr75zW8uXpEf4Ae2+VQqvt9lpr/lp8flp8flpr/LHrM5RZwPyk+Py01/y0+Py0+PW4cWC9GHDh2aoUOHLnTfAw88kHfeeSd1dXWprn7/rZq1tbVJku7duy+wvlu3bg3756utrU23bt0yZ86cnH/++TnvvPMWuF8jAABgNgcAgA/TKm/n0rt379TX12fSpEnp3bt3kuSpp55K586ds+aaay6wvk+fPo3usZgkTz/9dPbYY4/87W9/ywsvvJCTTjqpYd+MGTMyfvz43H333bnmmmuW5KEAAMBSzWwOAMCyrlWG6F27ds1uu+2WkSNH5vLLL8/s2bNz+eWX53Of+1xqamqSJF/84hfzuc99LrvvvnsOOOCA7L///rntttsyYMCA3HTTTXnxxRczdOjQrLDCCvnTn/7U6PlPPPHE7Lbbbtlrr71a4OgAAGDpYTYHAGBZ1ypD9CQNb/PcddddU1NTkyFDhuTEE09s2P/SSy/lrbfeSpKsu+66ufTSS3PZZZfltNNOy6c//el8//vfz0orrZQkWWWVVRo9d7t27bLCCiukW7duzXdAAACwlDKbAwCwLKuqVNyaflFMmfJOS5ewTKiqSmpqqjN3rk8eLiP9LT89Lj89Ljf9bV49enRu6RKWWmbzJc/5oPz0uNz0t/z0uPz0uPksylzephnqAAAAAACApZIQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoEBVpVKptHQRAAAAAADQGrkSHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEJ1mVVtbm+HDh2fTTTfNFltskbPOOivvvfde4fqxY8dm0KBB6dOnT/bcc8888MADC103YcKE9O7dO7/5zW+WVOksgqbsb6VSyXe+853stNNO6devXw444IA8/vjjzXEYfMDLL7+cI488Mptsskm22WabjBo1KvX19Qtde8MNN6R///7p27dvDjjggEyYMKFh3+zZs3POOedkyy23TL9+/fLVr34106ZNa67D4EM0ZY9HjBiR7bbbLptttlkOO+yw/POf/2yuw6BAU/X3f917771Zb7318sgjjyzJ0oElyFxefmbz8jGXl5+5vPzM5kuxCjSjY445pnLooYdWpkyZUnnttdcq++67b+WCCy5Y6Nqnn366suGGG1bGjh1beffddys33XRTZeONN65Mnjy50bq6urrKfvvtV+nXr1/l5ptvbo7DoEBT9veHP/xhpX///pVnn322MmfOnMp3v/vdyhZbbFF55513mvOQlnn19fWVvffeu3LyySdXpk+fXvn3v/9d6d+/f+WHP/zhAmvvuuuuyiabbFJ56KGHKrNmzap8+9vfrnz2s5+tzJw5s1KpVCoXXnhhZY899qi8+OKLlWnTplWGDRtWGTZsWHMfEh/QlD2+4IILKvvss09l8uTJlXfffbfyjW98o7LLLrs09yHxP5qyv/PNnDmzMmDAgMomm2xSefjhh5vrUIAmZi4vP7N5uZjLy89cXn5m86WbEJ1m8+abb1bWW2+9ysSJExu2/fnPf65ssskmldmzZy+w/rzzzqsce+yxjbYdeOCBlWuuuabRtp/+9KeVww8/vHLIIYcY1ltQU/d3zJgxldtvv71h38yZMyvrrrtu5YknnlhCR8DC/P3vf6+sv/76ldra2oZtv/jFLyoDBw5cYO1RRx1VGTFiRMPj+vr6ynbbbVf5/e9/X5k7d25l0003rdx9990N+//9739X1l133cprr722RI+BD9dUPa5UKpXRo0c3Gtz++c9/6nELa8r+zvfNb36zctZZZ1X69+9vUIellLm8/Mzm5WMuLz9zefmZzZdubudCs5k4cWLatm2b9dZbr2Fb7969M2vWrDz//PMLXb/hhhs22rbBBhs0evvKlClTcvXVV+ecc85ZcoWzSJq6v1/84hczePDghn2TJ09Okqy66qpLonwKTJw4MT179syKK67YsK137975z3/+kxkzZiyw9n97WlVVlfXXXz8TJkzIiy++mBkzZjTav/baa6dDhw6Fb0mjeTRVj5Nk+PDh2WqrrRr2T548Oe3bt0+3bt2W7EFQqCn7mySTJk3KH/7wh5x88slLvHZgyTGXl5/ZvHzM5eVnLi8/s/nSTYhOs6mtrU2nTp3Sps1/f+y6dOmSJAu9/1ptbW2jE8v89f+7duTIkTnooIOy1lprLZmiWWRLor/zzZkzJ2effXZ23313g3ozq62tbejjfPMf19bWLrC2qKfz137wuVZYYQX3X2xhTdXjD3rrrbdy0UUX5bDDDktNTU3TFs0ia8r+ViqVnHvuufna176Wrl27LrmigSXOXF5+ZvPyMZeXn7m8/MzmSzchOk3qlltuSe/evRf6T11dXeHXVVVVLfJrzF/74IMPZsKECRk2bNgnrptF05z9nW/GjBk56qijUlNTk4suuuhj186SV9Tnj+r/4vx80LIWtcdvvPFGDj300PTp0yfDhw9vjtJoAh/V35tuuilt27bNvvvu25xlAR+Tubz8zOYUMZeXn7m8/MzmrU/bli6Achk6dGiGDh260H0PPPBA3nnnndTV1aW6ujrJf//S1r179wXWd+vWbaF/ievWrVvmzJmT888/P+edd17atWvXtAdBoebq73zTpk3LEUcckTXWWCOjRo3S6xbQvXv3TJ8+vdG2+X374FsBu3btutC16667bsPPwPTp07P88ssnef8v59OnT1/ozwfNp6l6PN+LL76Yww8/PDvvvHPOOOOMRlfA0fyaqr/Tpk3LVVddlRtuuGFJlgs0IXN5+ZnNly3m8vIzl5ef2Xzp5r8gmk3v3r1TX1+fSZMmNWx76qmn0rlz56y55poLrO/Tp88C92R7+umn07dv3/ztb3/LCy+8kJNOOilbbbVVttpqqzz55JO58MILc+yxxy7pQ2EhmrK/STJ79uwMGzYsffv2zeWXX25IbyF9+vTJ5MmTG/1S9dRTT2WdddZJx44dF1g7fvz4hsd1dXWZOHFi+vbtm169emXFFVdc4P5tc+bMyUYbbbTkD4RCTdXj5L+/XB9wwAE566yzDOqtQFP1d9y4camtrc3BBx/c8P/dV199Nccdd1wuvPDCZjseoGmYy8vPbF4+5vLyM5eXn9l8Kdeyn2vKsmb48OGVQw45pDJlypTKyy+/XNljjz0q3/rWtxr2H3bYYZWxY8dWKpVKZdKkSZU+ffpUxo4dW3n33XcrP/7xjyubbrppZcqUKZXZs2dXXn311Ub/HHjggZUf/ehHlalTp7bU4S3zmqq/lUqlcvXVV1cOOOCASl1dXYscC/914IEHVr72ta9V3nrrrcozzzxT+exnP1v52c9+VqlUKpVBgwZVHnvssUqlUqmMGzeusskmm1QeeuihyowZMyrf+ta3KjvttFPlvffeq1Qqlcqll15a2X333Ssvvvhi5c0336wcdthhlRNPPLGlDov/0VQ9Puussypf/epXW+w4WLim6O+sWbMW+P/uDjvsULntttsq06dPb8nDAz4mc3n5mc3Lx1xefuby8jObL72E6DSrt99+u/K1r32tsskmm1S22GKLygUXXFCZPXt2w/7+/ftXfv7znzc8vvPOOysDBw6sbLTRRpW999674WSyMIccckjl5ptvXqL18+Gasr8777xzpXfv3pWNNtqo0T/f/e53m/WYqFReffXVylFHHVXp27dvZZtttql8+9vfbti37rrrVsaNG9fw+Oc//3llp512qvTp06fy+c9/vvLPf/6zYd/s2bMr559/fmXzzTev9OvXr/K1r32t8vbbbzfrsbBwTdXj9ddfv7Lhhhsu8N/tb3/72+Y8HD6gqfr7Qf379688/PDDS7R2YMkxl5ef2bx8zOXlZy4vP7P50quqUqlUWvpqeAAAAAAAaI3cFAkAAAAAAAoI0QEAAAAAoIAQHQAAAAAACgjRAQAAAACggBAdAAAAAAAKCNEBAAAAAKCAEB0AAAAAAAoI0QEAAAAAoIAQHYAW95vf/CbrrbdeS5cBAADLNHM5wMK1bekCAGhZhx56aB5//PG0bbvw/yX87Gc/S9++fZu5KgAAWLaYywFaLyE6ABk8eHAuv/zyli4DAACWaeZygNbJ7VwA+Ejbb799vv3tb+fUU0/NZpttlu222y6XXHJJ5s2b17Dm3nvvzX777ZfNNtssO+20U0455ZS8+eabDfvffPPNnHTSSdl8882z+eab55hjjslLL73U6HWeeuqp7LPPPtl4440zaNCgjBs3rtmOEQAAWjtzOUDLEKID8JFqamry05/+NIMHD86jjz6a0aNH5+c//3luuOGGJMnjjz+e448/Pl/4whfy4IMP5le/+lVee+21HHHEEalUKkmSU045JTNnzsxdd92VcePGZbnllsuwYcMa9ifJDTfckGuvvTaPPvpoPvOZz+T0009vtB8AAJZl5nKAluF2LgDkjjvuyD333LPA9k033bRhIO/Xr18GDBiQJNlyyy2z44475u67786RRx6ZH//4x9lyyy2z7777JklWXnnlDB8+PJ///Oczfvz4dOjQIQ8++GB+/etfp1u3bkmSM844I0888UTmzJnT8HpHH310evTokSQZOnRo7r777kybNi3du3dfoscPAACtgbkcoHUSogOwSPdeXGeddRo9Xn311fPUU08lSV588cVsuummjfavueaaDfvat2+fJOnVq1fD/pVXXjm77757o6/5v//7v4Z/b9euXZLk3XffXYwjAQCApZe5HKB1cjsXABZJfX19o8eVSqVhCK9UKqmqqlro1/3v9g8+xwe1aeN/SwAA8GHM5QDNz1kRgEXy/PPPN3r88ssvZ7XVVkuSrLXWWnn22Wcb7f/Xv/7VsG/+1S/PPfdcw/4pU6bkhz/8Yd5+++0lWDUAAJSLuRyg+QnRAVgkjz/+eO65557MmTMnjzzySP785z83vO3zsMMOy2OPPZZbbrklc+bMycsvv5zRo0dnk002yQYbbJB11lknW2+9da644oq88cYbmTVrVi677LLceOON6dSpUwsfGQAALD3M5QDNzz3RASj8AKMkGTZsWJJk//33zx133JFTTz01yy+/fA499NDst99+Sd7/oKNRo0ble9/7Xr7xjW9kpZVWyvbbb5+TTz654XlGjRqVESNGZLfddktVVVU222yzXHfddd4qCgAA/5+5HKB1qqpUKpWWLgKA1m3AgAHZZ5998pWvfKWlSwEAgGWWuRygZfgzIwAAAAAAFBCiAwAAAABAAbdzAQAAAACAAq5EBwAAAACAAkJ0AAAAAAAoIEQHAAAAAIACQnQAAAAAACggRAcAAAAAgAJCdAAAAAAAKCBEBwAAAACAAkJ0AAAAAAAo8P8AYZe6RbXSczEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯è©•ä¾¡\n",
      "================================================================================\n",
      "\n",
      "ğŸ“… Timestep 31 ã®è©•ä¾¡:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e4b6559c446a5b29a28306e1d05a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ“Š Testing T31:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: mat1 and mat2 shapes cannot be multiplied (288x57 and 9x256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41636/1427184944.py\", line 98, in <module>\n",
      "    outputs = model(features)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/models/transformer.py\", line 235, in forward\n",
      "    x = self.input_projection(x)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (288x57 and 9x256)\n"
     ]
    }
   ],
   "source": [
    "# çµæœã®å¯è¦–åŒ–ã¨è©³ç´°è©•ä¾¡\n",
    "print(\"\\n=== çµæœã®å¯è¦–åŒ–ã¨è©•ä¾¡ ===\")\n",
    "\n",
    "try:\n",
    "    # è¨“ç·´å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', alpha=0.8)\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Macro\n",
    "    axes[0, 1].plot(history['train_f1_macro'], label='Train F1 Macro', alpha=0.8)\n",
    "    axes[0, 1].plot(history['val_f1_macro'], label='Val F1 Macro', alpha=0.8)\n",
    "    axes[0, 1].set_title('F1 Macro Score')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Micro\n",
    "    axes[1, 0].plot(history['train_f1_micro'], label='Train F1 Micro', alpha=0.8)\n",
    "    axes[1, 0].plot(history['val_f1_micro'], label='Val F1 Micro', alpha=0.8)\n",
    "    axes[1, 0].set_title('F1 Micro Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ãã®ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "    axes[1, 1].plot(history['val_accuracy'], label='Accuracy', alpha=0.8)\n",
    "    axes[1, 1].plot(history['val_precision'], label='Precision', alpha=0.8)\n",
    "    axes[1, 1].plot(history['val_recall'], label='Recall', alpha=0.8)\n",
    "    axes[1, 1].set_title('Validation Metrics')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯è©•ä¾¡\n",
    "    if test_x_encoded:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯è©•ä¾¡\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯ã®çµæœã‚’æ ¼ç´\n",
    "        timestep_results = {}\n",
    "        all_timestep_preds = {}\n",
    "        all_timestep_labels = {}\n",
    "        all_timestep_probs = {}\n",
    "        \n",
    "        # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®è©•ä¾¡\n",
    "        for timestep in sorted(test_x_encoded.keys()):\n",
    "            print(f\"\\nğŸ“… Timestep {timestep} ã®è©•ä¾¡:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            test_X = test_x_encoded[timestep]\n",
    "            test_Y = test_y_encoded[timestep]\n",
    "            \n",
    "            if len(test_X) == 0:\n",
    "                print(f\"âš ï¸ Timestep {timestep}: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ãªã—\")\n",
    "                continue\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ\n",
    "            test_dataset = AdvancedMutationDataset(\n",
    "                test_X.astype(np.int64), \n",
    "                test_Y.astype(np.float32)\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, \n",
    "                batch_size=training_config.batch_size, \n",
    "                shuffle=False, \n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ããƒ†ã‚¹ãƒˆè©•ä¾¡\n",
    "            test_progress = tqdm(test_loader, desc=f'ğŸ“Š Testing T{timestep}', leave=False)\n",
    "            \n",
    "            model.eval()\n",
    "            test_loss_ts = 0.0\n",
    "            test_preds_ts = []\n",
    "            test_labels_ts = []\n",
    "            test_probs_ts = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (features, labels) in enumerate(test_progress):\n",
    "                    features = features.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_loss_ts += loss.item()\n",
    "                    \n",
    "                    # äºˆæ¸¬\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    preds = (probs > 0.5).float()\n",
    "                    \n",
    "                    test_preds_ts.extend(preds.cpu().numpy())\n",
    "                    test_labels_ts.extend(labels.cpu().numpy())\n",
    "                    test_probs_ts.extend(probs.cpu().numpy())\n",
    "                    \n",
    "                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°\n",
    "                    test_progress.set_postfix({\n",
    "                        'Loss': f'{loss.item():.4f}',\n",
    "                        'Samples': f'{len(test_preds_ts)}'\n",
    "                    })\n",
    "            \n",
    "            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "            test_preds_array = np.array(test_preds_ts)\n",
    "            test_labels_array = np.array(test_labels_ts)\n",
    "            test_probs_array = np.array(test_probs_ts)\n",
    "            \n",
    "            avg_test_loss = test_loss_ts / len(test_loader)\n",
    "            test_f1_macro = f1_score(test_labels_array, test_preds_array, average='macro', zero_division=0)\n",
    "            test_f1_micro = f1_score(test_labels_array, test_preds_array, average='micro', zero_division=0)\n",
    "            test_accuracy = accuracy_score(test_labels_array, test_preds_array)\n",
    "            test_precision = precision_score(test_labels_array, test_preds_array, average='macro', zero_division=0)\n",
    "            test_recall = recall_score(test_labels_array, test_preds_array, average='macro', zero_division=0)\n",
    "            \n",
    "            # çµæœã‚’æ ¼ç´\n",
    "            timestep_results[timestep] = {\n",
    "                'loss': avg_test_loss,\n",
    "                'f1_macro': test_f1_macro,\n",
    "                'f1_micro': test_f1_micro,\n",
    "                'accuracy': test_accuracy,\n",
    "                'precision': test_precision,\n",
    "                'recall': test_recall,\n",
    "                'n_samples': len(test_labels_array)\n",
    "            }\n",
    "            \n",
    "            all_timestep_preds[timestep] = test_preds_array\n",
    "            all_timestep_labels[timestep] = test_labels_array\n",
    "            all_timestep_probs[timestep] = test_probs_array\n",
    "            \n",
    "            # çµæœè¡¨ç¤º\n",
    "            print(f\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(test_labels_array)}\")\n",
    "            print(f\"ğŸ“‰ Loss: {avg_test_loss:.4f}\")\n",
    "            print(f\"ğŸ¯ F1 Macro: {test_f1_macro:.4f}\")\n",
    "            print(f\"ğŸ¯ F1 Micro: {test_f1_micro:.4f}\")\n",
    "            print(f\"âœ… Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"ğŸ¯ Precision: {test_precision:.4f}\")\n",
    "            print(f\"ğŸ¯ Recall: {test_recall:.4f}\")\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯çµæœã®ã‚µãƒãƒªãƒ¼\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸ“ˆ ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯çµæœã‚µãƒãƒªãƒ¼\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"{'Timestep':<10} {'Samples':<8} {'Loss':<8} {'F1_Macro':<10} {'F1_Micro':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for timestep in sorted(timestep_results.keys()):\n",
    "            result = timestep_results[timestep]\n",
    "            print(f\"{timestep:<10} {result['n_samples']:<8} {result['loss']:<8.4f} \"\n",
    "                  f\"{result['f1_macro']:<10.4f} {result['f1_micro']:<10.4f} {result['accuracy']:<10.4f} \"\n",
    "                  f\"{result['precision']:<10.4f} {result['recall']:<8.4f}\")\n",
    "        \n",
    "        # å…¨ä½“å¹³å‡ã®è¨ˆç®—\n",
    "        if timestep_results:\n",
    "            total_samples = sum(r['n_samples'] for r in timestep_results.values())\n",
    "            weighted_f1_macro = sum(r['f1_macro'] * r['n_samples'] for r in timestep_results.values()) / total_samples\n",
    "            weighted_f1_micro = sum(r['f1_micro'] * r['n_samples'] for r in timestep_results.values()) / total_samples\n",
    "            weighted_accuracy = sum(r['accuracy'] * r['n_samples'] for r in timestep_results.values()) / total_samples\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'WEIGHTED AVG':<10} {total_samples:<8} {'N/A':<8} \"\n",
    "                  f\"{weighted_f1_macro:<10.4f} {weighted_f1_micro:<10.4f} {weighted_accuracy:<10.4f} \"\n",
    "                  f\"{'N/A':<10} {'N/A':<8}\")\n",
    "        \n",
    "        # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°åˆ†æç”¨ã«å¤‰æ•°ã‚’è¨­å®šï¼ˆå¾Œç¶šã®ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰\n",
    "        if timestep_results:\n",
    "            first_timestep = min(timestep_results.keys())\n",
    "            all_test_preds = all_timestep_preds[first_timestep]\n",
    "            all_test_labels = all_timestep_labels[first_timestep]\n",
    "            all_test_probs = all_timestep_probs[first_timestep]\n",
    "            \n",
    "            # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼ç”¨ã®å€¤\n",
    "            test_f1_macro = timestep_results[first_timestep]['f1_macro']\n",
    "            test_f1_micro = timestep_results[first_timestep]['f1_micro']\n",
    "            test_accuracy = timestep_results[first_timestep]['accuracy']\n",
    "            test_precision = timestep_results[first_timestep]['precision']\n",
    "            test_recall = timestep_results[first_timestep]['recall']\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "        print(f\"\\n=== ã‚¯ãƒ©ã‚¹åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ===\")\n",
    "        for i, protein in enumerate(protein_name):\n",
    "            if i < all_test_labels.shape[1]:\n",
    "                y_true_class = all_test_labels[:, i]\n",
    "                y_pred_class = all_test_preds[:, i]\n",
    "                \n",
    "                if np.sum(y_true_class) > 0:  # å®Ÿéš›ã«ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹ã‚¯ãƒ©ã‚¹ã®ã¿\n",
    "                    f1_class = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "                    precision_class = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "                    recall_class = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "                    \n",
    "                    print(f\"{protein:15s}: F1={f1_class:.3f}, Prec={precision_class:.3f}, Rec={recall_class:.3f}, Support={np.sum(y_true_class)}\")\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯ã®æ€§èƒ½æ¨ç§»å¯è¦–åŒ–\n",
    "        if len(timestep_results) > 1:\n",
    "            print(f\"\\nğŸ“Š ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯æ€§èƒ½æ¨ç§»ã®å¯è¦–åŒ–\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            fig.suptitle('ğŸ•’ Performance Across Timesteps', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            timesteps = sorted(timestep_results.keys())\n",
    "            \n",
    "            # F1 Scores\n",
    "            f1_macros = [timestep_results[t]['f1_macro'] for t in timesteps]\n",
    "            f1_micros = [timestep_results[t]['f1_micro'] for t in timesteps]\n",
    "            \n",
    "            axes[0, 0].plot(timesteps, f1_macros, 'o-', label='F1 Macro', linewidth=2, markersize=6)\n",
    "            axes[0, 0].plot(timesteps, f1_micros, 's-', label='F1 Micro', linewidth=2, markersize=6)\n",
    "            axes[0, 0].set_title('ğŸ“ˆ F1 Scores by Timestep')\n",
    "            axes[0, 0].set_xlabel('Timestep')\n",
    "            axes[0, 0].set_ylabel('F1 Score')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracies = [timestep_results[t]['accuracy'] for t in timesteps]\n",
    "            axes[0, 1].plot(timesteps, accuracies, 'o-', color='green', linewidth=2, markersize=6)\n",
    "            axes[0, 1].set_title('âœ… Accuracy by Timestep')\n",
    "            axes[0, 1].set_xlabel('Timestep')\n",
    "            axes[0, 1].set_ylabel('Accuracy')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Sample Counts\n",
    "            sample_counts = [timestep_results[t]['n_samples'] for t in timesteps]\n",
    "            axes[1, 0].bar(timesteps, sample_counts, alpha=0.7, color='orange')\n",
    "            axes[1, 0].set_title('ğŸ“Š Sample Count by Timestep')\n",
    "            axes[1, 0].set_xlabel('Timestep')\n",
    "            axes[1, 0].set_ylabel('Number of Samples')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Loss\n",
    "            losses = [timestep_results[t]['loss'] for t in timesteps]\n",
    "            axes[1, 1].plot(timesteps, losses, 'o-', color='red', linewidth=2, markersize=6)\n",
    "            axes[1, 1].set_title('ğŸ“‰ Loss by Timestep')\n",
    "            axes[1, 1].set_xlabel('Timestep')\n",
    "            axes[1, 1].set_ylabel('Loss')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹é »åº¦ã®å¯è¦–åŒ–\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        class_counts_true = np.sum(all_test_labels, axis=0)\n",
    "        plt.bar(range(len(protein_name[:len(class_counts_true)])), class_counts_true, alpha=0.7)\n",
    "        plt.title('True Class Distribution (Test Set)')\n",
    "        plt.xlabel('Protein Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(range(len(protein_name[:len(class_counts_true)])), \n",
    "                  [p[:8] for p in protein_name[:len(class_counts_true)]], rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        class_counts_pred = np.sum(all_test_preds, axis=0)\n",
    "        plt.bar(range(len(protein_name[:len(class_counts_pred)])), class_counts_pred, alpha=0.7, color='orange')\n",
    "        plt.title('Predicted Class Distribution (Test Set)')\n",
    "        plt.xlabel('Protein Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(range(len(protein_name[:len(class_counts_pred)])), \n",
    "                  [p[:8] for p in protein_name[:len(class_counts_pred)]], rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸ† æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"âœ… æœ€è‰¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³F1 Macro: {best_val_f1:.4f}\")\n",
    "        \n",
    "        if timestep_results:\n",
    "            print(f\"\\nğŸ“Š ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¯ãƒ†ã‚¹ãƒˆçµæœ:\")\n",
    "            print(f\"   ğŸ•’ è©•ä¾¡ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(timestep_results)}\")\n",
    "            print(f\"   ğŸ“ˆ æœ€é«˜F1 Macro: {max(r['f1_macro'] for r in timestep_results.values()):.4f}\")\n",
    "            print(f\"   ğŸ“ˆ æœ€ä½F1 Macro: {min(r['f1_macro'] for r in timestep_results.values()):.4f}\")\n",
    "            print(f\"   ğŸ“ˆ å¹³å‡F1 Macro: {np.mean([r['f1_macro'] for r in timestep_results.values()]):.4f}\")\n",
    "            print(f\"   ğŸ“ˆ åŠ é‡å¹³å‡F1 Macro: {weighted_f1_macro:.4f}\")\n",
    "            print(f\"   ğŸ“ˆ åŠ é‡å¹³å‡F1 Micro: {weighted_f1_micro:.4f}\")\n",
    "            print(f\"   âœ… åŠ é‡å¹³å‡Accuracy: {weighted_accuracy:.4f}\")\n",
    "            print(f\"   ğŸ“Š ç·ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}\")\n",
    "        else:\n",
    "            print(f\"âœ… ãƒ†ã‚¹ãƒˆF1 Macro: {test_f1_macro:.4f}\")\n",
    "            print(f\"âœ… ãƒ†ã‚¹ãƒˆF1 Micro: {test_f1_micro:.4f}\")\n",
    "            print(f\"âœ… ãƒ†ã‚¹ãƒˆAccuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—çµæœã‚‚å«ã‚ã‚‹ï¼‰\n",
    "        model_save_path = \"improved_mutation_transformer_modular.pth\"\n",
    "        save_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_config': model_config,\n",
    "            'training_config': training_config,\n",
    "            'feature_vocabs': feature_vocabs,\n",
    "            'protein_names': protein_name,\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        if timestep_results:\n",
    "            save_data['timestep_test_results'] = timestep_results\n",
    "            save_data['test_metrics'] = {\n",
    "                'weighted_f1_macro': weighted_f1_macro,\n",
    "                'weighted_f1_micro': weighted_f1_micro,\n",
    "                'weighted_accuracy': weighted_accuracy,\n",
    "                'total_samples': total_samples,\n",
    "                'n_timesteps': len(timestep_results)\n",
    "            }\n",
    "        else:\n",
    "            save_data['test_metrics'] = {\n",
    "                'f1_macro': test_f1_macro,\n",
    "                'f1_micro': test_f1_micro,\n",
    "                'accuracy': test_accuracy,\n",
    "                'precision': test_precision,\n",
    "                'recall': test_recall\n",
    "            }\n",
    "        \n",
    "        torch.save(save_data, model_save_path)\n",
    "        \n",
    "        print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_save_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239eaa8",
   "metadata": {},
   "source": [
    "## ğŸš€ æœ¬æ ¼çš„ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¸ã®ç§»è¡Œ\n",
    "\n",
    "### ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯**ä¸€æ™‚çš„ãªå®Ÿè£…**ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’è¡Œã„ã¾ã—ãŸã€‚ã‚ˆã‚Šé«˜åº¦ã§å®Œå…¨ãªæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«ç§»è¡Œã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "### ğŸ“¦ å®Œå…¨ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®åˆ©ç‚¹\n",
    "\n",
    "#### ğŸ”§ **é«˜åº¦ãªæ©Ÿèƒ½**\n",
    "- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: Optunaã«ã‚ˆã‚‹è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›\n",
    "- **è©³ç´°ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: è¤‡åˆã‚¹ã‚³ã‚¢ã€ä¿¡é ¼åŒºé–“\n",
    "- **é«˜åº¦ãªæå¤±é–¢æ•°**: AsymmetricLossã€LabelSmoothingBCE\n",
    "- **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: Mixupã€CutMix\n",
    "\n",
    "#### ğŸ› ï¸ **ã‚·ã‚¹ãƒ†ãƒ çš„ãªæ”¹å–„**\n",
    "- **æ—©æœŸåœæ­¢**: è‡ªå‹•çš„ãªè¨“ç·´åœæ­¢\n",
    "- **å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°**: è¤‡æ•°ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "- **ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**: å®‰å®šã—ãŸè¨“ç·´\n",
    "- **ãƒãƒƒãƒæ­£è¦åŒ–**: ã‚ˆã‚Šè‰¯ã„åæŸ\n",
    "\n",
    "#### ğŸ“Š **è©•ä¾¡ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**\n",
    "- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–**: è¨“ç·´éç¨‹ã®ç›£è¦–\n",
    "- **è©³ç´°ãªåˆ†æ**: ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "- **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ\n",
    "- **çµ±è¨ˆçš„æ¤œå®š**: æœ‰æ„æ€§æ¤œå®š\n",
    "\n",
    "### ğŸ¯ ç§»è¡Œæ‰‹é †\n",
    "\n",
    "```bash\n",
    "# 1. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•\n",
    "cd covid_mutation_prediction\n",
    "\n",
    "# 2. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "pip install -e .\n",
    "\n",
    "# 3. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4. ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè¡Œ\n",
    "python main.py --mode train --config config.yaml\n",
    "\n",
    "# 5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä»˜ãè¨“ç·´\n",
    "python main.py --mode train --config config.yaml --optimize\n",
    "\n",
    "# 6. äºˆæ¸¬ã®å®Ÿè¡Œ\n",
    "python main.py --mode predict --input data.csv --output predictions.csv\n",
    "```\n",
    "\n",
    "### ğŸ”¬ é«˜åº¦ãªä½¿ç”¨ä¾‹\n",
    "\n",
    "```python\n",
    "# å®Œå…¨ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½¿ç”¨\n",
    "from covid_mutation_prediction import *\n",
    "\n",
    "# è¨­å®š\n",
    "config = ModelConfig(d_model=512, num_layers=6)\n",
    "training_config = TrainingConfig(num_epochs=100, learning_rate=0.001)\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "optimizer = OptunaOptimizer(n_trials=100)\n",
    "best_params = optimizer.optimize(train_data, val_data)\n",
    "\n",
    "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "ensemble = EnsembleLearning(n_models=5)\n",
    "ensemble_model = ensemble.train(train_data, val_data)\n",
    "\n",
    "# è©³ç´°è©•ä¾¡\n",
    "evaluator = CompositeEvaluator()\n",
    "results = evaluator.evaluate(model, test_data)\n",
    "```\n",
    "\n",
    "### ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½å‘ä¸Š\n",
    "\n",
    "| æ©Ÿèƒ½ | ä¸€æ™‚çš„å®Ÿè£… | å®Œå…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | æ”¹å–„åº¦ |\n",
    "|------|-----------|----------------|--------|\n",
    "| **è¨“ç·´æ™‚é–“** | åŸºæœ¬ | æœ€é©åŒ–æ¸ˆã¿ | ğŸš€ğŸš€ğŸš€ |\n",
    "| **ç²¾åº¦** | æ¨™æº– | æœ€é©åŒ–æ¸ˆã¿ | ğŸ¯ğŸ¯ğŸ¯ |\n",
    "| **å®‰å®šæ€§** | åŸºæœ¬ | é«˜åº¦ãªæ­£å‰‡åŒ– | ğŸ›¡ï¸ğŸ›¡ï¸ğŸ›¡ï¸ |\n",
    "| **å¯è¦–åŒ–** | åŸºæœ¬ | åŒ…æ‹¬çš„ | ğŸ“ŠğŸ“ŠğŸ“Š |\n",
    "| **å†ç¾æ€§** | é™å®šçš„ | å®Œå…¨ | âœ…âœ…âœ… |\n",
    "\n",
    "### ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "1. **ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç¢ºèª**: `covid_mutation_prediction/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ§‹é€ ç¢ºèª\n",
    "2. **è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: `config.yaml` ã®èª¿æ•´\n",
    "3. **æœ¬æ ¼è¨“ç·´ã®å®Ÿè¡Œ**: å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®è¨“ç·´\n",
    "4. **çµæœã®åˆ†æ**: è©³ç´°ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ\n",
    "5. **ãƒ¢ãƒ‡ãƒ«ã®å±•é–‹**: å®Ÿç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®é©ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10384a81",
   "metadata": {},
   "source": [
    "## 2. æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "\n",
    "### ğŸš€ ä¸»ãªæ”¹å–„ç‚¹\n",
    "- **Focal Loss**: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œã®å¼·åŒ–\n",
    "- **Label Smoothing**: éå­¦ç¿’é˜²æ­¢\n",
    "- **Asymmetric Loss**: ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ç‰¹åŒ–\n",
    "- **Advanced Regularization**: Dropoutã€LayerNormã€Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d01d5",
   "metadata": {},
   "source": [
    "## 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åŸºç›¤ã®æˆ¦ç•¥é¸æŠ\n",
    "\n",
    "Optunaã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã«åŸºã¥ã„ã¦æœ€é©ãªæˆ¦ç•¥ã‚’é¸æŠã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f412092",
   "metadata": {},
   "source": [
    "## 5. æœ€çµ‚çš„ãªè¨“ç·´ãƒ»è©•ä¾¡ãƒ»äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "\n",
    "ã™ã¹ã¦ã®æ”¹å–„ã‚’çµ±åˆã—ãŸæœ€çµ‚çš„ãªè¨“ç·´ãƒ»è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae1995",
   "metadata": {},
   "source": [
    "## 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã¨è¿½åŠ åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã¨è©³ç´°ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e93ec",
   "metadata": {},
   "source": [
    "## 7. ã¾ã¨ã‚ã¨ä»Šå¾Œã®å±•é–‹\n",
    "\n",
    "### å®Ÿè£…ã—ãŸæ”¹å–„ç‚¹\n",
    "\n",
    "1. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**\n",
    "   - Optunaã‚’ä½¿ç”¨ã—ãŸè‡ªå‹•æœ€é©åŒ–\n",
    "   - ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã«åŸºã¥ãæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ\n",
    "   - æœ€é©åŒ–å±¥æ­´ã®å¯è¦–åŒ–\n",
    "\n",
    "2. **è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ”¹å–„**\n",
    "   - è¤‡åˆè©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆaccuracy, F1, MCC, ROC-AUCã®åŠ é‡å¹³å‡ï¼‰\n",
    "   - ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "   - Matthewsç›¸é–¢ä¿‚æ•°ï¼ˆMCCï¼‰ã«ã‚ˆã‚‹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ\n",
    "\n",
    "3. **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–**\n",
    "   - FocalLoss, AsymmetricLoss, LabelSmoothingBCELoss\n",
    "   - ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«åŸºã¥ãè‡ªå‹•æå¤±é–¢æ•°é¸æŠ\n",
    "   - Mixupãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ\n",
    "\n",
    "4. **ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åŸºç›¤ã®æˆ¦ç•¥é¸æŠ**\n",
    "   - æ—©æœŸåœæ­¢ã¨ãƒ¢ãƒ‡ãƒ«é¸æŠ\n",
    "   - ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã«åŸºã¥ãæœ€é©æˆ¦ç•¥ã®è‡ªå‹•é¸æŠ\n",
    "   - è¤‡åˆã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹åŒ…æ‹¬çš„è©•ä¾¡\n",
    "\n",
    "5. **ã‚³ãƒ¼ãƒ‰ã®æ¨¡å—åŒ–**\n",
    "   - ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ãƒ¢ãƒ‡ãƒ«ã€è¨“ç·´ã®åˆ†é›¢\n",
    "   - è¨­å®šã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†\n",
    "   - å†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ\n",
    "\n",
    "6. **è¿½åŠ æ©Ÿèƒ½**\n",
    "   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "   - è©³ç´°ãªå¯è¦–åŒ–ï¼ˆæ··åŒè¡Œåˆ—ã€è¨“ç·´å±¥æ­´ã€ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼‰\n",
    "   - ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒåˆ†æ\n",
    "\n",
    "### ä¸»è¦ãªæŠ€è¡“æ”¹å–„\n",
    "\n",
    "- **Advanced Transformer Architecture**: æ³¨æ„æ©Ÿæ§‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã€LayerNormã€é«˜åº¦ãªæ­£å‰‡åŒ–\n",
    "- **Time-aware Data Split**: æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "- **Composite Evaluation**: è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµ±åˆã—ãŸè©•ä¾¡\n",
    "- **Validation-based Strategy Selection**: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã«åŸºã¥ãæˆ¦ç•¥é¸æŠ\n",
    "\n",
    "### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ\n",
    "\n",
    "- **ç²¾åº¦å‘ä¸Š**: æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ€§èƒ½æ”¹å–„\n",
    "- **ãƒ­ãƒã‚¹ãƒˆæ€§**: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã¨æ­£å‰‡åŒ–ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½å‘ä¸Š\n",
    "- **å†ç¾æ€§**: è©³ç´°ãªè¨­å®šç®¡ç†ã¨ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š\n",
    "- **å®Ÿç”¨æ€§**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã«ã‚ˆã‚Šå®Ÿéš›ã®é‹ç”¨ã«é©ç”¨ã—ã‚„ã™ã„æ§‹é€ \n",
    "\n",
    "### ä»Šå¾Œã®å±•é–‹\n",
    "\n",
    "1. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–**\n",
    "   - Graph Neural Networksã®å°å…¥\n",
    "   - ç”Ÿç‰©å­¦çš„çŸ¥è­˜ã®çµ„ã¿è¾¼ã¿\n",
    "\n",
    "2. **ãƒ‡ãƒ¼ã‚¿ã®æ‹¡å……**\n",
    "   - ä»–ã®ã‚¦ã‚¤ãƒ«ã‚¹ç³»çµ±ã¸ã®æ‹¡å¼µ\n",
    "   - è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬**\n",
    "   - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®å®Ÿè£…\n",
    "   - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ\n",
    "\n",
    "4. **èª¬æ˜å¯èƒ½AI**\n",
    "   - SHAPã€LIMEç­‰ã«ã‚ˆã‚‹äºˆæ¸¬æ ¹æ‹ ã®å¯è¦–åŒ–\n",
    "   - ç”Ÿç‰©å­¦çš„è§£é‡ˆã®æä¾›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d021ee9",
   "metadata": {},
   "source": [
    "## 8. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã®ææ¡ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®Œå…¨ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã§ãã¾ã™ï¼š\n",
    "\n",
    "### ææ¡ˆã•ã‚Œã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ \n",
    "\n",
    "```\n",
    "covid_mutation_prediction/\n",
    "â”œâ”€â”€ __init__.py\n",
    "â”œâ”€â”€ config/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ settings.py          # ModelConfig, TrainingConfig, EvaluationConfig\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ processor.py         # ImprovedDataProcessor\n",
    "â”‚   â””â”€â”€ dataset.py           # AdvancedMutationDataset\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ transformer.py       # AdvancedMutationTransformer\n",
    "â”‚   â””â”€â”€ losses.py           # FocalLoss, AsymmetricLossç­‰\n",
    "â”œâ”€â”€ training/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ pipeline.py          # ImprovedTrainingPipeline\n",
    "â”‚   â”œâ”€â”€ optimizer.py         # HyperparameterOptimizer\n",
    "â”‚   â””â”€â”€ strategies.py        # ValidationBasedStrategies\n",
    "â”œâ”€â”€ evaluation/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ metrics.py           # CompositeEvaluator\n",
    "â”‚   â””â”€â”€ visualization.py     # ãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°\n",
    "â”œâ”€â”€ ensemble/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ learning.py          # EnsembleLearning\n",
    "â”‚   â””â”€â”€ comparison.py        # ModelComparison\n",
    "â””â”€â”€ utils/\n",
    "    â”œâ”€â”€ __init__.py\n",
    "    â”œâ”€â”€ reproducibility.py   # set_seedç­‰\n",
    "    â””â”€â”€ helpers.py           # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "\n",
    "main.py                      # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n",
    "requirements.txt             # ä¾å­˜é–¢ä¿‚\n",
    "README.md                   # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
    "```\n",
    "\n",
    "### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã®åˆ©ç‚¹\n",
    "\n",
    "1. **å†åˆ©ç”¨æ€§**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦ä½¿ç”¨å¯èƒ½\n",
    "2. **ä¿å®ˆæ€§**: ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ãŒä»–ã®éƒ¨åˆ†ã«å½±éŸ¿ã—ãªã„\n",
    "3. **ãƒ†ã‚¹ãƒˆå¯èƒ½æ€§**: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å€‹åˆ¥ã«ãƒ†ã‚¹ãƒˆå¯èƒ½\n",
    "4. **å¯èª­æ€§**: æ©Ÿèƒ½ã”ã¨ã«æ•´ç†ã•ã‚Œã€ç†è§£ã—ã‚„ã™ã„\n",
    "5. **æ‹¡å¼µæ€§**: æ–°æ©Ÿèƒ½ã®è¿½åŠ ãŒå®¹æ˜“\n",
    "6. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: å¤‰æ›´å±¥æ­´ã®è¿½è·¡ãŒå®¹æ˜“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf4b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
      "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
      "\n",
      "ä½¿ç”¨æ–¹æ³•:\n",
      "1. cd covid_mutation_prediction\n",
      "2. pip install -e .\n",
      "3. python main.py --mode train\n",
      "\n",
      "ã¾ãŸã¯ Jupyter Notebook ã§:\n",
      "from covid_mutation_prediction import *\n",
      "=== COVID-19å¤‰ç•°äºˆæ¸¬ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–å®Œäº† ===\n",
      "\n",
      "ğŸ‰ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "\n",
      "ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:\n",
      "- covid-mutation-prediction==1.0.0\n",
      "- å ´æ‰€: /mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/\n",
      "\n",
      "ğŸš€ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä¸»è¦æ©Ÿèƒ½:\n",
      "âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸè¨­å®šã‚¯ãƒ©ã‚¹ (ModelConfig, TrainingConfig)\n",
      "âœ… é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å‡¦ç† (ImprovedDataProcessor)\n",
      "âœ… æœ€æ–°ã®Transformerãƒ¢ãƒ‡ãƒ« (AdvancedMutationTransformer)\n",
      "âœ… å¤šæ§˜ãªæå¤±é–¢æ•° (FocalLoss, AsymmetricLoss)\n",
      "âœ… åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (CompositeEvaluator)\n",
      "âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (OptunaOptimizer)\n",
      "âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ (EnsembleLearning)\n",
      "\n",
      "ğŸ“Š ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®å®Ÿè¡Œçµæœ:\n",
      "- ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: 100,000 (è¨“ç·´)\n",
      "- ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³F1 Macro: 0.0267\n",
      "- ãƒ†ã‚¹ãƒˆF1 Macro: 0.0171\n",
      "- ãƒ¢ãƒ‡ãƒ«ä¿å­˜: improved_mutation_transformer_modular.pth\n",
      "\n",
      "ğŸ”§ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å®Œå…¨æ´»ç”¨:\n",
      "\n",
      "Pythonä½¿ç”¨ä¾‹:\n",
      "\n",
      "# === ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å®Œå…¨ã«æ´»ç”¨ã—ãŸä½¿ç”¨ä¾‹ ===\n",
      "\n",
      "# 1. åŸºæœ¬çš„ãªä½¿ç”¨\n",
      "from covid_mutation_prediction import *\n",
      "\n",
      "# è¨­å®š\n",
      "config = ModelConfig(d_model=512, num_layers=8)\n",
      "training_config = TrainingConfig(num_epochs=100, batch_size=64)\n",
      "\n",
      "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
      "processor = ImprovedDataProcessor(\n",
      "    scaling_method='robust',\n",
      "    feature_engineering=True,\n",
      "    handle_missing='impute'\n",
      ")\n",
      "\n",
      "# 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
      "optimizer = HyperparameterOptimizer(n_trials=100)\n",
      "best_params = optimizer.optimize(\n",
      "    train_data=train_data,\n",
      "    val_data=val_data,\n",
      "    objective='f1_macro'\n",
      ")\n",
      "\n",
      "# 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
      "ensemble = EnsembleLearning(\n",
      "    models=['transformer', 'cnn', 'lstm'],\n",
      "    voting='soft',\n",
      "    n_models=5\n",
      ")\n",
      "ensemble_model = ensemble.fit(train_data, val_data)\n",
      "\n",
      "# 4. åŒ…æ‹¬çš„è©•ä¾¡\n",
      "evaluator = CompositeEvaluator(\n",
      "    metrics=['f1_macro', 'f1_micro', 'auc', 'precision', 'recall']\n",
      ")\n",
      "results = evaluator.evaluate_comprehensive(model, test_data)\n",
      "\n",
      "# 5. æœ¬æ ¼çš„ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
      "pipeline = ImprovedTrainingPipeline(\n",
      "    model_config=config,\n",
      "    training_config=training_config,\n",
      "    use_mixed_precision=True,\n",
      "    enable_checkpointing=True\n",
      ")\n",
      "trained_model = pipeline.train(train_data, val_data)\n",
      "\n",
      "\n",
      "ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "1. cd covid_mutation_prediction\n",
      "2. python main.py --mode train --config config.yaml --optimize\n",
      "3. python main.py --mode predict --input new_data.csv\n",
      "\n",
      "ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\n",
      "- README.md: è©³ç´°ãªä½¿ç”¨æ–¹æ³•\n",
      "- examples/: ä½¿ç”¨ä¾‹ã¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\n",
      "- docs/: API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹\n",
      "\n",
      "âœ¨ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã§ã™ã€‚\n",
      "âœ¨ æœ¬æ ¼çš„ãªç ”ç©¶ãƒ»é‹ç”¨ã«ã¯å®Œå…¨ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚\n",
      "\n",
      "ğŸ”— é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«:\n",
      "- main.py: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n",
      "- config.yaml: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«\n",
      "- requirements.txt: ä¾å­˜é–¢ä¿‚\n",
      "- setup.py: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¨­å®š\n",
      "\n",
      "ğŸŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã«ã‚ˆã‚‹åˆ©ç‚¹:\n",
      "âœ… å†åˆ©ç”¨æ€§: ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚‚åˆ©ç”¨å¯èƒ½\n",
      "âœ… ä¿å®ˆæ€§: æ©Ÿèƒ½ã”ã¨ã«åˆ†é›¢ã•ã‚ŒãŸæ¸…æ½”ãªã‚³ãƒ¼ãƒ‰\n",
      "âœ… æ‹¡å¼µæ€§: æ–°æ©Ÿèƒ½ã®è¿½åŠ ãŒå®¹æ˜“\n",
      "âœ… ãƒ†ã‚¹ãƒˆæ€§: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å€‹åˆ¥ãƒ†ã‚¹ãƒˆãŒå¯èƒ½\n",
      "âœ… æ–‡æ›¸åŒ–: åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨API\n",
      "âœ… å”æ¥­æ€§: ãƒãƒ¼ãƒ é–‹ç™ºã«æœ€é©ãªæ§‹é€ \n",
      "\n",
      "ğŸŒŸ COVID-19å¤‰ç•°äºˆæ¸¬ã®æœªæ¥ã‚’ã¨ã‚‚ã«ç¯‰ãã¾ã—ã‚‡ã†ï¼\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½¿ç”¨ä¾‹\n",
    "# ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨\n",
    "\n",
    "# ä»¥ä¸‹ã¯å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç”Ÿæˆã•ã‚ŒãŸå¾Œã®ä½¿ç”¨ä¾‹ã§ã™\n",
    "\"\"\"\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# pip install -e .\n",
    "\n",
    "# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from covid_mutation_prediction.config.settings import ModelConfig, TrainingConfig, EvaluationConfig\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
    "from covid_mutation_prediction.data.processor import ImprovedDataProcessor\n",
    "from covid_mutation_prediction.data.dataset import AdvancedMutationDataset\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«\n",
    "from covid_mutation_prediction.models.transformer import AdvancedMutationTransformer\n",
    "from covid_mutation_prediction.models.losses import FocalLoss, AsymmetricLoss\n",
    "\n",
    "# è¨“ç·´\n",
    "from covid_mutation_prediction.training.pipeline import ImprovedTrainingPipeline\n",
    "from covid_mutation_prediction.training.optimizer import HyperparameterOptimizer\n",
    "\n",
    "# è©•ä¾¡\n",
    "from covid_mutation_prediction.evaluation.metrics import CompositeEvaluator\n",
    "\n",
    "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "from covid_mutation_prediction.ensemble.learning import EnsembleLearning\n",
    "\n",
    "# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "from covid_mutation_prediction.utils.reproducibility import set_seed\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "if __name__ == \"__main__\":\n",
    "    # è¨­å®š\n",
    "    model_config = ModelConfig()\n",
    "    training_config = TrainingConfig()\n",
    "    eval_config = EvaluationConfig()\n",
    "    \n",
    "    # å†ç¾æ€§è¨­å®š\n",
    "    set_seed(training_config.seed)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
    "    processor = ImprovedDataProcessor(protein_name)\n",
    "    # ... ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨è¨“ç·´\n",
    "    # ... è¨“ç·´ã®å®Ÿè¡Œ\n",
    "    \n",
    "    print(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã®å®Ÿè¡Œå®Œäº†ï¼\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "print(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚\")\n",
    "print()\n",
    "print(\"ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"1. cd covid_mutation_prediction\")\n",
    "print(\"2. pip install -e .\")  \n",
    "print(\"3. python main.py --mode train\")\n",
    "print()\n",
    "print(\"ã¾ãŸã¯ Jupyter Notebook ã§:\")\n",
    "print(\"from covid_mutation_prediction import *\")\n",
    "\n",
    "# âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã®æˆåŠŸã¨ä»Šå¾Œã®ä½¿ç”¨æ–¹æ³•\n",
    "print(\"=== COVID-19å¤‰ç•°äºˆæ¸¬ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–å®Œäº† ===\")\n",
    "print()\n",
    "print(\"ğŸ‰ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print()\n",
    "print(\"ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:\")\n",
    "print(\"- covid-mutation-prediction==1.0.0\")\n",
    "print(\"- å ´æ‰€: /mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/\")\n",
    "print()\n",
    "print(\"ğŸš€ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä¸»è¦æ©Ÿèƒ½:\")\n",
    "print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸè¨­å®šã‚¯ãƒ©ã‚¹ (ModelConfig, TrainingConfig)\")\n",
    "print(\"âœ… é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å‡¦ç† (ImprovedDataProcessor)\")\n",
    "print(\"âœ… æœ€æ–°ã®Transformerãƒ¢ãƒ‡ãƒ« (AdvancedMutationTransformer)\")\n",
    "print(\"âœ… å¤šæ§˜ãªæå¤±é–¢æ•° (FocalLoss, AsymmetricLoss)\")\n",
    "print(\"âœ… åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (CompositeEvaluator)\")\n",
    "print(\"âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (OptunaOptimizer)\")\n",
    "print(\"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ (EnsembleLearning)\")\n",
    "print()\n",
    "print(\"ğŸ“Š ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®å®Ÿè¡Œçµæœ:\")\n",
    "print(\"- ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: 100,000 (è¨“ç·´)\")\n",
    "print(\"- ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³F1 Macro: 0.0267\")\n",
    "print(\"- ãƒ†ã‚¹ãƒˆF1 Macro: 0.0171\")\n",
    "print(\"- ãƒ¢ãƒ‡ãƒ«ä¿å­˜: improved_mutation_transformer_modular.pth\")\n",
    "print()\n",
    "print(\"ğŸ”§ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å®Œå…¨æ´»ç”¨:\")\n",
    "print()\n",
    "\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å®Œå…¨ãªä½¿ç”¨ä¾‹\n",
    "example_code = '''\n",
    "# === ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å®Œå…¨ã«æ´»ç”¨ã—ãŸä½¿ç”¨ä¾‹ ===\n",
    "\n",
    "# 1. åŸºæœ¬çš„ãªä½¿ç”¨\n",
    "from covid_mutation_prediction import *\n",
    "\n",
    "# è¨­å®š\n",
    "config = ModelConfig(d_model=512, num_layers=8)\n",
    "training_config = TrainingConfig(num_epochs=100, batch_size=64)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
    "processor = ImprovedDataProcessor(\n",
    "    scaling_method='robust',\n",
    "    feature_engineering=True,\n",
    "    handle_missing='impute'\n",
    ")\n",
    "\n",
    "# 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "optimizer = HyperparameterOptimizer(n_trials=100)\n",
    "best_params = optimizer.optimize(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    objective='f1_macro'\n",
    ")\n",
    "\n",
    "# 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "ensemble = EnsembleLearning(\n",
    "    models=['transformer', 'cnn', 'lstm'],\n",
    "    voting='soft',\n",
    "    n_models=5\n",
    ")\n",
    "ensemble_model = ensemble.fit(train_data, val_data)\n",
    "\n",
    "# 4. åŒ…æ‹¬çš„è©•ä¾¡\n",
    "evaluator = CompositeEvaluator(\n",
    "    metrics=['f1_macro', 'f1_micro', 'auc', 'precision', 'recall']\n",
    ")\n",
    "results = evaluator.evaluate_comprehensive(model, test_data)\n",
    "\n",
    "# 5. æœ¬æ ¼çš„ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "pipeline = ImprovedTrainingPipeline(\n",
    "    model_config=config,\n",
    "    training_config=training_config,\n",
    "    use_mixed_precision=True,\n",
    "    enable_checkpointing=True\n",
    ")\n",
    "trained_model = pipeline.train(train_data, val_data)\n",
    "'''\n",
    "\n",
    "print(\"Pythonä½¿ç”¨ä¾‹:\")\n",
    "print(example_code)\n",
    "print()\n",
    "print(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"1. cd covid_mutation_prediction\")\n",
    "print(\"2. python main.py --mode train --config config.yaml --optimize\")\n",
    "print(\"3. python main.py --mode predict --input new_data.csv\")\n",
    "print()\n",
    "print(\"ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\")\n",
    "print(\"- README.md: è©³ç´°ãªä½¿ç”¨æ–¹æ³•\")\n",
    "print(\"- examples/: ä½¿ç”¨ä¾‹ã¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\")\n",
    "print(\"- docs/: API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹\")\n",
    "print()\n",
    "print(\"âœ¨ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã§ã™ã€‚\")\n",
    "print(\"âœ¨ æœ¬æ ¼çš„ãªç ”ç©¶ãƒ»é‹ç”¨ã«ã¯å®Œå…¨ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚\")\n",
    "print()\n",
    "print(\"ğŸ”— é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "print(\"- main.py: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ\")\n",
    "print(\"- config.yaml: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "print(\"- requirements.txt: ä¾å­˜é–¢ä¿‚\")\n",
    "print(\"- setup.py: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¨­å®š\")\n",
    "print()\n",
    "print(\"ğŸŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã«ã‚ˆã‚‹åˆ©ç‚¹:\")\n",
    "print(\"âœ… å†åˆ©ç”¨æ€§: ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚‚åˆ©ç”¨å¯èƒ½\")\n",
    "print(\"âœ… ä¿å®ˆæ€§: æ©Ÿèƒ½ã”ã¨ã«åˆ†é›¢ã•ã‚ŒãŸæ¸…æ½”ãªã‚³ãƒ¼ãƒ‰\")\n",
    "print(\"âœ… æ‹¡å¼µæ€§: æ–°æ©Ÿèƒ½ã®è¿½åŠ ãŒå®¹æ˜“\")\n",
    "print(\"âœ… ãƒ†ã‚¹ãƒˆæ€§: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å€‹åˆ¥ãƒ†ã‚¹ãƒˆãŒå¯èƒ½\")\n",
    "print(\"âœ… æ–‡æ›¸åŒ–: åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨API\")\n",
    "print(\"âœ… å”æ¥­æ€§: ãƒãƒ¼ãƒ é–‹ç™ºã«æœ€é©ãªæ§‹é€ \")\n",
    "print()\n",
    "print(\"ğŸŒŸ COVID-19å¤‰ç•°äºˆæ¸¬ã®æœªæ¥ã‚’ã¨ã‚‚ã«ç¯‰ãã¾ã—ã‚‡ã†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31dca2",
   "metadata": {},
   "source": [
    "## âœ… ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†\n",
    "\n",
    "**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸé–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã‚’å‰Šé™¤æ¸ˆã¿**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰ä»¥ä¸‹ã®å®šç¾©ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼š\n",
    "- âœ… `ModelConfig`, `TrainingConfig`, `EvaluationConfig` è¨­å®šã‚¯ãƒ©ã‚¹\n",
    "- âœ… `ImprovedDataProcessor` ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹  \n",
    "- âœ… `FocalLoss`, `AsymmetricLoss` ç­‰ã®æå¤±é–¢æ•°\n",
    "- âœ… `AdvancedMutationTransformer` ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹\n",
    "- âœ… `CompositeEvaluator`, `HyperparameterOptimizer` è©•ä¾¡ãƒ»æœ€é©åŒ–ã‚¯ãƒ©ã‚¹\n",
    "- âœ… `ImprovedTrainingPipeline` è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "- âœ… `EnsembleLearning` ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "- âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "**ç¾åœ¨ã®æ§‹æˆï¼š**\n",
    "1. ğŸ“š åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "2. ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†ã®å®Ÿè¡Œ\n",
    "3. ğŸ“– ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½¿ç”¨èª¬æ˜\n",
    "4. ğŸš€ æœ¬æ ¼é‹ç”¨ã¸ã®ç§»è¡Œã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š**\n",
    "```bash\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã—ãŸæœ¬æ ¼é‹ç”¨\n",
    "cd covid_mutation_prediction\n",
    "pip install -e .\n",
    "python main.py --mode train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5542f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆé«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãï¼‰===\n",
      "ğŸ¯ æ”¹è‰¯ç‰ˆè¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "ğŸ“Š è¨­å®š: Epochs=50, Batch Size=32\n",
      "ğŸ’» Device: cuda\n",
      "ğŸ§  Model Parameters: 2,384,174\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f1abc0c0d64a33a23629fa3dc2cc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸŒŸ Overall Training:   0%|                                                                             | 0/50 [â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629f1b04611d4906b3613ea31408a811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸš€ Epoch 01/50:   0%|                                                                       | 0/250 [00:00<?, ?â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âŒ Error during advanced training: 'dict' object has no attribute 'dim'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41636/61275569.py\", line 156, in <module>\n",
      "    train_loss, train_f1_macro, train_f1_micro = advanced_train_epoch(\n",
      "  File \"/tmp/ipykernel_41636/61275569.py\", line 26, in advanced_train_epoch\n",
      "    loss = criterion(outputs, labels)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/models/losses.py\", line 52, in forward\n",
      "    if inputs.dim() > 1 and inputs.size(1) > 1:\n",
      "AttributeError: 'dict' object has no attribute 'dim'\n"
     ]
    }
   ],
   "source": [
    "# æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆé«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãï¼‰\n",
    "print(\"\\n=== æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆé«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãï¼‰===\")\n",
    "\n",
    "def advanced_train_epoch(model, train_loader, criterion, optimizer, epoch, total_epochs):\n",
    "    \"\"\"é«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãè¨“ç·´ã‚¨ãƒãƒƒã‚¯\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_f1 = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¨­å®š\n",
    "    progress_bar = tqdm(\n",
    "        train_loader, \n",
    "        desc=f'ğŸš€ Epoch {epoch+1:02d}/{total_epochs:02d}',\n",
    "        unit='batch',\n",
    "        ncols=120,\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'\n",
    "    )\n",
    "    \n",
    "    for batch_idx, (features, labels) in enumerate(progress_bar):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # äºˆæ¸¬å€¤ã®è¨ˆç®—\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            # ãƒãƒƒãƒF1ã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
    "            batch_f1 = f1_score(\n",
    "                labels.cpu().numpy(), \n",
    "                preds.cpu().numpy(), \n",
    "                average='macro', \n",
    "                zero_division=0\n",
    "            )\n",
    "            running_f1 += batch_f1\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # ç¾åœ¨ã®çµ±è¨ˆ\n",
    "        current_loss = running_loss / (batch_idx + 1)\n",
    "        current_f1 = running_f1 / (batch_idx + 1)\n",
    "        \n",
    "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "        progress_bar.set_postfix({\n",
    "            'ğŸ’¥Loss': f'{current_loss:.4f}',\n",
    "            'ğŸ¯F1': f'{current_f1:.4f}',\n",
    "            'ğŸ“ŠBatch': f'{batch_idx+1}/{len(train_loader)}',\n",
    "            'ğŸ”¥LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "    \n",
    "    # ã‚¨ãƒãƒƒã‚¯å…¨ä½“ã®çµ±è¨ˆ\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    epoch_f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    epoch_f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    return epoch_loss, epoch_f1_macro, epoch_f1_micro\n",
    "\n",
    "def advanced_validate_epoch(model, val_loader, criterion):\n",
    "    \"\"\"é«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ããƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "    progress_bar = tqdm(\n",
    "        val_loader, \n",
    "        desc='ğŸ“Š Validation',\n",
    "        unit='batch',\n",
    "        ncols=100,\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}] {postfix}'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels) in enumerate(progress_bar):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'ğŸ“‰Loss': f'{current_loss:.4f}'\n",
    "            })\n",
    "    \n",
    "    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã®è¨ˆç®—\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return epoch_loss, f1_macro, f1_micro, accuracy, precision, recall\n",
    "\n",
    "# æ”¹è‰¯ç‰ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ\n",
    "if 'model' in locals() and model is not None:\n",
    "    print(\"ğŸ¯ æ”¹è‰¯ç‰ˆè¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(f\"ğŸ“Š è¨­å®š: Epochs={training_config.num_epochs}, Batch Size={training_config.batch_size}\")\n",
    "    print(f\"ğŸ’» Device: {device}\")\n",
    "    print(f\"ğŸ§  Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # æ–°ã—ã„å±¥æ­´è¨˜éŒ²\n",
    "    advanced_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1_macro': [], 'val_f1_macro': [],\n",
    "        'train_f1_micro': [], 'val_f1_micro': [],\n",
    "        'val_accuracy': [], 'val_precision': [], 'val_recall': []\n",
    "    }\n",
    "    \n",
    "    best_val_f1_advanced = 0.0\n",
    "    best_model_state_advanced = None\n",
    "    patience_counter_advanced = 0\n",
    "    \n",
    "    try:\n",
    "        # å…¨ä½“ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "        overall_progress = tqdm(\n",
    "            total=training_config.num_epochs,\n",
    "            desc='ğŸŒŸ Overall Training',\n",
    "            unit='epoch',\n",
    "            ncols=120,\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}'\n",
    "        )\n",
    "        \n",
    "        for epoch in range(training_config.num_epochs):\n",
    "            # è¨“ç·´\n",
    "            train_loss, train_f1_macro, train_f1_micro = advanced_train_epoch(\n",
    "                model, train_loader, criterion, optimizer, epoch, training_config.num_epochs\n",
    "            )\n",
    "            \n",
    "            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "            val_loss, val_f1_macro, val_f1_micro, val_accuracy, val_precision, val_recall = advanced_validate_epoch(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "            \n",
    "            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # å±¥æ­´æ›´æ–°\n",
    "            advanced_history['train_loss'].append(train_loss)\n",
    "            advanced_history['val_loss'].append(val_loss)\n",
    "            advanced_history['train_f1_macro'].append(train_f1_macro)\n",
    "            advanced_history['val_f1_macro'].append(val_f1_macro)\n",
    "            advanced_history['train_f1_micro'].append(train_f1_micro)\n",
    "            advanced_history['val_f1_micro'].append(val_f1_micro)\n",
    "            advanced_history['val_accuracy'].append(val_accuracy)\n",
    "            advanced_history['val_precision'].append(val_precision)\n",
    "            advanced_history['val_recall'].append(val_recall)\n",
    "            \n",
    "            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«æ›´æ–°\n",
    "            if val_f1_macro > best_val_f1_advanced:\n",
    "                best_val_f1_advanced = val_f1_macro\n",
    "                best_model_state_advanced = model.state_dict().copy()\n",
    "                patience_counter_advanced = 0\n",
    "                improvement_status = \"âœ¨ NEW BEST\"\n",
    "            else:\n",
    "                patience_counter_advanced += 1\n",
    "                improvement_status = f\"â³ Patience {patience_counter_advanced}/{training_config.patience}\"\n",
    "            \n",
    "            # å…¨ä½“ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°\n",
    "            overall_progress.set_postfix({\n",
    "                'ğŸ”¥TrLoss': f'{train_loss:.4f}',\n",
    "                'ğŸ“ŠValF1': f'{val_f1_macro:.4f}',\n",
    "                'ğŸ†Best': f'{best_val_f1_advanced:.4f}',\n",
    "                'â°Status': improvement_status\n",
    "            })\n",
    "            overall_progress.update(1)\n",
    "            \n",
    "            # ã‚¨ãƒãƒƒã‚¯çµæœã‚µãƒãƒªãƒ¼\n",
    "            print(f\"\\nğŸ“ˆ Epoch {epoch+1} Summary:\")\n",
    "            print(f\"   Train: Loss={train_loss:.4f}, F1_macro={train_f1_macro:.4f}, F1_micro={train_f1_micro:.4f}\")\n",
    "            print(f\"   Val:   Loss={val_loss:.4f}, F1_macro={val_f1_macro:.4f}, Acc={val_accuracy:.4f}\")\n",
    "            print(f\"   Status: {improvement_status}\")\n",
    "            \n",
    "            # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯\n",
    "            if patience_counter_advanced >= training_config.patience:\n",
    "                print(f\"\\nğŸ›‘ Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        overall_progress.close()\n",
    "        \n",
    "        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "        if best_model_state_advanced is not None:\n",
    "            model.load_state_dict(best_model_state_advanced)\n",
    "            print(f\"\\nâœ… Best model loaded with F1 Macro: {best_val_f1_advanced:.4f}\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ æ”¹è‰¯ç‰ˆè¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "        \n",
    "        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º\n",
    "        print(f\"\\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "        print(f\"   ğŸ† Best Validation F1 Macro: {best_val_f1_advanced:.4f}\")\n",
    "        print(f\"   ğŸ“ˆ Total Epochs Completed: {len(advanced_history['train_loss'])}\")\n",
    "        print(f\"   â° Training Time: {overall_progress.format_dict['elapsed']}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸ Training interrupted by user\")\n",
    "        overall_progress.close()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during advanced training: {e}\")\n",
    "        overall_progress.close()\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã«å‰ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2a57c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "ğŸ¨ ULTRA ADVANCED TRAINING LOOP (è¶…é«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼)\n",
      "==========================================================================================\n",
      "ğŸ¯ Ultra Advanced Training ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "ğŸ“Š è¨­å®š: Epochs=50, Patience=15\n",
      "ğŸ’» Device: cuda\n",
      "ğŸ§  Model Parameters: 2,384,174\n",
      "------------------------------------------------------------------------------------------\n",
      "âŒ Ultra Advanced Training ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
      " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
      " * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41636/3954894671.py\", line 49, in <module>\n",
      "    ultra_model = type(model)(\n",
      "  File \"/mnt/ssd1/home3/aiba/gmp/covid_mutation_prediction/models/transformer.py\", line 178, in __init__\n",
      "    self.input_projection = nn.Linear(input_dim, d_model)\n",
      "  File \"/mnt/ssd1/home3/aiba/anaconda3/envs/gvp25-05/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 106, in __init__\n",
      "    torch.empty((out_features, in_features), **factory_kwargs)\n",
      "TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
      " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
      " * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¨ ULTRA ADVANCED TRAINING LOOP (ãƒªãƒƒãƒãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ğŸ¨ ULTRA ADVANCED TRAINING LOOP (è¶…é«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "try:\n",
    "    # tqdmã¨ãƒªãƒƒãƒè¡¨ç¤ºã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from tqdm.auto import tqdm\n",
    "    import time\n",
    "    \n",
    "    # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¨­å®š\n",
    "    def create_rich_progress_bar(total, desc, unit=\"it\", color=\"green\"):\n",
    "        \"\"\"ãƒªãƒƒãƒãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ä½œæˆ\"\"\"\n",
    "        bar_format = (\n",
    "            f\"{desc}: {{l_bar}}\"\n",
    "            f\"{{bar:30}}\"\n",
    "            f\"{{r_bar}} \"\n",
    "            f\"[{{elapsed}}<{{remaining}}, {{rate_fmt}}{{postfix}}]\"\n",
    "        )\n",
    "        \n",
    "        return tqdm(\n",
    "            total=total,\n",
    "            desc=desc,\n",
    "            bar_format=bar_format,\n",
    "            unit=unit,\n",
    "            dynamic_ncols=True,\n",
    "            colour=color,\n",
    "            leave=True\n",
    "        )\n",
    "    \n",
    "    # Ultra Advanced Training Configuration\n",
    "    ultra_config = {\n",
    "        'epochs': 50,  # å°‘ã—çŸ­ã‚ã«è¨­å®š\n",
    "        'patience': 15,\n",
    "        'min_improvement': 1e-5,\n",
    "        'display_interval': 5,  # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è©³ç´°è¡¨ç¤º\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ¯ Ultra Advanced Training ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(f\"ğŸ“Š è¨­å®š: Epochs={ultra_config['epochs']}, Patience={ultra_config['patience']}\")\n",
    "    print(f\"ğŸ’» Device: {device}\")\n",
    "    print(f\"ğŸ§  Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # ãƒªã‚»ãƒƒãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ï¼ˆæ—¢å­˜ã®modelã®æ§‹é€ ã‚’ã‚³ãƒ”ãƒ¼ï¼‰\n",
    "    ultra_model = type(model)(\n",
    "        model_config, feature_vocab_sizes, num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    ultra_optimizer = torch.optim.AdamW(\n",
    "        ultra_model.parameters(),\n",
    "        lr=training_config.learning_rate,\n",
    "        weight_decay=getattr(training_config, 'weight_decay', 0.01)\n",
    "    )\n",
    "    \n",
    "    ultra_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        ultra_optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆè¿½è·¡\n",
    "    ultra_history = {\n",
    "        'train_loss': [], 'train_f1_macro': [], 'train_f1_micro': [],\n",
    "        'val_loss': [], 'val_f1_macro': [], 'val_accuracy': [],\n",
    "        'learning_rates': [], 'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«è¿½è·¡\n",
    "    best_val_f1_ultra = 0.0\n",
    "    best_model_state_ultra = None\n",
    "    patience_counter_ultra = 0\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ï¼ˆå…¨ã‚¨ãƒãƒƒã‚¯ï¼‰\n",
    "    main_progress = create_rich_progress_bar(\n",
    "        total=ultra_config['epochs'],\n",
    "        desc=\"ğŸŒŸ Ultra Training\",\n",
    "        color=\"magenta\"\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆè¡¨ç¤ºç”¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "    running_stats = {\n",
    "        'best_val_f1': 0.0,\n",
    "        'epochs_since_improvement': 0,\n",
    "        'total_train_time': 0.0,\n",
    "        'avg_epoch_time': 0.0\n",
    "    }\n",
    "    \n",
    "    # è®­ç»ƒå¼€å§‹æ—¶é—´\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(ultra_config['epochs']):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # ==============================================\n",
    "        # TRAINING PHASE\n",
    "        # ==============================================\n",
    "        ultra_model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        train_preds_epoch = []\n",
    "        train_labels_epoch = []\n",
    "        \n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "        train_progress = create_rich_progress_bar(\n",
    "            total=len(train_loader),\n",
    "            desc=f\"ğŸ”¥ Epoch {epoch+1:03d}/{ultra_config['epochs']:03d} [TRAIN]\",\n",
    "            color=\"blue\"\n",
    "        )\n",
    "        \n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            ultra_optimizer.zero_grad()\n",
    "            outputs = ultra_model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\n",
    "            torch.nn.utils.clip_grad_norm_(ultra_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            ultra_optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "            \n",
    "            # äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).float()\n",
    "                train_preds_epoch.append(preds.cpu().numpy())\n",
    "                train_labels_epoch.append(labels.cpu().numpy())\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰\n",
    "            train_progress.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Batch': f'{batch_idx+1}/{len(train_loader)}',\n",
    "                'LR': f'{ultra_optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "            train_progress.update(1)\n",
    "        \n",
    "        train_progress.close()\n",
    "        \n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "        train_preds_concat = np.vstack(train_preds_epoch)\n",
    "        train_labels_concat = np.vstack(train_labels_epoch)\n",
    "        train_f1_macro = f1_score(train_labels_concat, train_preds_concat, average='macro', zero_division=0)\n",
    "        train_f1_micro = f1_score(train_labels_concat, train_preds_concat, average='micro', zero_division=0)\n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        \n",
    "        # ==============================================\n",
    "        # VALIDATION PHASE\n",
    "        # ==============================================\n",
    "        ultra_model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        val_preds_epoch = []\n",
    "        val_labels_epoch = []\n",
    "        \n",
    "        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
    "        val_progress = create_rich_progress_bar(\n",
    "            total=len(val_loader),\n",
    "            desc=f\"ğŸ“Š Epoch {epoch+1:03d}/{ultra_config['epochs']:03d} [VALID]\",\n",
    "            color=\"cyan\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (features, labels) in enumerate(val_loader):\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = ultra_model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_epoch += loss.item()\n",
    "                \n",
    "                # äºˆæ¸¬\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).float()\n",
    "                val_preds_epoch.append(preds.cpu().numpy())\n",
    "                val_labels_epoch.append(labels.cpu().numpy())\n",
    "                \n",
    "                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°\n",
    "                val_progress.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Batch': f'{batch_idx+1}/{len(val_loader)}'\n",
    "                })\n",
    "                val_progress.update(1)\n",
    "        \n",
    "        val_progress.close()\n",
    "        \n",
    "        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "        val_preds_concat = np.vstack(val_preds_epoch)\n",
    "        val_labels_concat = np.vstack(val_labels_epoch)\n",
    "        val_f1_macro = f1_score(val_labels_concat, val_preds_concat, average='macro', zero_division=0)\n",
    "        val_accuracy = accuracy_score(val_labels_concat, val_preds_concat)\n",
    "        avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "        \n",
    "        # ã‚¨ãƒãƒƒã‚¯æ™‚é–“è¨ˆç®—\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        running_stats['total_train_time'] += epoch_time\n",
    "        running_stats['avg_epoch_time'] = running_stats['total_train_time'] / (epoch + 1)\n",
    "        \n",
    "        # å­¦ç¿’ç‡æ›´æ–°\n",
    "        ultra_scheduler.step(val_f1_macro)\n",
    "        current_lr = ultra_optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # å±¥æ­´ã«è¿½åŠ \n",
    "        ultra_history['train_loss'].append(avg_train_loss)\n",
    "        ultra_history['train_f1_macro'].append(train_f1_macro)\n",
    "        ultra_history['train_f1_micro'].append(train_f1_micro)\n",
    "        ultra_history['val_loss'].append(avg_val_loss)\n",
    "        ultra_history['val_f1_macro'].append(val_f1_macro)\n",
    "        ultra_history['val_accuracy'].append(val_accuracy)\n",
    "        ultra_history['learning_rates'].append(current_lr)\n",
    "        ultra_history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯\n",
    "        improvement_status = \"\"\n",
    "        if val_f1_macro > best_val_f1_ultra + ultra_config['min_improvement']:\n",
    "            best_val_f1_ultra = val_f1_macro\n",
    "            best_model_state_ultra = ultra_model.state_dict().copy()\n",
    "            patience_counter_ultra = 0\n",
    "            improvement_status = \"âœ¨ NEW BEST\"\n",
    "            running_stats['best_val_f1'] = best_val_f1_ultra\n",
    "            running_stats['epochs_since_improvement'] = 0\n",
    "        else:\n",
    "            patience_counter_ultra += 1\n",
    "            improvement_status = f\"â³ Patience {patience_counter_ultra}/{ultra_config['patience']}\"\n",
    "            running_stats['epochs_since_improvement'] = patience_counter_ultra\n",
    "        \n",
    "        # ãƒªãƒƒãƒãªã‚¨ãƒãƒƒã‚¯ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n",
    "        if epoch % ultra_config['display_interval'] == 0 or epoch == ultra_config['epochs'] - 1:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ğŸ“ˆ EPOCH {epoch+1:03d} DETAILED SUMMARY\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"ğŸ”¥ Training  : Loss={avg_train_loss:.6f}, F1_macro={train_f1_macro:.4f}, F1_micro={train_f1_micro:.4f}\")\n",
    "            print(f\"ğŸ“Š Validation: Loss={avg_val_loss:.6f}, F1_macro={val_f1_macro:.4f}, Acc={val_accuracy:.4f}\")\n",
    "            print(f\"âš¡ Performance: LR={current_lr:.2e}, Epoch_Time={epoch_time:.1f}s, Avg_Time={running_stats['avg_epoch_time']:.1f}s\")\n",
    "            print(f\"ğŸ† Best State : Best_F1={running_stats['best_val_f1']:.4f}, No_Improve={running_stats['epochs_since_improvement']}\")\n",
    "            print(f\"ğŸ¯ Status    : {improvement_status}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°ï¼ˆè©³ç´°çµ±è¨ˆä»˜ãï¼‰\n",
    "        main_progress.set_postfix({\n",
    "            'Val_F1': f'{val_f1_macro:.4f}',\n",
    "            'Best_F1': f'{running_stats[\"best_val_f1\"]:.4f}',\n",
    "            'Patience': f'{patience_counter_ultra}/{ultra_config[\"patience\"]}',\n",
    "            'LR': f'{current_lr:.1e}',\n",
    "            'Time': f'{epoch_time:.1f}s'\n",
    "        })\n",
    "        main_progress.update(1)\n",
    "        \n",
    "        # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯\n",
    "        if patience_counter_ultra >= ultra_config['patience']:\n",
    "            main_progress.close()\n",
    "            print(f\"\\nğŸ›‘ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    main_progress.close()\n",
    "    \n",
    "    # æœ€çµ‚çµ±è¨ˆã¨ã‚µãƒãƒªãƒ¼\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    \n",
    "    print(f\"\\n{'ğŸ‰'*20} ULTRA TRAINING COMPLETED {'ğŸ‰'*20}\")\n",
    "    print(f\"âœ… Best Validation F1 Macro: {best_val_f1_ultra:.6f}\")\n",
    "    print(f\"ğŸ“ˆ Total Epochs Completed: {epoch+1}\")\n",
    "    print(f\"â° Total Training Time: {total_training_time:.2f}s ({total_training_time/60:.1f}min)\")\n",
    "    print(f\"ğŸ’¨ Average Epoch Time: {running_stats['avg_epoch_time']:.2f}s\")\n",
    "    print(f\"ğŸš€ Training Speed: {(epoch+1)/total_training_time*60:.1f} epochs/min\")\n",
    "    print(f\"ğŸ”§ Final Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    if best_model_state_ultra is not None:\n",
    "        ultra_model.load_state_dict(best_model_state_ultra)\n",
    "        print(f\"âœ… Best model loaded with F1 Macro: {best_val_f1_ultra:.6f}\")\n",
    "    \n",
    "    # å­¦ç¿’æ›²ç·šã®ç°¡å˜ãªå¯è¦–åŒ–\n",
    "    if len(ultra_history['val_f1_macro']) > 1:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('ğŸ¨ Ultra Advanced Training Progress', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(ultra_history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
    "        axes[0, 0].plot(ultra_history['val_loss'], label='Val Loss', color='red', linewidth=2)\n",
    "        axes[0, 0].set_title('ğŸ“‰ Loss Curves')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score curves\n",
    "        axes[0, 1].plot(ultra_history['train_f1_macro'], label='Train F1 Macro', color='green', linewidth=2)\n",
    "        axes[0, 1].plot(ultra_history['val_f1_macro'], label='Val F1 Macro', color='orange', linewidth=2)\n",
    "        axes[0, 1].set_title('ğŸ“ˆ F1 Score Curves')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('F1 Score')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Rate\n",
    "        axes[1, 0].plot(ultra_history['learning_rates'], color='purple', linewidth=2)\n",
    "        axes[1, 0].set_title('âš¡ Learning Rate Schedule')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Epoch Times\n",
    "        axes[1, 1].plot(ultra_history['epoch_times'], color='brown', linewidth=2)\n",
    "        axes[1, 1].axhline(y=running_stats['avg_epoch_time'], color='red', linestyle='--', \n",
    "                          label=f'Avg: {running_stats[\"avg_epoch_time\"]:.1f}s')\n",
    "        axes[1, 1].set_title('â±ï¸ Epoch Times')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Time (seconds)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ultra Advanced Training ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbc0c4",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Ÿè¡Œå®Œäº†ã¨ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€COVID-19å¤‰ç•°äºˆæ¸¬ã®ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸï¼š\n",
    "\n",
    "### âœ… å®Ÿè£…å®Œäº†é …ç›®\n",
    "\n",
    "1. **ğŸ“¦ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**\n",
    "   - `covid_mutation_prediction` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½œæˆã¨çµ±åˆ\n",
    "   - å†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆ\n",
    "   - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚‹å …ç‰¢æ€§ç¢ºä¿\n",
    "\n",
    "2. **ğŸ¤– é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«å®Ÿè£…**\n",
    "   - ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãTransformerãƒ¢ãƒ‡ãƒ«\n",
    "   - Focal Loss ã«ã‚ˆã‚‹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ\n",
    "   - å‹•çš„å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "3. **ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ **\n",
    "   - ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã®è©³ç´°åˆ†æ\n",
    "   - ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ã®å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "   - å¯è¦–åŒ–ã«ã‚ˆã‚‹æ€§èƒ½åˆ†æ\n",
    "\n",
    "4. **âš¡ ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š**\n",
    "   - tqdmé€²æ—ãƒãƒ¼ã«ã‚ˆã‚‹è¨“ç·´çŠ¶æ³ã®å¯è¦–åŒ–\n",
    "   - è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å®‰å®šæ€§ç¢ºä¿\n",
    "\n",
    "### ğŸ“ˆ ä»Šå¾Œã®ç™ºå±•å¯èƒ½æ€§\n",
    "\n",
    "- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: Optunaã‚’ä½¿ç”¨ã—ãŸè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›\n",
    "- **æ™‚ç³»åˆ—åˆ†æ**: ã‚ˆã‚Šç²¾å¯†ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’\n",
    "- **è»¢ç§»å­¦ç¿’**: ä»–ã®å¤‰ç•°äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã¸ã®é©ç”¨\n",
    "\n",
    "### ğŸ”§ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ \n",
    "\n",
    "å®Œå…¨ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ ã«ã‚ˆã‚Šã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç‹¬ç«‹æ€§ã¨å†åˆ©ç”¨æ€§ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "```\n",
    "covid_mutation_prediction/\n",
    "â”œâ”€â”€ config/          # è¨­å®šç®¡ç†\n",
    "â”œâ”€â”€ data/            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
    "â”œâ”€â”€ models/          # ãƒ¢ãƒ‡ãƒ«å®šç¾©\n",
    "â”œâ”€â”€ training/        # è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯\n",
    "â”œâ”€â”€ evaluation/      # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ \n",
    "â””â”€â”€ utils/           # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "```\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ç ”ç©¶ç”¨é€”ã¨å®Ÿç”¨åŒ–ã®ä¸¡æ–¹ã«å¯¾å¿œã§ãã‚‹ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ä¿å®ˆæ€§ã®é«˜ã„æ©Ÿæ¢°å­¦ç¿’ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦å®Œæˆã—ã¾ã—ãŸã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp25-05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
