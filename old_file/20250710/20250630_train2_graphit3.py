# %%
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import importlib
import pickle
import hashlib
import traceback
from datetime import datetime
from torch.utils.data import DataLoader
from torch import nn, optim
from sklearn.metrics import classification_report

# é«˜é€ŸåŒ–ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import torch.cuda.amp as amp  # æ··åˆç²¾åº¦
try:
    torch._dynamo.config.suppress_errors = True  # torch.compile ã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶
except:
    pass

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import module.input_mutation_path as imp
import module.get_feature as gfea
import module.mutation_graphit as mt
import module.make_dataset as mds
import module.evaluation2 as ev
import module.save2 as save

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# %%
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å†èª­ã¿è¾¼ã¿ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
importlib.reload(imp)
importlib.reload(gfea)
importlib.reload(mt)
importlib.reload(mds)
importlib.reload(ev)
importlib.reload(save)

# %%
# å®Ÿé¨“è¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# =============================================================================

# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
folder_name = "../model/20250630_train2/"
save_dir = os.path.join(folder_name, current_time)
os.makedirs(save_dir, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
model_config = {
    'num_epochs': 50,
    'batch_size': 128,
    'd_model': 256,
    'nhead': 8,
    'num_layers': 4,
    'learning_rate': 1e-4,
    'weight_decay': 1e-5,
    'auto_adjust': True,  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´æ©Ÿèƒ½
    
    # ğŸš€ GraphiTé«˜é€ŸåŒ–è¨­å®š
    'optimization': {
        'enable_graph_cache': True,        # ã‚°ãƒ©ãƒ•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
        'max_graph_connections': 8,        # æœ€å¤§æ¥ç¶šæ•°åˆ¶é™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5â†’8ï¼‰
        'similarity_threshold': 0.75,      # é¡ä¼¼æ€§é–¾å€¤ã‚’ä¸Šã’ã¦æ¥ç¶šæ•°å‰Šæ¸›
        'use_compile': True,               # torch.compileæœ‰åŠ¹åŒ–ï¼ˆPyTorch 2.0+ï¼‰
        'mixed_precision': True,           # æ··åˆç²¾åº¦æ¼”ç®—
        'gradient_checkpointing': False,   # ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼ˆé€Ÿåº¦ä½ä¸‹ï¼‰
        'fast_math': True                  # é«˜é€Ÿæ•°å­¦æ¼”ç®—
    },
    
    # é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
    'dev_mode': {
        'enabled': False,                  # Trueã§é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
        'sample_ratio': 0.1,              # å…¨ãƒ‡ãƒ¼ã‚¿ã®10%ä½¿ç”¨
        'quick_epochs': 10,               # çŸ­ç¸®ã‚¨ãƒãƒƒã‚¯æ•°
        'batch_size_multiplier': 2        # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ 
    }
}

# ç‰¹å¾´é‡ãƒã‚¹ã‚¯è¨­å®šï¼ˆä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’æŒ‡å®šï¼‰
feature_mask = [
    True,   # ts (ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—)
    True,   # base_mut (å¡©åŸºå¤‰ç•°)
    True,   # base_pos (å¡©åŸºä½ç½®)
    True,   # amino_mut (ã‚¢ãƒŸãƒé…¸å¤‰ç•°)
    True,   # amino_pos (ã‚¢ãƒŸãƒé…¸ä½ç½®)
    True,   # mut_type (å¤‰ç•°ã‚¿ã‚¤ãƒ—)
    True,   # protein (ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³)
    True,   # codon_pos (ã‚³ãƒ‰ãƒ³ä½ç½®)
    True    # count (ã‚«ã‚¦ãƒ³ãƒˆ)
]

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²è¨­å®š
data_config = {
    'test_start': 36,
    'ylen': 1,
    'val_ratio': 0.2,
    'feature_idx': 6,  # proteinç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    'nmax': 100000000,
    'nmax_per_strain': 1000000
}

# ğŸš€ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰é©ç”¨
if model_config['dev_mode']['enabled']:
    print("âš¡ é–‹ç™ºç”¨é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã‚’é©ç”¨ä¸­...")
    data_config['nmax_per_strain'] = int(data_config['nmax_per_strain'] * model_config['dev_mode']['sample_ratio'])
    model_config['num_epochs'] = model_config['dev_mode']['quick_epochs']
    model_config['batch_size'] = model_config['batch_size'] * model_config['dev_mode']['batch_size_multiplier']
    print(f"  - ãƒ‡ãƒ¼ã‚¿åˆ¶é™: {data_config['nmax_per_strain']:,} samples per strain")
    print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: {model_config['num_epochs']}")
    print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {model_config['batch_size']}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
dataset_config = {
    'strains': ['B.1.1.7'],  # ['B.1.1.7','P.1','BA.2','BA.1.1','BA.1','B.1.617.2','B.1.351','B.1.1.529']
    'usher_dir': '../usher_output/',
    'bunpu_csv': "table_heatmap/250621/table_set/table_set.csv",
    'codon_csv': 'meta_data/codon_mutation4.csv',
    'cache_dir': '../cache'  # ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
}


print(f"å®Ÿé¨“è¨­å®šå®Œäº† - ä¿å­˜å…ˆ: {save_dir}")
print(f"å¯¾è±¡å¤‰ç•°æ ª: {dataset_config['strains']}")
print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š: d_model={model_config['d_model']}, nhead={model_config['nhead']}, num_layers={model_config['num_layers']}")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆæœŸåŒ–ã¨æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
cache_dir = dataset_config['cache_dir']
os.makedirs(cache_dir, exist_ok=True)

if os.path.exists(cache_dir):
    cache_files = [f for f in os.listdir(cache_dir) if f.startswith('feature_data_cache_') and f.endswith('.pkl')]
    if cache_files:
        print(f"\næ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ« ({len(cache_files)}å€‹):")
        total_cache_size = 0
        for cache_file in sorted(cache_files):
            cache_path = os.path.join(cache_dir, cache_file)
            cache_size = os.path.getsize(cache_path) / (1024 * 1024)  # MB
            total_cache_size += cache_size
            mtime = os.path.getmtime(cache_path)
            mtime_str = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
            print(f"  {cache_file}: {cache_size:.1f}MB (ä½œæˆ: {mtime_str})")
        print(f"ç·ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {total_cache_size:.1f}MB")
    else:
        print("\næ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
else:
    print(f"\nã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {cache_dir}")

# %%
# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
# =============================================================================

# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
names, lengths, base_HGVS_paths = imp.input(
    dataset_config['strains'], 
    dataset_config['usher_dir'], 
    nmax=data_config['nmax'], 
    nmax_per_strain=data_config['nmax_per_strain']
)
bunpu_df = pd.read_csv(dataset_config['bunpu_csv'])
codon_df = pd.read_csv(dataset_config['codon_csv'])

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
import pickle
import hashlib

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆï¼ˆå¤‰ç•°æ ªã€nmaxã€nmax_per_strainã‹ã‚‰ä¸€æ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½œæˆï¼‰
cache_key_data = {
    'strains': sorted(dataset_config['strains']),  # ã‚½ãƒ¼ãƒˆã—ã¦é †åºã‚’çµ±ä¸€
    'nmax': data_config['nmax'],
    'nmax_per_strain': data_config['nmax_per_strain'],
    'bunpu_csv': dataset_config['bunpu_csv'],
    'codon_csv': dataset_config['codon_csv']
}
cache_key_str = str(cache_key_data)
cache_hash = hashlib.md5(cache_key_str.encode()).hexdigest()[:12]  # çŸ­ç¸®ãƒãƒƒã‚·ãƒ¥
cache_filename = f"feature_data_cache_{cache_hash}.pkl"
cache_dir = dataset_config['cache_dir']
os.makedirs(cache_dir, exist_ok=True)
cache_filepath = os.path.join(cache_dir, cache_filename)

print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: {cache_key_data}")
print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«: {cache_filepath}")

# ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’å«ã‚€ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰
cache_valid = False
if os.path.exists(cache_filepath):
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’ãƒã‚§ãƒƒã‚¯
    cache_mtime = os.path.getmtime(cache_filepath)
    
    # ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’ãƒã‚§ãƒƒã‚¯
    source_files = [dataset_config['bunpu_csv'], dataset_config['codon_csv']]
    latest_source_mtime = 0
    
    for source_file in source_files:
        if os.path.exists(source_file):
            source_mtime = os.path.getmtime(source_file)
            latest_source_mtime = max(latest_source_mtime, source_mtime)
        else:
            print(f"è­¦å‘Š: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {source_file}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šæ–°ã—ã„å ´åˆã®ã¿æœ‰åŠ¹
    if cache_mtime > latest_source_mtime:
        cache_valid = True
        print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        with open(cache_filepath, 'rb') as f:
            data = pickle.load(f)
        load_time = time.time() - start_time
        cache_size = os.path.getsize(cache_filepath) / (1024 * 1024)  # MB
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®èª­ã¿è¾¼ã¿å®Œäº†: {len(data)} sequences ({load_time:.2f}ç§’, {cache_size:.1f}MB)")
    else:
        print("ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ˆã‚Šæ–°ã—ã„ãŸã‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™")

if not cache_valid:
    print("ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’æ–°è¦ç”Ÿæˆä¸­...")
    start_time = time.time()
    data = gfea.Feature_path_incl_ts(base_HGVS_paths, codon_df, bunpu_df)
    extraction_time = time.time() - start_time
    print(f"ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(data)} sequences ({extraction_time:.2f}ç§’)")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    print("ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ä¸­...")
    save_start = time.time()
    with open(cache_filepath, 'wb') as f:
        pickle.dump(data, f)
    save_time = time.time() - save_start
    cache_size = os.path.getsize(cache_filepath) / (1024 * 1024)  # MB
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {cache_filepath} ({save_time:.2f}ç§’, {cache_size:.1f}MB)")

print(f"Sample data structure: {data[0][1]}")

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®å®Ÿè¡Œ
train_x, train_y, val_x, val_y, test_x, test_y = mds.create_time_aware_split_modified(
    data, data_config['test_start'], data_config['ylen'], data_config['val_ratio']
)

# ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º
train_y_protein = mds.extract_feature_sequences(train_y, data_config['feature_idx'])
val_y_protein = mds.extract_feature_sequences(val_y, data_config['feature_idx'])

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«ã®çµåˆ
train_x2, train_y2 = mds.add_x_by_y(train_x, train_y_protein)
val_x2, val_y2 = mds.add_x_by_y(val_x, val_y_protein)

print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_x2)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_x2)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_x)} ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—")

# %%
# èªå½™æ§‹ç¯‰ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
# =============================================================================

# èªå½™ã‚’æ§‹ç¯‰
print("å®šç¾©æ¸ˆã¿ç‰¹å¾´é‡åã‹ã‚‰èªå½™ã‚’æ§‹ç¯‰ä¸­ï¼ˆåˆ¶é™ãªã—ï¼‰...")
feature_vocabs = mds.build_feature_vocabularies_from_definitions()

print(f"\nç·ç‰¹å¾´é‡æ•°: {len(feature_vocabs)}")
print(f"ç·èªå½™ã‚µã‚¤ã‚º: {sum(len(vocab) for vocab in feature_vocabs):,}")

# å„ç‰¹å¾´é‡ã®èªå½™ã‚µã‚¤ã‚ºã‚’è©³ç´°è¡¨ç¤ºï¼ˆã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®ã¿ï¼‰
print("\nå„ç‰¹å¾´é‡ã®è©³ç´°:")
feature_names = ['ts', 'base_mut', 'base_pos', 'amino_mut', 'amino_pos', 'mut_type', 'protein', 'codon_pos']
for i, name in enumerate(feature_names):
    print(f"  {name}: {len(feature_vocabs[i]):,} tokens")

print(f"  count: æ•°å€¤ï¼ˆèªå½™è¾æ›¸ãªã—ï¼‰")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
train_dataset = mds.MutationSequenceDataset(train_x2, train_y2, feature_vocabs)
val_dataset = mds.MutationSequenceDataset(val_x2, val_y2, feature_vocabs, train_dataset.max_length)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆé«˜é€ŸåŒ–è¨­å®šä»˜ãï¼‰
train_loader = DataLoader(
    train_dataset, 
    batch_size=model_config['batch_size'], 
    shuffle=True,
    num_workers=4,          # ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    pin_memory=True,        # GPUè»¢é€é«˜é€ŸåŒ–
    persistent_workers=True # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹å†åˆ©ç”¨
)
val_loader = DataLoader(
    val_dataset, 
    batch_size=model_config['batch_size'], 
    shuffle=False,
    num_workers=2,          # æ¤œè¨¼ç”¨ã¯å°‘ãªã‚
    pin_memory=True,
    persistent_workers=True
)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
print(f"æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {train_dataset.max_length}")
print(f"ã‚¯ãƒ©ã‚¹: {train_dataset.label_encoder.classes_}")

# %%
# ãƒ‡ãƒãƒƒã‚°ç”¨ã®èªå½™ãƒ»ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
def debug_vocab_mismatch(train_loader, feature_vocabs):
    print("=== èªå½™ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¸æ•´åˆãƒã‚§ãƒƒã‚¯ ===")
    
    for batch_idx, batch in enumerate(train_loader):
        categorical_data = batch['categorical']
        print(f"ãƒãƒƒãƒ {batch_idx}: å½¢çŠ¶ {categorical_data.shape}")
        
        for feature_idx in range(categorical_data.shape[1]):
            feature_data = categorical_data[:, feature_idx, :]
            max_val = feature_data.max().item()
            min_val = feature_data.min().item()
            vocab_size = len(feature_vocabs[feature_idx])
            
            print(f"  ç‰¹å¾´é‡{feature_idx}: min={min_val}, max={max_val}, vocab_size={vocab_size}")
            
            if max_val >= vocab_size:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: max_val({max_val}) >= vocab_size({vocab_size})")
                # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                problematic_values = feature_data[feature_data >= vocab_size]
                print(f"    å•é¡Œã®ã‚ã‚‹å€¤: {problematic_values[:10].tolist()}")
                return feature_idx, max_val, vocab_size
            elif max_val < 0:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: è² ã®å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                return feature_idx, max_val, vocab_size
        
        if batch_idx >= 2:  # æœ€åˆã®3ãƒãƒƒãƒã®ã¿ãƒã‚§ãƒƒã‚¯
            break
    
    print("âœ… å…¨ã¦ã®ç‰¹å¾´é‡ã§èªå½™ã‚µã‚¤ã‚ºã®ç¯„å›²å†…ã§ã™")
    return None, None, None

# ãƒ‡ãƒãƒƒã‚°ã‚’å®Ÿè¡Œ
debug_vocab_mismatch(train_loader, feature_vocabs)

# %%
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã¨è¨“ç·´è¨­å®š
# =============================================================================

# ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€æå¤±é–¢æ•°ã®åˆæœŸåŒ–
print("ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = mt.MutationGraphiT(
    feature_vocabs=feature_vocabs,
    d_model=model_config['d_model'],
    nhead=model_config['nhead'],
    num_layers=model_config['num_layers'],
    num_classes=train_dataset.num_classes,
    max_seq_length=train_dataset.max_length,
    feature_mask=feature_mask,
    auto_adjust=model_config['auto_adjust'],
    similarity_threshold=model_config['optimization']['similarity_threshold']  # ğŸš€ è¨­å®šã‹ã‚‰é–¾å€¤ã‚’å–å¾—
).to(device)

# ğŸš€ é«˜é€ŸåŒ–æœ€é©åŒ–ã®é©ç”¨
print("é«˜é€ŸåŒ–æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")

# torch.compileæœ€é©åŒ–ï¼ˆPyTorch 2.0+ï¼‰
if model_config['optimization']['use_compile'] and hasattr(torch, 'compile'):
    try:
        print("  - torch.compileæœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
        model = torch.compile(model, mode='max-autotune')
        print("  âœ… torch.compileæœ€é©åŒ–å®Œäº†")
    except Exception as e:
        print(f"  âš ï¸ torch.compileæœ€é©åŒ–å¤±æ•—: {e}")

# æ··åˆç²¾åº¦ç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–
scaler = None
if model_config['optimization']['mixed_precision'] and device.type == 'cuda':
    print("  - æ··åˆç²¾åº¦æ¼”ç®—ã‚’æœ‰åŠ¹åŒ–...")
    scaler = torch.amp.GradScaler('cuda')
    print("  âœ… æ··åˆç²¾åº¦è¨­å®šå®Œäº†")

# é«˜é€Ÿæ•°å­¦æ¼”ç®—ã®æœ‰åŠ¹åŒ–
if model_config['optimization']['fast_math'] and device.type == 'cuda':
    print("  - CUDAé«˜é€Ÿæ•°å­¦æ¼”ç®—ã‚’æœ‰åŠ¹åŒ–...")
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    print("  âœ… é«˜é€Ÿæ•°å­¦æ¼”ç®—è¨­å®šå®Œäº†")

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=model_config['learning_rate'], weight_decay=model_config['weight_decay'])
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)

print(f"å®Ÿéš›ã®d_model: {model.actual_d_model}")
print(f"å®Ÿéš›ã®nhead: {model.actual_nhead}")
print(f"å®Ÿéš›ã®num_layers: {model.actual_num_layers}")
print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"ã‚¯ãƒ©ã‚¹æ•°: {train_dataset.num_classes}")
print(f"æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {train_dataset.max_length}")

# %%
# ğŸš€ é«˜é€ŸåŒ–ã•ã‚ŒãŸè¨“ç·´é–¢æ•°
def train_epoch_mixed_precision(model, dataloader, criterion, optimizer, device, scaler):
    """æ··åˆç²¾åº¦ã‚’ä½¿ç”¨ã—ãŸé«˜é€ŸåŒ–è¨“ç·´ã‚¨ãƒãƒƒã‚¯"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch in dataloader:
        categorical_data = batch['categorical'].to(device, non_blocking=True)
        count_data = batch['count'].to(device, non_blocking=True)
        labels = batch['label'].to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦ã§ã®é †ä¼æ’­
        with torch.amp.autocast('cuda'):
            outputs = model(categorical_data, count_data)
            loss = criterion(outputs, labels)
        
        # æ··åˆç²¾åº¦ã§ã®é€†ä¼æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total
    return avg_loss, accuracy

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
best_val_acc = 0
best_model_state = None
train_losses = []
val_losses = []
train_accs = []
val_accs = []
epoch_times = []  # ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã‚’è¨˜éŒ²

print("è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
training_start_time = time.time()  # å…¨ä½“ã®é–‹å§‹æ™‚é–“

try:
    for epoch in range(model_config['num_epochs']):
        epoch_start_time = time.time()  # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚é–“
        print(f"\nEpoch {epoch+1}/{model_config['num_epochs']}")
        
        # ğŸš€ é«˜é€ŸåŒ–ã•ã‚ŒãŸè¨“ç·´
        if scaler is not None:
            # æ··åˆç²¾åº¦è¨“ç·´
            train_loss, train_acc = train_epoch_mixed_precision(
                model, train_loader, criterion, optimizer, device, scaler
            )
        else:
            # æ¨™æº–è¨“ç·´
            train_loss, train_acc = mt.train_epoch(model, train_loader, criterion, optimizer, device)
        
        # æ¤œè¨¼
        val_loss, val_acc, val_preds, val_targets = mt.evaluate(model, val_loader, criterion, device)
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ›´æ–°
        scheduler.step(val_loss)
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚é–“è¨ˆç®—
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)
        
        # çµæœã‚’è¨˜éŒ²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        # æ™‚é–“æƒ…å ±ã‚’å«ã‚€å‡ºåŠ›
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print(f"Epoch Time: {epoch_duration:.2f}s ({epoch_duration/60:.1f}min)")
        
        # ç´¯ç©æ™‚é–“ã¨æ¨å®šæ®‹ã‚Šæ™‚é–“
        total_elapsed = sum(epoch_times)
        avg_epoch_time = total_elapsed / len(epoch_times)
        remaining_epochs = model_config['num_epochs'] - (epoch + 1)
        estimated_remaining = avg_epoch_time * remaining_epochs
        
        print(f"Elapsed: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min), "
              f"ETA: {estimated_remaining:.1f}s ({estimated_remaining/60:.1f}min)")
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            print(f"æ–°ã—ã„æœ€è‰¯ãƒ¢ãƒ‡ãƒ« (Val Acc: {val_acc:.4f})")

    # è¨“ç·´å®Œäº†æ™‚ã®çµ±è¨ˆ
    training_end_time = time.time()
    total_training_time = training_end_time - training_start_time
    
    print(f"\n=== è¨“ç·´å®Œäº†! ===")
    print(f"æœ€è‰¯æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f}")
    print(f"ç·è¨“ç·´æ™‚é–“: {total_training_time:.1f}s ({total_training_time/60:.1f}min)")
    print(f"å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {np.mean(epoch_times):.2f}s")
    print(f"æœ€é€Ÿã‚¨ãƒãƒƒã‚¯: {min(epoch_times):.2f}s")
    print(f"æœ€é…ã‚¨ãƒãƒƒã‚¯: {max(epoch_times):.2f}s")

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    if best_model_state:
        model.load_state_dict(best_model_state)
        
except Exception as e:
    print(f"è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    import traceback
    traceback.print_exc()

# %%
# è¨“ç·´çµæœã®åˆ†æã¨å¯è¦–åŒ–
# =============================================================================

# è¨“ç·´çµæœã®å¯è¦–åŒ–
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 3, 3)
# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡
val_loss, val_acc, val_preds, val_targets = mt.evaluate(model, val_loader, criterion, device)
print(f"æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")

# ã‚¯ãƒ©ã‚¹åã¨ãƒ©ãƒ™ãƒ«ã®å¯¾å¿œã‚’ç¢ºèª
class_names = train_dataset.label_encoder.classes_
print(f"å…¨ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¯ãƒ©ã‚¹æ•°: {len(set(val_targets))}")
print(f"äºˆæ¸¬ã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¯ãƒ©ã‚¹æ•°: {len(set(val_preds))}")

# å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã®ã¿ã‚’å–å¾—
unique_labels = sorted(set(val_targets) | set(val_preds))
actual_class_names = [class_names[i] for i in unique_labels]

print(f"å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹: {actual_class_names}")

# åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã®ã¿ï¼‰
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(
    val_targets, 
    val_preds, 
    labels=unique_labels,
    target_names=actual_class_names, 
    zero_division=0
))

# è¿½åŠ ã®çµ±è¨ˆæƒ…å ±
print(f"\nè©³ç´°çµ±è¨ˆ:")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
unique_targets, target_counts = np.unique(val_targets, return_counts=True)
for label, count in zip(unique_targets, target_counts):
    class_name = class_names[label]
    print(f"  {class_name}: {count}ã‚µãƒ³ãƒ—ãƒ«")

plt.tight_layout()
plt.show()

# %%
# ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®è©³ç´°åˆ†æ
# =============================================================================

print("=== ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®è©³ç´°åˆ†æ ===")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
train_labels = [train_dataset.encoded_labels[i] for i in range(len(train_dataset))]
train_unique, train_counts = np.unique(train_labels, return_counts=True)

print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
for label, count in zip(train_unique, train_counts):
    class_name = class_names[label]
    print(f"  {class_name}: {count}ã‚µãƒ³ãƒ—ãƒ«")

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
val_labels = [val_dataset.encoded_labels[i] for i in range(len(val_dataset))]
val_unique, val_counts = np.unique(val_labels, return_counts=True)

print(f"\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
for label, count in zip(val_unique, val_counts):
    class_name = class_names[label]
    print(f"  {class_name}: {count}ã‚µãƒ³ãƒ—ãƒ«")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã‚ã£ã¦æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ãªã„ã‚¯ãƒ©ã‚¹
train_only = set(train_unique) - set(val_unique)
if train_only:
    print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿å­˜åœ¨ã™ã‚‹ã‚¯ãƒ©ã‚¹:")
    for label in sorted(train_only):
        print(f"  {class_names[label]}")

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã‚ã£ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªã„ã‚¯ãƒ©ã‚¹
val_only = set(val_unique) - set(train_unique)
if val_only:
    print(f"\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿å­˜åœ¨ã™ã‚‹ã‚¯ãƒ©ã‚¹:")
    for label in sorted(val_only):
        print(f"  {class_names[label]}")

# %%
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡
# =============================================================================

print("ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã®å‘¼ã³å‡ºã—ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
timestep_results = ev.evaluate_test_data_timestep(
    model, test_x, test_y, data_config['feature_idx'],
    feature_vocabs, val_dataset, device, criterion
)

# çµæœã®å¯è¦–åŒ–ï¼ˆevaluation2.pyã®é–¢æ•°å‘¼ã³å‡ºã—ã®ã¿ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãªã—ï¼‰
print("çµæœã‚’å¯è¦–åŒ–ä¸­...")
ev.plot_timestep_results(timestep_results)

# è©³ç´°åˆ†æã®è¡¨ç¤º
ev.print_detailed_timestep_analysis(timestep_results, train_dataset)

# ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
ev.print_test_summary(timestep_results)

# %%
# ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜
# =============================================================================

print("=== ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜ã‚’é–‹å§‹ ===")

# 1. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
save.save_model_and_training_state(save_dir, best_model_state, model, optimizer, scheduler)

# 2. è¨­å®šã®ä¿å­˜
save.save_hyperparameters_and_config_legacy(
    dataset_config['strains'], data_config['nmax'], data_config['nmax_per_strain'], 
    data_config['test_start'], data_config['ylen'], data_config['val_ratio'],
    data_config['feature_idx'], train_dataset, val_dataset, model,
    model_config['num_epochs'], model_config['batch_size'], train_losses, val_losses,
    train_accs, val_accs, best_val_acc, feature_vocabs,
    device, save_dir, feature_mask=feature_mask
)

# 3. èªå½™è¾æ›¸ã®ä¿å­˜
save.save_vocabularies(save_dir, feature_vocabs, train_dataset)

# 4. è¨“ç·´å±¥æ­´ã®ä¿å­˜
save.save_training_plots(train_losses, val_losses, train_accs, val_accs, scheduler, save_dir)

# 5. ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜
save.save_test_results(timestep_results, save_dir)

# 6. READMEã®ä¿å­˜
save.save_readme(dataset_config['strains'], model, train_dataset, val_dataset,
                  model_config['num_epochs'], train_accs, val_accs, best_val_acc,
                  feature_vocabs, save_dir)

# 7. å®Ÿé¨“ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
save.save_experiment_summary(dataset_config['strains'], train_dataset, val_dataset, model,
                             model_config['num_epochs'], train_accs, val_accs, best_val_acc,
                             timestep_results, save_dir)

print(f"\n=== å…¨ã¦ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ ===")
print(f"ä¿å­˜å…ˆ: {os.path.abspath(save_dir)}")

# æœ€çµ‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
if os.path.exists(save_dir):
    files = os.listdir(save_dir)
    print(f"ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
    print("ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    for file in sorted(files):
        file_path = os.path.join(save_dir, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            print(f"  {file}: {size:,} bytes")
    
    # ä¿å­˜å®Œäº†ã®ç¢ºèª
    expected_files = [
        "best_model.pth", "config.json", "feature_vocabularies.pkl", 
        "label_encoder.pkl", "training_history.png", "test_results.json",
        "README.md", "experiment_summary.json"
    ]
    
    missing_files = [f for f in expected_files if f not in files]
    if missing_files:
        print(f"\nè­¦å‘Š: ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_files}")
    else:
        print(f"\nâœ… å…¨ã¦ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
else:
    print(f"âŒ ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {save_dir}")

print("\n=== å®Ÿé¨“å®Œäº† ===")
print(f"å®Ÿé¨“å: {current_time}")
print(f"å¯¾è±¡å¤‰ç•°æ ª: {dataset_config['strains']}")
print(f"æœ€è‰¯æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f}")
print(f"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {model_config['num_epochs']}")
print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š: d_model={model_config['d_model']}, nhead={model_config['nhead']}, num_layers={model_config['num_layers']}")

# %%