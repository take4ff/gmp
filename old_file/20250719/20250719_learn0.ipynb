{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] import: ../usher_output/B.1.1.7/0/mutation_paths_B.1.1.7.tsv\n",
      "[INFO] 指定されたnmax=1000に達しました。\n",
      "[INFO] B.1.1.7のデータを読み込みました: 1000 サンプル\n",
      "[INFO] 読み込み完了: 1000 サンプル\n",
      "元データ 1000\n",
      "共起フィルタ 1000\n",
      "ユニークパス 979\n",
      "keys: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38]\n",
      "train 2683\n",
      "valid 194\n",
      "test:34 4\n",
      "test:35 2\n",
      "test:36 1\n",
      "test:38 2\n"
     ]
    }
   ],
   "source": [
    "import module.input_mutation_path as imp\n",
    "from module.make_dataset import base\n",
    "\n",
    "data_config = {\n",
    "    'train_end': 37,\n",
    "    'test_start': 34,\n",
    "    'ylen': 1,\n",
    "    'val_ratio': 0.2,\n",
    "    'mix_ratio': 0.2, \n",
    "    'frag_len': 10,\n",
    "    'max_co_occur': 20,\n",
    "    'nmax': 1000,\n",
    "    'nmax_per_strain': 1000000\n",
    "}\n",
    "\n",
    "dataset_config = {\n",
    "    'strains': ['B.1.1.7','P.1','BA.2','BA.1.1','BA.1','B.1.617.2','B.1.351','B.1.1.529'],\n",
    "    'usher_dir': '../usher_output/',\n",
    "    'bunpu_csv': \"table_heatmap/250621/table_set/table_set.csv\",\n",
    "    'codon_csv': 'meta_data/codon_mutation4.csv',\n",
    "    'cache_dir': '../cache',  # 特徴データキャッシュ用ディレクトリ\n",
    "}\n",
    "\n",
    "names, lengths, base_HGVS_paths = imp.input(\n",
    "dataset_config['strains'], \n",
    "dataset_config['usher_dir'], \n",
    "nmax=data_config['nmax'], \n",
    "nmax_per_strain=data_config['nmax_per_strain']\n",
    ")\n",
    "\n",
    "print(\"元データ\",len(base_HGVS_paths))\n",
    "\n",
    "def filter_co_occur(data,sample_name,data_len,max_co_occur,out_num=None):\n",
    "    filted_data = []\n",
    "    filted_sample_name =[]\n",
    "    filted_data_len = []\n",
    "    for i in range(len(data)):\n",
    "        compare = 0\n",
    "        for j in range(len(data[i])):\n",
    "            mutation = data[i][j].split(',')\n",
    "            if(compare < len(mutation)):\n",
    "                compare = len(mutation)\n",
    "        if(compare <= max_co_occur):\n",
    "            filted_data.append(data[i])\n",
    "            filted_sample_name.append(sample_name[i])\n",
    "            filted_data_len.append(data_len[i])\n",
    "        if(out_num is not None and len(filted_data)>=out_num):\n",
    "            break\n",
    "    return filted_data,filted_sample_name,filted_data_len\n",
    "\n",
    "def unique_path(data):\n",
    "    return [list(item) for item in dict.fromkeys(tuple(path) for path in data)]\n",
    "\n",
    "filted_data, temp, temp  = filter_co_occur(base_HGVS_paths,names,lengths,data_config['max_co_occur'])\n",
    "print(\"共起フィルタ\",len(filted_data))\n",
    "\n",
    "data = unique_path(filted_data)\n",
    "print(\"ユニークパス\",len(data))\n",
    "\n",
    "def data_by_ts(data):\n",
    "    data_ts = {}\n",
    "    for i in range(len(data)):\n",
    "        length = len(data[i])\n",
    "        if(data_ts.get(length) is None):\n",
    "            data_ts[length] = []\n",
    "        data_ts[length].append(data[i])\n",
    "    return data_ts\n",
    "\n",
    "def fragmentation(data,frag_len,end_opt=False):\n",
    "    frag_data = []\n",
    "    for i in range(len(data)):\n",
    "        if end_opt:\n",
    "            start = len(data[i])-frag_len\n",
    "        else:\n",
    "            start = 0\n",
    "        for j in range(start,len(data[i])-frag_len+1):\n",
    "            frag_data.append(data[i][j:j+frag_len])\n",
    "    return frag_data\n",
    "\n",
    "def dataset_by_ts(data, train_end, test_start, mix_ratio=0.2, val_ratio=0.2, frag_len=None, unique=False):\n",
    "    import random\n",
    "    train = []\n",
    "    test = {}\n",
    "    data_ts = data_by_ts(data)\n",
    "    keys = sorted(list(data_ts.keys()))\n",
    "    print(\"keys:\",keys)\n",
    "\n",
    "    if train_end >= test_start:\n",
    "        train_test_mix = [test_start, train_end]\n",
    "    else:\n",
    "        train_test_mix = None\n",
    "    \n",
    "    if train_test_mix is None:\n",
    "        for k in keys:\n",
    "            items = data_ts[k]\n",
    "            if k <= train_end:\n",
    "                train.extend(items)\n",
    "            if test_start <= k:\n",
    "                test[k] = items.copy()\n",
    "    else:\n",
    "        for k in keys:\n",
    "            items = data_ts[k]\n",
    "            if k < test_start:\n",
    "                train.extend(items)\n",
    "            elif train_end < k:\n",
    "                test[k] = items.copy()\n",
    "            else:\n",
    "                train_temp, test[k] = train_test_split(items, test_size=mix_ratio)\n",
    "                train.extend(train_temp)\n",
    "    if frag_len is not None:\n",
    "        train, valid = train_test_split(train, test_size=val_ratio)\n",
    "        train = fragmentation(train,frag_len=frag_len)\n",
    "        valid = fragmentation(valid,frag_len=frag_len,end_opt=True)\n",
    "        for k in sorted(list(test.keys())):\n",
    "            test[k] = fragmentation(test[k],frag_len=frag_len,end_opt=True)\n",
    "    \n",
    "    if unique:\n",
    "        train = unique_path(train)\n",
    "        valid = unique_path(valid)\n",
    "        for k in sorted(list(test.keys())):\n",
    "            test[k] = unique_path(test[k])\n",
    "        \n",
    "    return train, valid, test\n",
    "\n",
    "train,valid,test = dataset_by_ts(data,data_config['train_end'],data_config['test_start'],\n",
    "                                    data_config['mix_ratio'],data_config['val_ratio'],data_config['frag_len'],unique=True)\n",
    "\n",
    "print(\"train\",len(train))\n",
    "print(\"valid\",len(valid))\n",
    "for k in sorted(list(test.keys())):\n",
    "    print(f\"test:{k} {len(test[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C14408T', 'A23403G', 'C241T', 'C3037T', 'G28881A', 'G28882A,G28883C', 'C884T', 'T884C,G28280C,A28281T,T28282A', 'A23063T', 'C15279T', 'A28111G,C28977T', 'T23063A', 'C913T', 'C5986T', 'T6954C,A23063T,C23271A,C23604A,C23709T', 'T24506G,G24914C', 'T16176C', 'C14676T,C27972T', 'C3267T', 'C5388A', 'G28048T', 'A12097G', 'G3259T', 'C7029T', 'T1054C', 'C7749T', 'C203A', 'C21588T', 'C28312T']\n"
     ]
    }
   ],
   "source": [
    "print(base_HGVS_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp25-05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
